{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b507ed2-fa5a-428e-ada9-198dfab99acb",
   "metadata": {},
   "source": [
    "### Fine tuning & deploying Flan-T5-Large to Amazon Bedrock using Custom Model Import\n",
    "\n",
    "This notebook covers the step by step process of fine tuning a [FLAN-T5 large](https://huggingface.co/google/flan-t5-large) mode and deploying it using Bedrock's [Custom Model Import](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html) feature. \n",
    "The fine tuning data we will be using is based on medical terminology this data can be found on HuggingFace [here](https://huggingface.co/datasets/gamino/wiki_medical_terms). In many discplines, industry specific jargon is used and an LLM may not have the correct understanding of words, context or abbreviations. By fine tuning a model on medical terminology in this case, the LLM is given the ability to understand specific jargon and answer questions the user might have. \n",
    "\n",
    "This notebook will use the HuggingFace Transformers library to fine tune FLAN-T5-Large. due to the fine tuning process being \"local\" you can run this notebook anywhere as long as the proper compute is available. This notebook was originally created in Sagemaker Studio using a \"ml.g5.16xlarge\" instance.\n",
    "\n",
    "The resulting files are imported into Amazon Bedrock via custom model import\n",
    "\n",
    "WARNING: This method of Custom Model Import will only work with \"FLAN-t5-large\". \"FLAN-t5-small\" is incompatible with Bedrock Custom Model Import in its current state. This is due to the number of heads in the model needing to be a multiple of 4, due to the model needing to be sharded accordingly in the GPU. the number of heads for FLAN-t5-small is 6. this can be checked in the model's config.json file under the parameter \"num_heads\" \n",
    "\n",
    "Below is an overview of the architecture we will cover in this notebook:\n",
    "\n",
    "![Notebook Architecture](./images/notebook-architecture.jpg \"Notebook Architecture\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f9c86-be2a-4c13-958c-9ea5f60938a0",
   "metadata": {},
   "source": [
    "### Installs & Imports \n",
    "\n",
    "we will be utilizing HuggingFace Transformers library to pull a pretrained model from the Hub and fine tune it. The dataset we will be finetuning on will also be pulled from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "922b449e-0e7e-4392-a61c-af24b7fb6e57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install datasets --quiet\n",
    "!pip install transformers[torch] --quiet\n",
    "!pip install tokenizers --quiet\n",
    "!pip install sentencepiece --quiet\n",
    "!pip install huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f6e4ce2-00d0-40d0-b26e-03921ea62dc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from transformers import T5Tokenizer, DataCollatorForSeq2Seq\n",
    "from transformers import T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80580722-ac33-4d89-94e6-88162f4c322a",
   "metadata": {},
   "source": [
    "### Pulling a pre-trained model & a datset from HuggingFace \n",
    "\n",
    "as mentioned at the beginning of the notebook, we will be fine tuning google's FLAN-t5-large from HuggingFace. This model is free to pull, there is no requirement for a HuggingFace account.\n",
    "\n",
    "The dataset we are pulling can be looked at [here](https://huggingface.co/datasets/gamino/wiki_medical_terms). This is a dataset containing over 6000+ medical terms, and their wiki definitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5077f180-14e3-4cf8-8326-69428d12a149",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "#Load model\n",
    "model_name = \"google/flan-t5-large\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f74e9d47-1d1f-427d-9780-bc6bed90e3b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['page_title', 'page_text', '__index_level_0__'],\n",
       "        num_rows: 6861\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load data from huggingface\n",
    "ds = load_dataset(\"gamino/wiki_medical_terms\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a83824-f37a-4421-a6d0-bbd5f1398d7f",
   "metadata": {},
   "source": [
    "### Processing the dataset \n",
    "\n",
    "first we will take a percentage of the dataset to train out model. the test size in the cell below determines how much of the dataset is used for testing, while the rest is used for training (if you change the test size to 0.1 that means 10% is used for testing and 90% is used for training). Feel free to change this number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c64ea3-0b2c-4a7a-a156-b411467fe056",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['page_title', 'page_text', '__index_level_0__'],\n",
       "        num_rows: 4802\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['page_title', 'page_text', '__index_level_0__'],\n",
       "        num_rows: 2059\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using 70% of the dataset to train\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.3)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e2a5af-29a1-49c7-882d-fde8faa9bc90",
   "metadata": {},
   "source": [
    "We want to train our model in a Q & A format. This will allow the model to answer questions about specific medical terms.\n",
    "\n",
    "There are two columns of interest to us: \"page_title\" (medical term) & \"page_text\" (definition/explanation)\n",
    "\n",
    "in the cell below we will add \"what is\" as a prefix to the medical term to transform the title into the question. and we will keep \"page_text\" as is to act as the answer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "789565dc-b5f7-48c8-8ed3-1d894a5ff111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Process data to fit a Q & A format \n",
    "prefix = \"What is \"\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_data(raw_data):\n",
    "   # Transform page title into an answer:\n",
    "   inputs = [prefix + term for term in raw_data[\"page_title\"]]\n",
    "   model_inputs = tokenizer(inputs, max_length=128, truncation=True)\n",
    "   # keep explanations as is:\n",
    "   definitions = tokenizer(text_target=raw_data[\"page_text\"], \n",
    "                      max_length=512,         \n",
    "                      truncation=True)\n",
    "   model_inputs[\"labels\"] = definitions[\"input_ids\"]\n",
    "   return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "554d542d-a0f4-4bb8-989d-47af5b9f3117",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068fc50f2af84576a04dc0b3a4063a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4802 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69401699dd3f43adb25b647aa3e5545d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2059 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Map the preprocessing function across our dataset\n",
    "ds = ds.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf679c7c-e633-4698-a454-af4a4b58380c",
   "metadata": {},
   "source": [
    "### Setting up the training job \n",
    "\n",
    "For this training job we are using a HuggingFace's Trainer class. we are using the Seq2Seq trainer since T5 is an encoder-decoder architecture. \n",
    "\n",
    "there are many hyperparameters that can be set when submitting a training job with the trainer class. many hyperparameters will have default values, and optimal values can vary based on what CPUs, GPUs, and dataset is being used to train the model.\n",
    "\n",
    "HuggingFace has published guidance on optimizations that can be made when training on a single GPU [here](https://huggingface.co/docs/transformers/en/perf_train_gpu_one#gradient-accumulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ca9398a-be74-4e2f-b8cd-3a67a6ed6e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "   output_dir=\"./results\",\n",
    "   eval_strategy=\"steps\",\n",
    "   learning_rate=4e-4,\n",
    "   per_device_train_batch_size=2,\n",
    "   per_device_eval_batch_size=2,\n",
    "   weight_decay=0.01,\n",
    "   save_total_limit=3,\n",
    "   num_train_epochs=1, #can increase this to improve fine tuning results (will increase training time)\n",
    "   predict_with_generate=True,\n",
    "   push_to_hub=False,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_accumulation_steps=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4aba6e39-129a-4259-a0fc-2fbb4445a649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Setup Trainer \n",
    "trainer = Seq2SeqTrainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=ds[\"train\"],\n",
    "   eval_dataset=ds[\"test\"],\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d13ad38-7dd4-4cf6-9ee5-7eb7356d1a16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Empty pytorch cache\n",
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a95ab8b-87f0-4b83-a806-472801c14cb6",
   "metadata": {},
   "source": [
    "### Submitting the training job \n",
    "\n",
    "On this GPU with 1 epoch set, training should take roughly 30 mins. This is for a fast finetuning job to get to the main idea of this notebook - to showcase Bedrock Custom Model importing. If you require a higher performance for your fine tuned model please increase the Epoch's and adjust other hyperparameters as needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13d82532-5212-45e9-b6cd-a45417e01e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1200 25:21, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.611000</td>\n",
       "      <td>2.358274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.504000</td>\n",
       "      <td>2.300562</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1200, training_loss=2.541583786010742, metrics={'train_runtime': 1523.1069, 'train_samples_per_second': 3.153, 'train_steps_per_second': 0.788, 'total_flos': 235554486976512.0, 'train_loss': 2.541583786010742, 'epoch': 0.9995835068721366})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65ee77d-ca4c-4c70-85b4-bfaa5741176d",
   "metadata": {},
   "source": [
    "### Model Inference \n",
    "\n",
    "We will now take our latest checkpoint and generate text with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b39808e-bfbd-49a6-b90b-c0d24f43405d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>Dexamethasone suppression test (DST) is based on the results of an in vitro study. It was developed by Biogen, Inc and approved for medical use under UDA number NCT0377362. The FDA granted approval to bioGenesis Laboratories Ltd as well; however it has not been used clinically or commercialized since its introduction into medicine at least two years ago with no indication that this product should be available anywhere else except within Europe where there are restrictions regarding marketing authorization due either directly from Pharmaceuticals International Limited (\"PharmA\") nor through any other company\n"
     ]
    }
   ],
   "source": [
    "#Model Inference \n",
    "last_checkpoint = \"./results/checkpoint-1200\" #Load checkpoint that you want to test \n",
    "\n",
    "finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
    "tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)\n",
    "\n",
    "med_term = \"what is Dexamethasone suppression test\" \n",
    "\n",
    "query = tokenizer(med_term, return_tensors=\"pt\")\n",
    "output = finetuned_model.generate(**query, max_length=128, no_repeat_ngram_size=True)\n",
    "answer = tokenizer.decode(output[0])\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b8287-d8ac-4d72-9192-49d89010a21d",
   "metadata": {},
   "source": [
    "### Model Upload \n",
    "\n",
    "with out model now generating text related to medical terminology we will now upload it to S3 to ensure readiness for Bedrock Custom Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a7338d-a2dc-4b3a-be58-dfc2fe323776",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Upload model to S3 Bucket\n",
    "import boto3\n",
    "import os\n",
    "# Set up S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Specify your S3 bucket name and the prefix (folder) where you want to upload the files\n",
    "bucket_name = 'your-bucket-here'#YOU BUCKET HERE\n",
    "model_name = \"results/checkpoint-#\" #YOUR LATEST CHECKPOINT HERE (this will be in the \"results\" folder in your notebook directory replace the \"#\" with the latest checkpoint number)\n",
    "prefix = 'flan-t5-large-medical/' + model_name\n",
    "\n",
    "# Upload files to S3\n",
    "def upload_directory_to_s3(local_directory, bucket, s3_prefix):\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            relative_path = os.path.relpath(local_path, local_directory)\n",
    "            s3_path = os.path.join(s3_prefix, relative_path)\n",
    "            \n",
    "            print(f'Uploading {local_path} to s3://{bucket}/{s3_path}')\n",
    "            s3.upload_file(local_path, bucket, s3_path)\n",
    "\n",
    "# Call the function to upload the downloaded model files to S3\n",
    "upload_directory_to_s3(model_name, bucket_name, prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772d14e1-1ce4-40c9-b345-6c585a68ae46",
   "metadata": {},
   "source": [
    "### Importing Model to Amazon Bedrock\n",
    "\n",
    "Now that our model artifacts are uploaded into an S3 bucket, we can import it into Amazon Bedrock \n",
    "\n",
    "in the AWS console, we can go to the Amazon Bedrock page. On the left side under \"Foundation models\" we will click on \"Imported models\"\n",
    "\n",
    "![Step 1](./images/step1.png \"Step 1\")\n",
    "\n",
    "\n",
    "You can now click on \"Import model\"\n",
    "\n",
    "![Step 2](./images/step2.png \"Step 2\")\n",
    "\n",
    "In this next step you will have to configure:\n",
    "\n",
    "1. Model Name \n",
    "2. Import Job Name \n",
    "3. Model Import Settings \n",
    "    a. Select Amazon S3 bucket \n",
    "    b. Select your bucket location (uploaded in the previous section)\n",
    "4. Create a IAM role, or use an existing one (not shown in image)\n",
    "5. Click Import (not shown in image)\n",
    "\n",
    "![Step 3](./images/step3.png \"Step 3\")\n",
    "\n",
    "\n",
    "You will now be taken to the page below. Your model may take up to an hour to import. \n",
    "\n",
    "![Step 4](./images/step4.png \"Step 4\")\n",
    "\n",
    "After your model imports you will then be able to test it via the playground or API! \n",
    "\n",
    "![Playground](./images/playground.gif \"Playground\")\n",
    "\n",
    "END OF NOTEBOOK"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g5.16xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
