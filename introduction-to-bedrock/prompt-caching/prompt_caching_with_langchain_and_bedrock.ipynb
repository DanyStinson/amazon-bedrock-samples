{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Prompt Caching with LangChain and Amazon Bedrock Converse API\n",
    "\n",
    "This notebook demonstrates how to effectively use prompt caching with LangChain's ChatBedrockConverse class to improve performance when working with large documents or repetitive contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Prompt Caching?\n",
    "\n",
    "Prompt caching allows you to store portions of your conversation context, enabling models to:\n",
    "- Reuse cached context instead of reprocessing inputs\n",
    "- Reduce response Time-To-First-Token (TTFT) for subsequent queries\n",
    "- Potentially lower token usage by avoiding redundant processing\n",
    "\n",
    "This is particularly useful for scenarios like:\n",
    "- Chat with documents (RAG applications)\n",
    "- Coding assistants with large code files\n",
    "- Agentic workflows with complex system prompts\n",
    "- Few-shot learning with numerous examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -U langchain-aws boto3 pandas matplotlib seaborn requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import time\n",
    "from enum import Enum\n",
    "\n",
    "# AWS and external services\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "# Data processing and visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# LangChain components\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the LLM\n",
    "\n",
    "Let's set up our ChatBedrockConverse model with appropriate configuration for prompt caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the ChatBedrockConverse model\n",
    "llm = ChatBedrockConverse(\n",
    "    model_id=\"anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    region_name=\"us-west-2\",\n",
    "    temperature=0,  # Lower temperature for more deterministic responses\n",
    "    max_tokens=1000  # Adjust based on your needs\n",
    ")\n",
    "\n",
    "# Test the model with a simple query\n",
    "test_response = llm.invoke(\"Hello, are you ready to demonstrate prompt caching?\")\n",
    "print(\"Model initialized successfully. Test response:\")\n",
    "print(test_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Sample Documents\n",
    "\n",
    "To effectively demonstrate prompt caching, we need documents with sufficient length. Let's fetch some AWS blog posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs for sample documents\n",
    "topics = [\n",
    "    'https://aws.amazon.com/blogs/aws/reduce-costs-and-latency-with-amazon-bedrock-intelligent-prompt-routing-and-prompt-caching-preview/',\n",
    "    'https://aws.amazon.com/blogs/machine-learning/enhance-conversational-ai-with-advanced-routing-techniques-with-amazon-bedrock/',\n",
    "    'https://aws.amazon.com/blogs/security/cost-considerations-and-common-options-for-aws-network-firewall-log-management/'\n",
    "]\n",
    "\n",
    "# Fetch the first document\n",
    "response = requests.get(topics[0])\n",
    "blog = response.text\n",
    "\n",
    "# Print a preview of the document\n",
    "print(f\"Document length: {len(blog)} characters\")\n",
    "print(f\"Preview: {blog[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "\n",
    "Let's create helper functions to work with prompt caching and measure performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheMode(Enum):\n",
    "    ON = \"on\"\n",
    "    OFF = \"off\"\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        if self.__class__ is other.__class__:\n",
    "            return self.value < other.value\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "def chat_with_document_langchain(document, user_query, llm_model, use_cache=True):\n",
    "    \"\"\"Chat with a document using LangChain's ChatBedrockConverse with proper prompt caching.\"\"\"\n",
    "    \n",
    "    # Create system message with instructions\n",
    "    instructions = (\n",
    "        \"I will provide you with a document, followed by a question about its content. \"\n",
    "        \"Your task is to analyze the document, extract relevant information, and provide \"\n",
    "        \"a comprehensive answer to the question.\"\n",
    "    )\n",
    "    \n",
    "    document_content = f\"Here is the document: <document> {document} </document>\"\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create messages with cache point if enabled\n",
    "    messages = [\n",
    "        SystemMessage(content=instructions),\n",
    "    ]\n",
    "    \n",
    "    # Add document content with cache point if caching is enabled\n",
    "    if use_cache:\n",
    "        # This is the key part - add the cache point directly in the message content\n",
    "        human_message_content = [\n",
    "            {\"type\": \"text\", \"text\": document_content},\n",
    "            ChatBedrockConverse.create_cache_point()  # Add cache point here\n",
    "        ]\n",
    "        messages.append(HumanMessage(content=human_message_content))\n",
    "    else:\n",
    "        messages.append(HumanMessage(content=document_content))\n",
    "    \n",
    "    # First invoke to process the document (and cache it if enabled)\n",
    "    response = llm_model.invoke(messages)\n",
    "    \n",
    "    # Now add the user query\n",
    "    messages.append(HumanMessage(content=user_query))\n",
    "    \n",
    "    # Second invoke with the query\n",
    "    response = llm_model.invoke(messages)\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Response (elapsed time: {elapsed_time:.2f}s):\")\n",
    "    print(response.content)\n",
    "    \n",
    "    # Print usage metrics if available\n",
    "    if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "        print(\"\\nUsage metrics:\")\n",
    "        print(json.dumps(response.usage_metadata, indent=2))\n",
    "\n",
    "        # Check for cache-related metrics in input_token_details\n",
    "        if hasattr(response.usage_metadata, 'input_token_details'):\n",
    "            cache_details = response.usage_metadata.input_token_details\n",
    "            if cache_details.get('cache_read', 0) > 0:\n",
    "                print(f\"Cache was used! Read tokens: {cache_details['cache_read']}\")\n",
    "            if cache_details.get('cache_creation', 0) > 0:\n",
    "                print(f\"Cache was created! Write tokens: {cache_details['cache_creation']}\")\n",
    "    \n",
    "    return response, elapsed_time\n",
    "\n",
    "\n",
    "def add_median_labels(ax):\n",
    "    \"\"\"Add median value labels to a boxplot.\"\"\"\n",
    "    lines = ax.get_lines()\n",
    "    boxes = [c for c in ax.get_children() if type(c).__name__ == 'PathPatch']\n",
    "    lines_per_box = int(len(lines) / len(boxes))\n",
    "    for median in lines[4:len(lines):lines_per_box]:\n",
    "        x, y = (data.mean() for data in median.get_data())\n",
    "        # get text value from the median line\n",
    "        value = median.get_ydata()[0]\n",
    "        text = ax.text(x, y, f'{value:.2f}s', ha='center', va='center',\n",
    "                      fontweight='bold', color='white')\n",
    "        text.set_path_effects([path_effects.withStroke(linewidth=3, foreground='black')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Document Chat with Caching\n",
    "\n",
    "Now let's test our document chat function with prompt caching enabled. The key difference is that we're including the cache point directly in the message content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample questions\n",
    "questions = [\n",
    "    'What is this blog post about?',\n",
    "    'What are the main use cases for prompt caching?',\n",
    "    'How does prompt caching improve performance?'\n",
    "]\n",
    "\n",
    "# First query with caching enabled (this will create the cache)\n",
    "print(\"FIRST QUERY (CACHE CREATION):\")\n",
    "print(\"-\" * 50)\n",
    "response1, time1 = chat_with_document_langchain(blog, questions[0], llm, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second query with caching enabled (this should use the cache)\n",
    "print(\"\\n\\nSECOND QUERY (USING CACHE):\")\n",
    "print(\"-\" * 50)\n",
    "response2, time2 = chat_with_document_langchain(blog, questions[1], llm, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third query with caching disabled (for comparison)\n",
    "print(\"\\n\\nTHIRD QUERY (NO CACHE):\")\n",
    "print(\"-\" * 50)\n",
    "response3, time3 = chat_with_document_langchain(blog, questions[2], llm, use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Function\n",
    "\n",
    "Let's create a function to benchmark the performance of prompt caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "def benchmark_prompt_caching(document, questions, llm_model, iterations=3):\n",
    "    \"\"\"Benchmark the performance of prompt caching.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Test with caching enabled\n",
    "    print(\"\\nBenchmarking with caching ENABLED:\")\n",
    "    for i in range(iterations):\n",
    "        for q_idx, question in enumerate(questions):\n",
    "            print(f\"Iteration {i+1}, Question {q_idx+1}: {question[:30]}...\")\n",
    "            start_time = time.time()\n",
    "            response, _ = chat_with_document_langchain(document, question, llm_model, use_cache=True)\n",
    "            elapsed = time.time() - start_time\n",
    "            results.append({\n",
    "                'cache_mode': CacheMode.ON.value,\n",
    "                'iteration': i+1,\n",
    "                'question_idx': q_idx+1,\n",
    "                'time': elapsed\n",
    "            })\n",
    "            print(f\"Time: {elapsed:.2f}s\\n\")\n",
    "    \n",
    "    #sleep between tests\n",
    "    sleep(60)\n",
    "\n",
    "    # Test with caching disabled\n",
    "    print(\"\\nBenchmarking with caching DISABLED:\")\n",
    "    for i in range(iterations):\n",
    "        for q_idx, question in enumerate(questions):\n",
    "            print(f\"Iteration {i+1}, Question {q_idx+1}: {question[:30]}...\")\n",
    "            start_time = time.time()\n",
    "            response, _ = chat_with_document_langchain(document, question, llm_model, use_cache=False)\n",
    "            elapsed = time.time() - start_time\n",
    "            results.append({\n",
    "                'cache_mode': CacheMode.OFF.value,\n",
    "                'iteration': i+1,\n",
    "                'question_idx': q_idx+1,\n",
    "                'time': elapsed\n",
    "            })\n",
    "            print(f\"Time: {elapsed:.2f}s\\n\")\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark\n",
    "\n",
    "Let's run a more systematic benchmark to measure the performance improvements from prompt caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the benchmark with a smaller number of iterations for demonstration\n",
    "benchmark_results = benchmark_prompt_caching(\n",
    "    document=blog,\n",
    "    questions=questions,\n",
    "    llm_model=llm,\n",
    "    iterations=1  # Adjust based on your needs\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "benchmark_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.boxplot(x='cache_mode', y='time', data=benchmark_results)\n",
    "\n",
    "# Add median labels\n",
    "add_median_labels(ax)\n",
    "\n",
    "# Set titles and labels\n",
    "plt.title('Response Time by Cache Mode', fontsize=16)\n",
    "plt.xlabel('Cache Mode', fontsize=14)\n",
    "plt.ylabel('Time (seconds)', fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Prompt Caching with LangChain Chains\n",
    "\n",
    "Now let's see how to integrate prompt caching with LangChain chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain_with_caching(llm_model, document):\n",
    "    \"\"\"Create a LangChain chain with prompt caching.\"\"\"\n",
    "    # Create a prompt template with the document and a cache point\n",
    "    template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that answers questions about documents.\"),\n",
    "        (\"human\", [{\"type\": \"text\", \"text\": f\"Here is a document: {document[:2000]}...\"},\n",
    "                  ChatBedrockConverse.create_cache_point()]),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    \n",
    "    # Create the chain\n",
    "    chain = template | llm_model | StrOutputParser()\n",
    "    \n",
    "    return chain\n",
    "\n",
    "\n",
    "def run_chain_with_timing(chain, document, question):\n",
    "    \"\"\"Run a chain with timing.\"\"\"\n",
    "    start_time = time.time()\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Response (elapsed time: {elapsed_time:.2f}s):\")\n",
    "    print(response[:200] + \"...\" if len(response) > 200 else response)\n",
    "    \n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain with prompt caching\n",
    "chain = create_chain_with_caching(llm, blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the chain with multiple queries\n",
    "print(\"Running chain with prompt caching:\")\n",
    "time1 = run_chain_with_timing(chain, blog, questions[0])\n",
    "time2 = run_chain_with_timing(chain, blog, questions[1])\n",
    "time3 = run_chain_with_timing(chain, blog, questions[2])\n",
    "\n",
    "print(f\"First query time: {time1:.2f}s\")\n",
    "print(f\"Second query time: {time2:.2f}s\")\n",
    "print(f\"Third query time: {time3:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Example: Direct Message Construction\n",
    "\n",
    "Let's look at a more manual example where we construct the messages directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messages with cache point\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that answers questions about documents.\"),\n",
    "    HumanMessage(content=[\n",
    "        {\"type\": \"text\", \"text\": f\"Here is a document: {blog[:1000]}...\"},\n",
    "        ChatBedrockConverse.create_cache_point()  # Add cache point here\n",
    "    ])\n",
    "]\n",
    "\n",
    "# First invoke to process the document (and cache it)\n",
    "start_time = time.time()\n",
    "response = llm.invoke(messages)\n",
    "print(f\"First response time (cache creation): {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Add a question\n",
    "messages.append(HumanMessage(content=\"What is the main topic of this document?\"))\n",
    "\n",
    "# Second invoke with the question (should use cache)\n",
    "start_time = time.time()\n",
    "response = llm.invoke(messages)\n",
    "print(f\"Second response time (using cache): {time.time() - start_time:.2f}s\")\n",
    "print(f\"Response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Prompt Caching with LangChain\n",
    "\n",
    "Here are some best practices for using prompt caching with LangChain:\n",
    "\n",
    "1. **Include Cache Point in Message Content**: The cache point must be included directly in the message content as a special content block, not just as part of the configuration.\n",
    "\n",
    "2. **Place Cache Point After Static Content**: Place the cache point after the static content (like documents or system prompts) that you want to cache.\n",
    "\n",
    "3. **Use Consistent Cache Points**: Use the same cache point type for related requests to ensure proper caching.\n",
    "\n",
    "4. **Monitor Cache Metrics**: Check the `usage_metadata.input_token_details` field to confirm that caching is working as expected.\n",
    "\n",
    "5. **Structure Messages Properly**: Separate static content (like documents, system prompts) from dynamic content (user queries) to maximize caching benefits.\n",
    "\n",
    "6. **Consider Cache Lifetime**: Be aware that cached prompts expire after a period of inactivity (typically 24 hours)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to use prompt caching with LangChain's ChatBedrockConverse to improve performance when working with large documents or repetitive contexts. Key takeaways:\n",
    "\n",
    "- Prompt caching can significantly reduce response times for subsequent queries\n",
    "- The cache point must be included directly in the message content as a special content block\n",
    "- The `create_cache_point()` method makes it easy to generate cache configurations\n",
    "- Caching works well with LangChain's chains and other abstractions\n",
    "- Performance benefits are most noticeable with large documents or complex system prompts\n",
    "\n",
    "By leveraging prompt caching in your LangChain applications, you can create more responsive and efficient AI experiences while potentially reducing costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
