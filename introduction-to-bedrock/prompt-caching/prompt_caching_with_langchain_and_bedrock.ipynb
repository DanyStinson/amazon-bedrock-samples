{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing Prompt Caching with LangChain and Amazon Bedrock Converse API\n",
    "\n",
    "This notebook demonstrates how to effectively use prompt caching with LangChain's ChatBedrockConverse class to improve performance when working with large documents or repetitive contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Prompt Caching?\n",
    "\n",
    "Prompt caching allows you to store portions of your conversation context, enabling models to:\n",
    "- Reuse cached context instead of reprocessing inputs\n",
    "- Reduce response Time-To-First-Token (TTFT) for subsequent queries\n",
    "- Potentially lower token usage by avoiding redundant processing\n",
    "\n",
    "This is particularly useful for scenarios like:\n",
    "- Chat with documents (RAG applications)\n",
    "- Coding assistants with large code files\n",
    "- Agentic workflows with complex system prompts\n",
    "- Few-shot learning with numerous examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -U langchain-aws boto3 pandas matplotlib seaborn requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import time\n",
    "from enum import Enum\n",
    "\n",
    "# AWS and external services\n",
    "import boto3\n",
    "import requests\n",
    "\n",
    "# Data processing and visualization\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# LangChain components\n",
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the LLM\n",
    "\n",
    "Let's set up our ChatBedrockConverse model with appropriate configuration for prompt caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model initialized successfully. Test response:\n",
      "I want to be direct with you. I do not actually have a specific demonstration of prompt caching prepared. While I understand the concept of prompt caching in AI systems, I aim to be clear that I won't pretend to have a pre-planned demonstration. I'm happy to discuss prompt caching conceptually if you'd like. Would you prefer that?\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ChatBedrockConverse model\n",
    "llm = ChatBedrockConverse(\n",
    "    model_id=\"anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    region_name=\"us-west-2\",\n",
    "    temperature=0,  # Lower temperature for more deterministic responses\n",
    "    max_tokens=1000  # Adjust based on your needs\n",
    ")\n",
    "\n",
    "# Test the model with a simple query\n",
    "test_response = llm.invoke(\"Hello, are you ready to demonstrate prompt caching?\")\n",
    "print(\"Model initialized successfully. Test response:\")\n",
    "print(test_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Sample Documents\n",
    "\n",
    "To effectively demonstrate prompt caching, we need documents with sufficient length. Let's fetch some AWS blog posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document length: 102367 characters\n",
      "Preview: <!doctype html>\n",
      "<html lang=\"en-US\" class=\"no-js aws-lng-en_US\" xmlns=\"http://www.w3.org/1999/xhtml\" data-aws-assets=\"https://a0.awsstatic.com\" data-js-version=\"1.0.681\" data-css-version=\"1.0.538\" data...\n"
     ]
    }
   ],
   "source": [
    "# URLs for sample documents\n",
    "topics = [\n",
    "    'https://aws.amazon.com/blogs/aws/reduce-costs-and-latency-with-amazon-bedrock-intelligent-prompt-routing-and-prompt-caching-preview/',\n",
    "    'https://aws.amazon.com/blogs/machine-learning/enhance-conversational-ai-with-advanced-routing-techniques-with-amazon-bedrock/',\n",
    "    'https://aws.amazon.com/blogs/security/cost-considerations-and-common-options-for-aws-network-firewall-log-management/'\n",
    "]\n",
    "\n",
    "# Fetch the first document\n",
    "response = requests.get(topics[0])\n",
    "blog = response.text\n",
    "\n",
    "# Print a preview of the document\n",
    "print(f\"Document length: {len(blog)} characters\")\n",
    "print(f\"Preview: {blog[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Helper Functions\n",
    "\n",
    "Let's create helper functions to work with prompt caching and measure performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheMode(Enum):\n",
    "    ON = \"on\"\n",
    "    OFF = \"off\"\n",
    "    \n",
    "    def __lt__(self, other):\n",
    "        if self.__class__ is other.__class__:\n",
    "            return self.value < other.value\n",
    "        return NotImplemented\n",
    "\n",
    "\n",
    "def chat_with_document_langchain(document, user_query, llm_model, use_cache=True):\n",
    "    \"\"\"Chat with a document using LangChain's ChatBedrockConverse with proper prompt caching.\"\"\"\n",
    "    \n",
    "    # Create system message with instructions\n",
    "    instructions = (\n",
    "        \"I will provide you with a document, followed by a question about its content. \"\n",
    "        \"Your task is to analyze the document, extract relevant information, and provide \"\n",
    "        \"a comprehensive answer to the question.\"\n",
    "    )\n",
    "    \n",
    "    document_content = f\"Here is the document: <document> {document} </document>\"\n",
    "    \n",
    "    # Start timing\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create messages with cache point if enabled\n",
    "    messages = [\n",
    "        SystemMessage(content=instructions),\n",
    "    ]\n",
    "    \n",
    "    # Add document content with cache point if caching is enabled\n",
    "    if use_cache:\n",
    "        # This is the key part - add the cache point directly in the message content\n",
    "        human_message_content = [\n",
    "            {\"type\": \"text\", \"text\": document_content},\n",
    "            ChatBedrockConverse.create_cache_point()  # Add cache point here\n",
    "        ]\n",
    "        messages.append(HumanMessage(content=human_message_content))\n",
    "    else:\n",
    "        messages.append(HumanMessage(content=document_content))\n",
    "    \n",
    "    # First invoke to process the document (and cache it if enabled)\n",
    "    response = llm_model.invoke(messages)\n",
    "    \n",
    "    # Now add the user query\n",
    "    messages.append(HumanMessage(content=user_query))\n",
    "    \n",
    "    # Second invoke with the query\n",
    "    response = llm_model.invoke(messages)\n",
    "    \n",
    "    # Calculate elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Response (elapsed time: {elapsed_time:.2f}s):\")\n",
    "    print(response.content)\n",
    "    \n",
    "    # Print usage metrics if available\n",
    "    if hasattr(response, 'usage_metadata') and response.usage_metadata:\n",
    "        print(\"\\nUsage metrics:\")\n",
    "        print(json.dumps(response.usage_metadata, indent=2))\n",
    "\n",
    "        # Check for cache-related metrics in input_token_details\n",
    "        if hasattr(response.usage_metadata, 'input_token_details'):\n",
    "            cache_details = response.usage_metadata.input_token_details\n",
    "            if cache_details.get('cache_read', 0) > 0:\n",
    "                print(f\"Cache was used! Read tokens: {cache_details['cache_read']}\")\n",
    "            if cache_details.get('cache_creation', 0) > 0:\n",
    "                print(f\"Cache was created! Write tokens: {cache_details['cache_creation']}\")\n",
    "    \n",
    "    return response, elapsed_time\n",
    "\n",
    "\n",
    "def add_median_labels(ax):\n",
    "    \"\"\"Add median value labels to a boxplot.\"\"\"\n",
    "    lines = ax.get_lines()\n",
    "    boxes = [c for c in ax.get_children() if type(c).__name__ == 'PathPatch']\n",
    "    lines_per_box = int(len(lines) / len(boxes))\n",
    "    for median in lines[4:len(lines):lines_per_box]:\n",
    "        x, y = (data.mean() for data in median.get_data())\n",
    "        # get text value from the median line\n",
    "        value = median.get_ydata()[0]\n",
    "        text = ax.text(x, y, f'{value:.2f}s', ha='center', va='center',\n",
    "                      fontweight='bold', color='white')\n",
    "        text.set_path_effects([path_effects.withStroke(linewidth=3, foreground='black')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Document Chat with Caching\n",
    "\n",
    "Now let's test our document chat function with prompt caching enabled. The key difference is that we're including the cache point directly in the message content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample questions\n",
    "questions = [\n",
    "    'What is this blog post about?',\n",
    "    'What are the main use cases for prompt caching?',\n",
    "    'How does prompt caching improve performance?'\n",
    "]\n",
    "\n",
    "# First query with caching enabled (this will create the cache)\n",
    "print(\"FIRST QUERY (CACHE CREATION):\")\n",
    "print(\"-\" * 50)\n",
    "response1, time1 = chat_with_document_langchain(blog, questions[0], llm, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second query with caching enabled (this should use the cache)\n",
    "print(\"\\n\\nSECOND QUERY (USING CACHE):\")\n",
    "print(\"-\" * 50)\n",
    "response2, time2 = chat_with_document_langchain(blog, questions[1], llm, use_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third query with caching disabled (for comparison)\n",
    "print(\"\\n\\nTHIRD QUERY (NO CACHE):\")\n",
    "print(\"-\" * 50)\n",
    "response3, time3 = chat_with_document_langchain(blog, questions[2], llm, use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark Function\n",
    "\n",
    "Let's create a function to benchmark the performance of prompt caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "def benchmark_prompt_caching(document, questions, llm_model, iterations=3):\n",
    "    \"\"\"Benchmark the performance of prompt caching.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Test with caching enabled\n",
    "    print(\"\\nBenchmarking with caching ENABLED:\")\n",
    "    for i in range(iterations):\n",
    "        for q_idx, question in enumerate(questions):\n",
    "            print(f\"Iteration {i+1}, Question {q_idx+1}: {question[:30]}...\")\n",
    "            start_time = time.time()\n",
    "            response, _ = chat_with_document_langchain(document, question, llm_model, use_cache=True)\n",
    "            elapsed = time.time() - start_time\n",
    "            results.append({\n",
    "                'cache_mode': CacheMode.ON.value,\n",
    "                'iteration': i+1,\n",
    "                'question_idx': q_idx+1,\n",
    "                'time': elapsed\n",
    "            })\n",
    "            print(f\"Time: {elapsed:.2f}s\\n\")\n",
    "    \n",
    "    #sleep between tests\n",
    "    sleep(60)\n",
    "\n",
    "    # Test with caching disabled\n",
    "    print(\"\\nBenchmarking with caching DISABLED:\")\n",
    "    for i in range(iterations):\n",
    "        for q_idx, question in enumerate(questions):\n",
    "            print(f\"Iteration {i+1}, Question {q_idx+1}: {question[:30]}...\")\n",
    "            start_time = time.time()\n",
    "            response, _ = chat_with_document_langchain(document, question, llm_model, use_cache=False)\n",
    "            elapsed = time.time() - start_time\n",
    "            results.append({\n",
    "                'cache_mode': CacheMode.OFF.value,\n",
    "                'iteration': i+1,\n",
    "                'question_idx': q_idx+1,\n",
    "                'time': elapsed\n",
    "            })\n",
    "            print(f\"Time: {elapsed:.2f}s\\n\")\n",
    "    \n",
    "    # Convert to DataFrame for analysis\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Benchmark\n",
    "\n",
    "Let's run a more systematic benchmark to measure the performance improvements from prompt caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Benchmarking with caching ENABLED:\n",
      "Iteration 1, Question 1: What is this blog post about?...\n",
      "Response (elapsed time: 13.64s):\n",
      "This blog post is about two new preview features for Amazon Bedrock that help reduce costs and latency for generative AI applications:\n",
      "\n",
      "1. Amazon Bedrock Intelligent Prompt Routing:\n",
      "- Allows routing requests between different foundation models from the same model family\n",
      "- Helps optimize for both quality and cost\n",
      "- Can reduce costs by up to 30% without compromising accuracy\n",
      "- Examples include routing between Claude 3.5 Sonnet and Claude 3 Haiku, or Llama 3.1 70B and 8B models\n",
      "- Intelligently selects the most appropriate model based on prompt complexity\n",
      "\n",
      "2. Amazon Bedrock Prompt Caching:\n",
      "- Enables caching frequently used context across multiple model invocations\n",
      "- Particularly useful for applications like document Q&A or coding assistants\n",
      "- Cached context remains available for up to 5 minutes\n",
      "- Can reduce costs by up to 90% and latency by up to 85%\n",
      "\n",
      "The blog post provides detailed technical explanations, console screenshots, and code examples demonstrating how developers can use these new features to create more efficient and cost-effective generative AI applications. The features are currently in preview and available in select AWS regions.\n",
      "\n",
      "Usage metrics:\n",
      "{\n",
      "  \"input_tokens\": 11,\n",
      "  \"output_tokens\": 268,\n",
      "  \"total_tokens\": 36485,\n",
      "  \"input_token_details\": {\n",
      "    \"cache_creation\": 0,\n",
      "    \"cache_read\": 36206\n",
      "  }\n",
      "}\n",
      "Time: 13.64s\n",
      "\n",
      "Iteration 1, Question 2: What are the main use cases fo...\n",
      "Response (elapsed time: 17.04s):\n",
      "Based on the document, here are the main use cases for prompt caching in Amazon Bedrock:\n",
      "\n",
      "1. Document Q&A Systems\n",
      "- Applications where users ask multiple questions about the same document\n",
      "- The context of the document can be cached, reducing latency and costs for subsequent queries\n",
      "\n",
      "2. Coding Assistants\n",
      "- Maintaining context about code files across multiple interactions\n",
      "- Caching the code context to improve efficiency when asking multiple questions about the same codebase\n",
      "\n",
      "Key benefits of prompt caching include:\n",
      "\n",
      "- Reducing costs by up to 90% for cached input tokens\n",
      "- Reducing latency by up to 85% for supported models\n",
      "- Caching content for up to 5 minutes, with each cache hit resetting the countdown\n",
      "- Supporting cross-Region inference while maintaining cost optimization\n",
      "\n",
      "The document provides an example implementation showing how to cache documents and ask multiple questions about them, demonstrating how prompt caching can be practically applied.\n",
      "\n",
      "Important notes:\n",
      "- Currently available in preview for specific models like Anthropic's Claude 3.5 Sonnet and Claude 3.5 Haiku\n",
      "- Users can request access to the preview\n",
      "- Supports adding multiple cache points in a list of messages\n",
      "- Provides usage metrics to track cache read and write token counts\n",
      "\n",
      "Usage metrics:\n",
      "{\n",
      "  \"input_tokens\": 15,\n",
      "  \"output_tokens\": 278,\n",
      "  \"total_tokens\": 36499,\n",
      "  \"input_token_details\": {\n",
      "    \"cache_creation\": 0,\n",
      "    \"cache_read\": 36206\n",
      "  }\n",
      "}\n",
      "Time: 17.04s\n",
      "\n",
      "Iteration 1, Question 3: How does prompt caching improv...\n",
      "Response (elapsed time: 14.94s):\n",
      "Based on the document, prompt caching in Amazon Bedrock improves performance in several key ways:\n",
      "\n",
      "1. Reduced Latency\n",
      "- Prompt caching can reduce latency by up to 85% for supported models\n",
      "- In the author's tests, subsequent invocations took 55% less time to complete compared to the first invocation\n",
      "\n",
      "2. Cost Efficiency\n",
      "- Cache reads receive a 90% discount compared to non-cached input tokens\n",
      "- For Anthropic models, there are additional costs for tokens written to the cache\n",
      "- For Amazon Nova models, there are no additional costs for cache writes\n",
      "\n",
      "3. How Prompt Caching Works\n",
      "- Frequently used context in prompts can be cached across multiple model invocations\n",
      "- Cached content remains available for up to 5 minutes after each access\n",
      "- Users can tag specific sections of prompts for caching using a `cachePoint` block\n",
      "- When the same cached content is reused, the model loads preprocessed results from the cache instead of processing them again\n",
      "\n",
      "4. Ideal Use Cases\n",
      "- Document Q&A systems where users ask multiple questions about the same document\n",
      "- Coding assistants that need to maintain context about code files\n",
      "- Applications with repeated context that can benefit from preprocessing\n",
      "\n",
      "5. Tracking Performance\n",
      "- The response metadata includes `cacheReadInputTokenCount` and `cacheWriteInputTokenCount` to help users monitor cache usage and performance improvements\n",
      "\n",
      "The document provides a Python code example demonstrating how to implement prompt caching, showing how cached content can significantly reduce processing time and costs for repetitive tasks.\n",
      "\n",
      "Usage metrics:\n",
      "{\n",
      "  \"input_tokens\": 12,\n",
      "  \"output_tokens\": 344,\n",
      "  \"total_tokens\": 36562,\n",
      "  \"input_token_details\": {\n",
      "    \"cache_creation\": 0,\n",
      "    \"cache_read\": 36206\n",
      "  }\n",
      "}\n",
      "Time: 14.94s\n",
      "\n",
      "\n",
      "Benchmarking with caching DISABLED:\n",
      "Iteration 1, Question 1: What is this blog post about?...\n",
      "Response (elapsed time: 28.18s):\n",
      "This blog post is about two new preview features for Amazon Bedrock that help reduce costs and latency for generative AI applications:\n",
      "\n",
      "1. Amazon Bedrock Intelligent Prompt Routing\n",
      "- Allows routing requests between different foundation models from the same model family\n",
      "- Can optimize for quality and cost by selecting the most appropriate model for a given prompt\n",
      "- For example, routing between Claude 3.5 Sonnet and Claude 3 Haiku, or between Llama 3.1 70B and 8B models\n",
      "- Can reduce costs by up to 30% without compromising accuracy\n",
      "\n",
      "2. Amazon Bedrock Prompt Caching\n",
      "- Enables caching frequently used context across multiple model invocations\n",
      "- Particularly useful for applications like document Q&A or coding assistants\n",
      "- Cached context remains available for up to 5 minutes\n",
      "- Can reduce costs by up to 90% and latency by up to 85% for supported models\n",
      "\n",
      "The blog post provides detailed explanations, screenshots, and code examples of how to use these new features in the AWS Management Console, AWS CLI, and AWS SDKs. The features are currently in preview and available in select AWS regions for specific models.\n",
      "\n",
      "The overall goal is to help developers build more cost-effective and high-performing generative AI applications by intelligently routing requests and caching frequently used content.\n",
      "\n",
      "Usage metrics:\n",
      "{\n",
      "  \"input_tokens\": 36218,\n",
      "  \"output_tokens\": 298,\n",
      "  \"total_tokens\": 36516,\n",
      "  \"input_token_details\": {\n",
      "    \"cache_creation\": 0,\n",
      "    \"cache_read\": 0\n",
      "  }\n",
      "}\n",
      "Time: 28.18s\n",
      "\n",
      "Iteration 1, Question 2: What are the main use cases fo...\n"
     ]
    },
    {
     "ename": "ThrottlingException",
     "evalue": "An error occurred (ThrottlingException) when calling the Converse operation (reached max retries: 4): Too many tokens, please wait before trying again.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mThrottlingException\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run the benchmark with a smaller number of iterations for demonstration\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m benchmark_results = \u001b[43mbenchmark_prompt_caching\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m=\u001b[49m\u001b[43mblog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mllm_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Adjust based on your needs\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Display the results\u001b[39;00m\n\u001b[32m     10\u001b[39m benchmark_results\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mbenchmark_prompt_caching\u001b[39m\u001b[34m(document, questions, llm_model, iterations)\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Question \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_idx+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion[:\u001b[32m30\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m response, _ = \u001b[43mchat_with_document_langchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m elapsed = time.time() - start_time\n\u001b[32m     33\u001b[39m results.append({\n\u001b[32m     34\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcache_mode\u001b[39m\u001b[33m'\u001b[39m: CacheMode.OFF.value,\n\u001b[32m     35\u001b[39m     \u001b[33m'\u001b[39m\u001b[33miteration\u001b[39m\u001b[33m'\u001b[39m: i+\u001b[32m1\u001b[39m,\n\u001b[32m     36\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mquestion_idx\u001b[39m\u001b[33m'\u001b[39m: q_idx+\u001b[32m1\u001b[39m,\n\u001b[32m     37\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtime\u001b[39m\u001b[33m'\u001b[39m: elapsed\n\u001b[32m     38\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mchat_with_document_langchain\u001b[39m\u001b[34m(document, user_query, llm_model, use_cache)\u001b[39m\n\u001b[32m     40\u001b[39m     messages.append(HumanMessage(content=document_content))\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# First invoke to process the document (and cache it if enabled)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m response = \u001b[43mllm_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Now add the user query\u001b[39;00m\n\u001b[32m     46\u001b[39m messages.append(HumanMessage(content=user_query))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/bedrock-langchain-demo/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:370\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    360\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    365\u001b[39m     **kwargs: Any,\n\u001b[32m    366\u001b[39m ) -> BaseMessage:\n\u001b[32m    367\u001b[39m     config = ensure_config(config)\n\u001b[32m    368\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    369\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    371\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    373\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    374\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    375\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    380\u001b[39m     ).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/bedrock-langchain-demo/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:947\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    938\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    939\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m    940\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    944\u001b[39m     **kwargs: Any,\n\u001b[32m    945\u001b[39m ) -> LLMResult:\n\u001b[32m    946\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m947\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/bedrock-langchain-demo/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:766\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    764\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    765\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m766\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    767\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    768\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    769\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    770\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    771\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    772\u001b[39m         )\n\u001b[32m    773\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    774\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/bedrock-langchain-demo/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1012\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1010\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1016\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/bedrock-langchain-demo/.venv/lib/python3.12/site-packages/langchain_aws/chat_models/bedrock_converse.py:650\u001b[39m, in \u001b[36mChatBedrockConverse._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    648\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInput params: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    649\u001b[39m logger.info(\u001b[33m\"\u001b[39m\u001b[33mUsing Bedrock Converse API to generate response\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m650\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverse\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbedrock_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    653\u001b[39m logger.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResponse from Bedrock: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    654\u001b[39m response_message = _parse_response(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/bedrock-langchain-demo/.venv/lib/python3.12/site-packages/botocore/client.py:570\u001b[39m, in \u001b[36mClientCreator._create_api_method.<locals>._api_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    566\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    567\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m() only accepts keyword arguments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    568\u001b[39m     )\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/bedrock-langchain-demo/.venv/lib/python3.12/site-packages/botocore/context.py:123\u001b[39m, in \u001b[36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[32m    122\u001b[39m     hook()\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/bedrock-langchain-demo/.venv/lib/python3.12/site-packages/botocore/client.py:1031\u001b[39m, in \u001b[36mBaseClient._make_api_call\u001b[39m\u001b[34m(self, operation_name, api_params)\u001b[39m\n\u001b[32m   1027\u001b[39m     error_code = error_info.get(\u001b[33m\"\u001b[39m\u001b[33mQueryErrorCode\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info.get(\n\u001b[32m   1028\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCode\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1029\u001b[39m     )\n\u001b[32m   1030\u001b[39m     error_class = \u001b[38;5;28mself\u001b[39m.exceptions.from_code(error_code)\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[32m   1032\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1033\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[31mThrottlingException\u001b[39m: An error occurred (ThrottlingException) when calling the Converse operation (reached max retries: 4): Too many tokens, please wait before trying again."
     ]
    }
   ],
   "source": [
    "# Run the benchmark with a smaller number of iterations for demonstration\n",
    "benchmark_results = benchmark_prompt_caching(\n",
    "    document=blog,\n",
    "    questions=questions,\n",
    "    llm_model=llm,\n",
    "    iterations=1  # Adjust based on your needs\n",
    ")\n",
    "\n",
    "# Display the results\n",
    "benchmark_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Benchmark Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.boxplot(x='cache_mode', y='time', data=benchmark_results)\n",
    "\n",
    "# Add median labels\n",
    "add_median_labels(ax)\n",
    "\n",
    "# Set titles and labels\n",
    "plt.title('Response Time by Cache Mode', fontsize=16)\n",
    "plt.xlabel('Cache Mode', fontsize=14)\n",
    "plt.ylabel('Time (seconds)', fontsize=14)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Prompt Caching with LangChain Chains\n",
    "\n",
    "Now let's see how to integrate prompt caching with LangChain chains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chain_with_caching(llm_model, document):\n",
    "    \"\"\"Create a LangChain chain with prompt caching.\"\"\"\n",
    "    # Create a prompt template with the document and a cache point\n",
    "    template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful assistant that answers questions about documents.\"),\n",
    "        (\"human\", [{\"type\": \"text\", \"text\": f\"Here is a document: {document[:2000]}...\"},\n",
    "                  ChatBedrockConverse.create_cache_point()]),\n",
    "        (\"human\", \"{question}\")\n",
    "    ])\n",
    "    \n",
    "    # Create the chain\n",
    "    chain = template | llm_model | StrOutputParser()\n",
    "    \n",
    "    return chain\n",
    "\n",
    "\n",
    "def run_chain_with_timing(chain, document, question):\n",
    "    \"\"\"Run a chain with timing.\"\"\"\n",
    "    start_time = time.time()\n",
    "    response = chain.invoke({\"question\": question})\n",
    "    elapsed_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(f\"Response (elapsed time: {elapsed_time:.2f}s):\")\n",
    "    print(response[:200] + \"...\" if len(response) > 200 else response)\n",
    "    \n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain with prompt caching\n",
    "chain = create_chain_with_caching(llm, blog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running chain with prompt caching:\n",
      "\n",
      "Question: What is this blog post about?\n",
      "Response (elapsed time: 2.23s):\n",
      "Based on the HTML document, this appears to be a blog post from the AWS News Blog about a new feature for Amazon Bedrock called \"Intelligent Prompt Routing and prompt caching\". The title specifically ...\n",
      "\n",
      "Question: What are the main use cases for prompt caching?\n",
      "Response (elapsed time: 2.24s):\n",
      "I apologize, but while the document appears to be an HTML page about an AWS News Blog post regarding Amazon Bedrock's Intelligent Prompt Routing and prompt caching, the excerpt you've shared does not ...\n",
      "\n",
      "Question: How does prompt caching improve performance?\n",
      "Response (elapsed time: 2.90s):\n",
      "I apologize, but while the document appears to be an HTML page about Amazon Bedrock's prompt routing and caching, the text you've shared is cut off and does not provide the full details about how prom...\n",
      "First query time: 2.23s\n",
      "Second query time: 2.24s\n",
      "Third query time: 2.90s\n"
     ]
    }
   ],
   "source": [
    "# Test the chain with multiple queries\n",
    "print(\"Running chain with prompt caching:\")\n",
    "time1 = run_chain_with_timing(chain, blog, questions[0])\n",
    "time2 = run_chain_with_timing(chain, blog, questions[1])\n",
    "time3 = run_chain_with_timing(chain, blog, questions[2])\n",
    "\n",
    "print(f\"First query time: {time1:.2f}s\")\n",
    "print(f\"Second query time: {time2:.2f}s\")\n",
    "print(f\"Third query time: {time3:.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual Example: Direct Message Construction\n",
    "\n",
    "Let's look at a more manual example where we construct the messages directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create messages with cache point\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that answers questions about documents.\"),\n",
    "    HumanMessage(content=[\n",
    "        {\"type\": \"text\", \"text\": f\"Here is a document: {blog[:1000]}...\"},\n",
    "        ChatBedrockConverse.create_cache_point()  # Add cache point here\n",
    "    ])\n",
    "]\n",
    "\n",
    "# First invoke to process the document (and cache it)\n",
    "start_time = time.time()\n",
    "response = llm.invoke(messages)\n",
    "print(f\"First response time (cache creation): {time.time() - start_time:.2f}s\")\n",
    "\n",
    "# Add a question\n",
    "messages.append(HumanMessage(content=\"What is the main topic of this document?\"))\n",
    "\n",
    "# Second invoke with the question (should use cache)\n",
    "start_time = time.time()\n",
    "response = llm.invoke(messages)\n",
    "print(f\"Second response time (using cache): {time.time() - start_time:.2f}s\")\n",
    "print(f\"Response: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices for Prompt Caching with LangChain\n",
    "\n",
    "Here are some best practices for using prompt caching with LangChain:\n",
    "\n",
    "1. **Include Cache Point in Message Content**: The cache point must be included directly in the message content as a special content block, not just as part of the configuration.\n",
    "\n",
    "2. **Place Cache Point After Static Content**: Place the cache point after the static content (like documents or system prompts) that you want to cache.\n",
    "\n",
    "3. **Use Consistent Cache Points**: Use the same cache point type for related requests to ensure proper caching.\n",
    "\n",
    "4. **Monitor Cache Metrics**: Check the `usage_metadata.input_token_details` field to confirm that caching is working as expected.\n",
    "\n",
    "5. **Structure Messages Properly**: Separate static content (like documents, system prompts) from dynamic content (user queries) to maximize caching benefits.\n",
    "\n",
    "6. **Consider Cache Lifetime**: Be aware that cached prompts expire after a period of inactivity (typically 24 hours)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated how to use prompt caching with LangChain's ChatBedrockConverse to improve performance when working with large documents or repetitive contexts. Key takeaways:\n",
    "\n",
    "- Prompt caching can significantly reduce response times for subsequent queries\n",
    "- The cache point must be included directly in the message content as a special content block\n",
    "- The `create_cache_point()` method makes it easy to generate cache configurations\n",
    "- Caching works well with LangChain's chains and other abstractions\n",
    "- Performance benefits are most noticeable with large documents or complex system prompts\n",
    "\n",
    "By leveraging prompt caching in your LangChain applications, you can create more responsive and efficient AI experiences while potentially reducing costs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
