{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3b87af2-6de5-4055-b410-237fff246f98",
   "metadata": {},
   "source": [
    "# Prompt Caching in Amazon Bedrock\n",
    "\n",
    "Prompt caching is a powerful feature in Amazon Bedrock that significantly reduces response latency for workloads with repetitive contexts. This notebook demonstrates how to implement prompt caching effectively for document-based chat applications.\n",
    "\n",
    "## What is Prompt Caching?\n",
    "\n",
    "Prompt caching allows you to store portions of your conversation context, enabling models to:\n",
    "- Reuse cached context instead of reprocessing inputs\n",
    "- Reduce response Time-To-First-Token (TTFT) for subsequent queries\n",
    "\n",
    "## When to Use Prompt Caching\n",
    "\n",
    "Prompt caching delivers maximum benefits for:\n",
    "- **Chat with Document**: By caching the document as input context on the first request, each user query becomes more efficient, perhaps enabling simpler architectures that avoid heavier solutions like vector databases.\n",
    "- **Coding assistants**: Reusing long code files in prompts enables near real-time inline suggestions, eliminating much of the time spent reprocessing code files.\n",
    "- **Agentic workflows**: Longer system prompts can be used to refine agent behavior without degrading the end-user experience. By caching the system prompts and complex tool definitions, the time to process each step in the agentic flow can be reduced.\n",
    "- **Few-Shot Learning**: Including numerous high-quality examples and complex instructions, such as for customer service or technical troubleshooting, can benefit from prompt caching.\n",
    "\n",
    "## Benefits of Prompt Caching\n",
    "\n",
    "- **Faster Response Times**: Avoid reprocessing the same context repeatedly\n",
    "- **Improved User Experience**: Reduced TTFT to create more natural conversations\n",
    "- **Cost Efficiency**: Potentially lower token usage by avoiding redundant processing\n",
    "\n",
    "## Implementation Example\n",
    "\n",
    "This notebook walks through a document-based chat implementation using prompt caching to demonstrate:\n",
    "1. How to properly structure cache points in your requests\n",
    "2. Performance comparisons with and without caching\n",
    "3. Best practices for cache management\n",
    "4. Measuring and optimizing cache effectiveness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78dd36cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['AWS_PROFILE'] = 'khurpas+management-Admin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8673631a-102f-49d6-a809-7ced95a81256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (1.38.17)\n",
      "Requirement already satisfied: pandas in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (2.2.5)\n",
      "Requirement already satisfied: matplotlib in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: seaborn in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: pytz in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (2025.2)\n",
      "Requirement already satisfied: botocore<1.39.0,>=1.38.17 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from boto3) (1.38.17)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.13.0,>=0.12.0 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from boto3) (0.12.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from botocore<1.39.0,>=1.38.17->boto3) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/khurpas/.pyenv/versions/3.12.0/envs/aws-samples/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade boto3 pandas numpy matplotlib seaborn pytz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "057a87ff-672a-4307-a367-e49c26d2b0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import json\n",
    "import time\n",
    "from enum import Enum\n",
    "\n",
    "# Data processing and visualization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "import seaborn as sns\n",
    "\n",
    "# AWS and external services\n",
    "import boto3\n",
    "import requests\n",
    "import hashlib\n",
    "\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "boto3_bedrock = boto3.client('bedrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82524009-78ae-4ebf-b32f-5b8c65dff472",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info:</b> You will need the latest boto3 which includes prompt caching in the Converse API.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5853d36-b884-4272-ad01-9de605fa2f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "boto3 version: 1.38.17\n"
     ]
    }
   ],
   "source": [
    "print(f\"boto3 version: {boto3.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b986e-15f7-445e-ad79-7c368dc79091",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Info:</b> This notebook uses Anthropic Claude 3.5 Haiku as an example, please make sure you have enabled the model on Bedrock\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f574b52c-563f-4db2-ab21-f3fb7f5880dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amazon.titan-tg1-large',\n",
       " 'amazon.nova-premier-v1:0:8k',\n",
       " 'amazon.nova-premier-v1:0:20k',\n",
       " 'amazon.nova-premier-v1:0:1000k',\n",
       " 'amazon.nova-premier-v1:0:mm',\n",
       " 'amazon.nova-premier-v1:0',\n",
       " 'amazon.titan-embed-g1-text-02',\n",
       " 'amazon.titan-text-lite-v1:0:4k',\n",
       " 'amazon.titan-text-lite-v1',\n",
       " 'amazon.titan-text-express-v1:0:8k',\n",
       " 'amazon.titan-text-express-v1',\n",
       " 'amazon.nova-pro-v1:0',\n",
       " 'amazon.nova-lite-v1:0',\n",
       " 'amazon.nova-micro-v1:0',\n",
       " 'amazon.titan-embed-text-v1:2:8k',\n",
       " 'amazon.titan-embed-text-v1',\n",
       " 'amazon.titan-embed-text-v2:0',\n",
       " 'amazon.titan-embed-image-v1:0',\n",
       " 'amazon.titan-embed-image-v1',\n",
       " 'amazon.titan-image-generator-v1:0',\n",
       " 'amazon.titan-image-generator-v1',\n",
       " 'amazon.titan-image-generator-v2:0',\n",
       " 'amazon.rerank-v1:0',\n",
       " 'stability.stable-diffusion-xl-v1:0',\n",
       " 'stability.stable-diffusion-xl-v1',\n",
       " 'stability.sd3-large-v1:0',\n",
       " 'stability.sd3-5-large-v1:0',\n",
       " 'stability.stable-image-core-v1:0',\n",
       " 'stability.stable-image-core-v1:1',\n",
       " 'stability.stable-image-ultra-v1:0',\n",
       " 'stability.stable-image-ultra-v1:1',\n",
       " 'anthropic.claude-3-5-sonnet-20241022-v2:0:18k',\n",
       " 'anthropic.claude-3-5-sonnet-20241022-v2:0:51k',\n",
       " 'anthropic.claude-3-5-sonnet-20241022-v2:0:200k',\n",
       " 'anthropic.claude-3-5-sonnet-20241022-v2:0',\n",
       " 'anthropic.claude-3-7-sonnet-20250219-v1:0',\n",
       " 'anthropic.claude-3-5-haiku-20241022-v1:0',\n",
       " 'anthropic.claude-instant-v1:2:100k',\n",
       " 'anthropic.claude-instant-v1',\n",
       " 'anthropic.claude-v2:0:18k',\n",
       " 'anthropic.claude-v2:0:100k',\n",
       " 'anthropic.claude-v2:1:18k',\n",
       " 'anthropic.claude-v2:1:200k',\n",
       " 'anthropic.claude-v2:1',\n",
       " 'anthropic.claude-v2',\n",
       " 'anthropic.claude-3-sonnet-20240229-v1:0:28k',\n",
       " 'anthropic.claude-3-sonnet-20240229-v1:0:200k',\n",
       " 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
       " 'anthropic.claude-3-haiku-20240307-v1:0:48k',\n",
       " 'anthropic.claude-3-haiku-20240307-v1:0:200k',\n",
       " 'anthropic.claude-3-haiku-20240307-v1:0',\n",
       " 'anthropic.claude-3-opus-20240229-v1:0:12k',\n",
       " 'anthropic.claude-3-opus-20240229-v1:0:28k',\n",
       " 'anthropic.claude-3-opus-20240229-v1:0:200k',\n",
       " 'anthropic.claude-3-opus-20240229-v1:0',\n",
       " 'anthropic.claude-3-5-sonnet-20240620-v1:0:18k',\n",
       " 'anthropic.claude-3-5-sonnet-20240620-v1:0:51k',\n",
       " 'anthropic.claude-3-5-sonnet-20240620-v1:0:200k',\n",
       " 'anthropic.claude-3-5-sonnet-20240620-v1:0',\n",
       " 'cohere.command-text-v14:7:4k',\n",
       " 'cohere.command-text-v14',\n",
       " 'cohere.command-r-v1:0',\n",
       " 'cohere.command-r-plus-v1:0',\n",
       " 'cohere.command-light-text-v14:7:4k',\n",
       " 'cohere.command-light-text-v14',\n",
       " 'cohere.embed-english-v3:0:512',\n",
       " 'cohere.embed-english-v3',\n",
       " 'cohere.embed-multilingual-v3:0:512',\n",
       " 'cohere.embed-multilingual-v3',\n",
       " 'cohere.rerank-v3-5:0',\n",
       " 'deepseek.r1-v1:0',\n",
       " 'meta.llama3-8b-instruct-v1:0',\n",
       " 'meta.llama3-70b-instruct-v1:0',\n",
       " 'meta.llama3-1-8b-instruct-v1:0:128k',\n",
       " 'meta.llama3-1-8b-instruct-v1:0',\n",
       " 'meta.llama3-1-70b-instruct-v1:0:128k',\n",
       " 'meta.llama3-1-70b-instruct-v1:0',\n",
       " 'meta.llama3-1-405b-instruct-v1:0',\n",
       " 'meta.llama3-2-11b-instruct-v1:0:128k',\n",
       " 'meta.llama3-2-11b-instruct-v1:0',\n",
       " 'meta.llama3-2-90b-instruct-v1:0:128k',\n",
       " 'meta.llama3-2-90b-instruct-v1:0',\n",
       " 'meta.llama3-2-1b-instruct-v1:0:128k',\n",
       " 'meta.llama3-2-1b-instruct-v1:0',\n",
       " 'meta.llama3-2-3b-instruct-v1:0:128k',\n",
       " 'meta.llama3-2-3b-instruct-v1:0',\n",
       " 'meta.llama3-3-70b-instruct-v1:0',\n",
       " 'meta.llama4-scout-17b-instruct-v1:0',\n",
       " 'meta.llama4-maverick-17b-instruct-v1:0',\n",
       " 'mistral.mistral-7b-instruct-v0:2',\n",
       " 'mistral.mixtral-8x7b-instruct-v0:1',\n",
       " 'mistral.mistral-large-2402-v1:0',\n",
       " 'mistral.mistral-large-2407-v1:0',\n",
       " 'mistral.pixtral-large-2502-v1:0',\n",
       " 'luma.ray-v2:0',\n",
       " 'writer.palmyra-x4-v1:0',\n",
       " 'writer.palmyra-x5-v1:0']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[models['modelId'] for models in boto3_bedrock.list_foundation_models()['modelSummaries']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5dcd14a-6c6f-4572-9793-bbe7091589f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id=\"us.anthropic.claude-3-5-haiku-20241022-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7423052e-d2bf-4576-bc5e-5c264b1f7d74",
   "metadata": {},
   "source": [
    "### Use case: Chat with document\n",
    "\n",
    "To effectively use Prompt Caching, there is a [minimum number of tokens](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html#prompt-caching-models). Thus we need to request a long doc here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b77409a9-1e59-46df-aa94-d04a46c1ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\n",
    "    'https://aws.amazon.com/blogs/aws/reduce-costs-and-latency-with-amazon-bedrock-intelligent-prompt-routing-and-prompt-caching-preview/',\n",
    "    'https://aws.amazon.com/blogs/machine-learning/enhance-conversational-ai-with-advanced-routing-techniques-with-amazon-bedrock/',\n",
    "    'https://aws.amazon.com/blogs/security/cost-considerations-and-common-options-for-aws-network-firewall-log-management/'\n",
    "]\n",
    "\n",
    "questions = [\n",
    "    'what is it about?',\n",
    "    'what are the use cases?',\n",
    "    'what is intelligent prompt routing?',\n",
    "    'what is prompt caching?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0162905-9bbe-45f4-b3f8-37cae0720a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_with_document(document, user_query, model_id, tenant_id=None):\n",
    "    instructions = (\n",
    "    \"I will provide you with a document, followed by a question about its content. \"\n",
    "    \"Your task is to analyze the document, extract relevant information, and provide \"\n",
    "    \"a comprehensive answer to the question. Please follow these detailed instructions:\"\n",
    "\n",
    "    \"\\n\\n1. Identifying Relevant Quotes:\"\n",
    "    \"\\n   - Carefully read through the entire document.\"\n",
    "    \"\\n   - Identify sections of the text that are directly relevant to answering the question.\"\n",
    "    \"\\n   - Select quotes that provide key information, context, or support for the answer.\"\n",
    "    \"\\n   - Quotes should be concise and to the point, typically no more than 2-3 sentences each.\"\n",
    "    \"\\n   - Choose a diverse range of quotes if multiple aspects of the question need to be addressed.\"\n",
    "    \"\\n   - Aim to select between 2 to 5 quotes, depending on the complexity of the question.\"\n",
    "\n",
    "    \"\\n\\n2. Presenting the Quotes:\"\n",
    "    \"\\n   - List the selected quotes under the heading 'Relevant quotes:'\"\n",
    "    \"\\n   - Number each quote sequentially, starting from [1].\"\n",
    "    \"\\n   - Present each quote exactly as it appears in the original text, enclosed in quotation marks.\"\n",
    "    \"\\n   - If no relevant quotes can be found, write 'No relevant quotes' instead.\"\n",
    "    \"\\n   - Example format:\"\n",
    "    \"\\n     Relevant quotes:\"\n",
    "    \"\\n     [1] \\\"This is the first relevant quote from the document.\\\"\"\n",
    "    \"\\n     [2] \\\"This is the second relevant quote from the document.\\\"\"\n",
    "\n",
    "    \"\\n\\n3. Formulating the Answer:\"\n",
    "    \"\\n   - Begin your answer with the heading 'Answer:' on a new line after the quotes.\"\n",
    "    \"\\n   - Provide a clear, concise, and accurate answer to the question based on the information in the document.\"\n",
    "    \"\\n   - Ensure your answer is comprehensive and addresses all aspects of the question.\"\n",
    "    \"\\n   - Use information from the quotes to support your answer, but do not repeat them verbatim.\"\n",
    "    \"\\n   - Maintain a logical flow and structure in your response.\"\n",
    "    \"\\n   - Use clear and simple language, avoiding jargon unless it's necessary and explained.\"\n",
    "\n",
    "    \"\\n\\n4. Referencing Quotes in the Answer:\"\n",
    "    \"\\n   - Do not explicitly mention or introduce quotes in your answer (e.g., avoid phrases like 'According to quote [1]').\"\n",
    "    \"\\n   - Instead, add the bracketed number of the relevant quote at the end of each sentence or point that uses information from that quote.\"\n",
    "    \"\\n   - If a sentence or point is supported by multiple quotes, include all relevant quote numbers.\"\n",
    "    \"\\n   - Example: 'The company's revenue grew by 15% last year. [1] This growth was primarily driven by increased sales in the Asian market. [2][3]'\"\n",
    "\n",
    "    \"\\n\\n5. Handling Uncertainty or Lack of Information:\"\n",
    "    \"\\n   - If the document does not contain enough information to fully answer the question, clearly state this in your answer.\"\n",
    "    \"\\n   - Provide any partial information that is available, and explain what additional information would be needed to give a complete answer.\"\n",
    "    \"\\n   - If there are multiple possible interpretations of the question or the document's content, explain this and provide answers for each interpretation if possible.\"\n",
    "\n",
    "    \"\\n\\n6. Maintaining Objectivity:\"\n",
    "    \"\\n   - Stick to the facts presented in the document. Do not include personal opinions or external information not found in the text.\"\n",
    "    \"\\n   - If the document presents biased or controversial information, note this objectively in your answer without endorsing or refuting the claims.\"\n",
    "\n",
    "    \"\\n\\n7. Formatting and Style:\"\n",
    "    \"\\n   - Use clear paragraph breaks to separate different points or aspects of your answer.\"\n",
    "    \"\\n   - Employ bullet points or numbered lists if it helps to organize information more clearly.\"\n",
    "    \"\\n   - Ensure proper grammar, punctuation, and spelling throughout your response.\"\n",
    "    \"\\n   - Maintain a professional and neutral tone throughout your answer.\"\n",
    "\n",
    "    \"\\n\\n8. Length and Depth:\"\n",
    "    \"\\n   - Provide an answer that is sufficiently detailed to address the question comprehensively.\"\n",
    "    \"\\n   - However, avoid unnecessary verbosity. Aim for clarity and conciseness.\"\n",
    "    \"\\n   - The length of your answer should be proportional to the complexity of the question and the amount of relevant information in the document.\"\n",
    "\n",
    "    \"\\n\\n9. Dealing with Complex or Multi-part Questions:\"\n",
    "    \"\\n   - For questions with multiple parts, address each part separately and clearly.\"\n",
    "    \"\\n   - Use subheadings or numbered points to break down your answer if necessary.\"\n",
    "    \"\\n   - Ensure that you've addressed all aspects of the question in your response.\"\n",
    "\n",
    "    \"\\n\\n10. Concluding the Answer:\"\n",
    "    \"\\n    - If appropriate, provide a brief conclusion that summarizes the key points of your answer.\"\n",
    "    \"\\n    - If the question asks for recommendations or future implications, include these based strictly on the information provided in the document.\"\n",
    "\n",
    "    \"\\n\\nRemember, your goal is to provide a clear, accurate, and well-supported answer based solely on the content of the given document. \"\n",
    "    \"Adhere to these instructions carefully to ensure a high-quality response that effectively addresses the user's query.\"\n",
    "    )\n",
    "\n",
    "    if tenant_id:\n",
    "        sha256_hash = hashlib.sha256(tenant_id.encode()).hexdigest()\n",
    "        instructions = f\"{sha256_hash}:{instructions}\"\n",
    "        \n",
    "    document_content =  f\"Here is the document:  <document> {document} </document>\"\n",
    "\n",
    "    messages_body = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {\n",
    "                'text': instructions\n",
    "                },\n",
    "                {\n",
    "                'text': document_content\n",
    "                },\n",
    "                {\n",
    "                \"cachePoint\": {\n",
    "                    \"type\": \"default\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                'text': user_query\n",
    "                },\n",
    "            ]\n",
    "        },\n",
    "    ]\n",
    "\n",
    "    inference_config={\n",
    "        'maxTokens': 500,\n",
    "        'temperature': 0,\n",
    "        'topP': 1\n",
    "    }\n",
    "\n",
    "    response = bedrock_runtime.converse(\n",
    "                messages=messages_body,\n",
    "                modelId=model_id,\n",
    "                inferenceConfig=inference_config\n",
    "            )\n",
    "\n",
    "    output_message = response[\"output\"][\"message\"]\n",
    "    response_text = output_message[\"content\"][0][\"text\"]\n",
    "\n",
    "    print(\"Response text:\")\n",
    "    print(response_text)\n",
    "\n",
    "    print(\"Usage:\")\n",
    "    print(json.dumps(response[\"usage\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d725b9-0880-4d4b-9933-d41ad2a34a13",
   "metadata": {},
   "source": [
    "### Converse API with Prompt Caching\n",
    "\n",
    "**First invocation**\n",
    "\n",
    "When you first use the Converse API with prompt caching enabled, you'll initiate the cache creation process (indicated by **cacheWriteInputTokens** in the response). If you provide a tenant_id, it will be securely hashed and included as a prefix in the instructions, ensuring tenant-specific cache isolation. This is what happens during the first invocation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed1b6fc1-79d3-47ef-af52-cd4977cb4f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text:\n",
      "Relevant quotes:\n",
      "[1] \"Today, Amazon Bedrock has introduced in preview two capabilities that help reduce costs and latency for generative AI applications:\"\n",
      "\n",
      "[2] \"Amazon Bedrock Intelligent Prompt Routing – When invoking a model, you can now use a combination of foundation models (FMs) from the same model family to help optimize for quality and cost.\"\n",
      "\n",
      "[3] \"Amazon Bedrock now supports prompt caching – You can now cache frequently used context in prompts across multiple model invocations.\"\n",
      "\n",
      "Answer:\n",
      "The article discusses two new preview features for Amazon Bedrock that aim to improve generative AI applications:\n",
      "\n",
      "1. Intelligent Prompt Routing: This feature allows routing requests between different models in the same model family (like Anthropic's Claude or Meta Llama) to optimize performance and cost. The system intelligently selects the most appropriate model based on the complexity of the prompt, potentially reducing costs by up to 30% without compromising accuracy.\n",
      "\n",
      "2. Prompt Caching: This capability allows caching frequently used context in prompts, which is particularly useful for applications like document Q&A or coding assistants. By caching intermediate results, the feature can reduce costs by up to 90% and latency by up to 85% for supported models.\n",
      "\n",
      "These capabilities are designed to help developers build more cost-effective and high-performing generative AI applications by intelligently managing model selection and reducing redundant processing. The features are currently in preview and available in select AWS regions.\n",
      "Usage:\n",
      "{\n",
      "  \"inputTokens\": 9,\n",
      "  \"outputTokens\": 329,\n",
      "  \"totalTokens\": 37583,\n",
      "  \"cacheReadInputTokens\": 0,\n",
      "  \"cacheWriteInputTokens\": 37245\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(topics[0])\n",
    "blog = response.text\n",
    "\n",
    "chat_with_document(\n",
    "    document=blog,\n",
    "    user_query=questions[0],\n",
    "    model_id=model_id,\n",
    "    tenant_id=\"tenant1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe28e92-e26c-4838-9340-b7c792279689",
   "metadata": {},
   "source": [
    "**Subsequent invocations**\n",
    "\n",
    "When you submit a different question about the same document, the LLM retrieves context from the cache using the hashed tenant_id as part of the cache key to ensure tenant specific isolation instead of reprocessing the entire document. You can observe the **cacheReadInputTokens** metric in the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34aa4f53-df86-4499-8cc1-9a4d3f51bd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text:\n",
      "Relevant quotes:\n",
      "\n",
      "[1] \"This is particularly useful for applications such as customer service assistants, where uncomplicated queries can be handled by smaller, faster, and more cost-effective models, and complex queries are routed to more capable models.\"\n",
      "\n",
      "[2] \"This is especially valuable for applications that repeatedly use the same context, such as document Q&A systems where users ask multiple questions about the same document or coding assistants that need to maintain context about code files.\"\n",
      "\n",
      "Answer:\n",
      "\n",
      "The article highlights several key use cases for Amazon Bedrock's Intelligent Prompt Routing and prompt caching:\n",
      "\n",
      "1. Customer Service Assistants\n",
      "- Route simple queries to smaller, more cost-effective models\n",
      "- Route complex queries to more advanced models\n",
      "- Optimize both performance and cost\n",
      "\n",
      "2. Document Q&A Systems\n",
      "- Cache context when users ask multiple questions about the same document\n",
      "- Reduce latency and costs by reusing previously processed document information\n",
      "\n",
      "3. Coding Assistants\n",
      "- Maintain context about code files across multiple interactions\n",
      "- Efficiently handle code-related queries by preserving previous context\n",
      "\n",
      "The key benefits for these use cases include:\n",
      "- Reducing costs (up to 30% for prompt routing)\n",
      "- Reducing latency (up to 85% with prompt caching)\n",
      "- Optimizing model selection based on query complexity\n",
      "- Improving overall application performance and efficiency\n",
      "\n",
      "These capabilities are particularly valuable for generative AI applications that require repeated context processing or handle queries of varying complexity.\n",
      "Usage:\n",
      "{\n",
      "  \"inputTokens\": 10,\n",
      "  \"outputTokens\": 323,\n",
      "  \"totalTokens\": 37578,\n",
      "  \"cacheReadInputTokens\": 37245,\n",
      "  \"cacheWriteInputTokens\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chat_with_document(\n",
    "    document=blog,\n",
    "    user_query=questions[1],\n",
    "    model_id=model_id,\n",
    "     tenant_id=\"tenant1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9742ace",
   "metadata": {},
   "source": [
    "This cell runs a test using a different tenant_id value. By providing a new tenant identifier, we ensure that the cache is isolated per tenant demonstrating that responses and cache entries are kept separate for each tenant, as the hashed tenant_id is used to prefix the instructions and form a unique cache key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e6f8500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text:\n",
      "Relevant quotes:\n",
      "\n",
      "[1] \"Today, Amazon Bedrock has introduced in preview two capabilities that help reduce costs and latency for generative AI applications:\"\n",
      "\n",
      "[2] \"Amazon Bedrock Intelligent Prompt Routing – When invoking a model, you can now use a combination of foundation models (FMs) from the same model family to help optimize for quality and cost.\"\n",
      "\n",
      "[3] \"Amazon Bedrock now supports prompt caching – You can now cache frequently used context in prompts across multiple model invocations.\"\n",
      "\n",
      "Answer:\n",
      "\n",
      "The article is about two new preview features for Amazon Bedrock that aim to improve the performance and cost-efficiency of generative AI applications:\n",
      "\n",
      "1. Intelligent Prompt Routing: This feature allows users to intelligently route prompts between different models in the same model family (like Anthropic's Claude or Meta's Llama) based on the complexity of the prompt. By selecting the most appropriate model for each request, it can reduce costs by up to 30% without compromising accuracy. For example, simpler queries can be handled by smaller, faster, and more cost-effective models, while complex queries are routed to more capable models. [1][2]\n",
      "\n",
      "2. Prompt Caching: This capability allows users to cache frequently used context in prompts across multiple model invocations. It is particularly useful for applications like document Q&A systems or coding assistants that repeatedly use the same context. The cached context remains available for up to 5 minutes after each access. Prompt caching can potentially reduce costs by up to 90% and latency by up to 85% for supported models. [1][3]\n",
      "\n",
      "These features are currently in preview and are designed to help developers build more cost-effective and high-performing generative AI applications by optimizing model selection and reducing redundant processing.\n",
      "Usage:\n",
      "{\n",
      "  \"inputTokens\": 9,\n",
      "  \"outputTokens\": 398,\n",
      "  \"totalTokens\": 37646,\n",
      "  \"cacheReadInputTokens\": 0,\n",
      "  \"cacheWriteInputTokens\": 37239\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chat_with_document(\n",
    "    document=blog,\n",
    "    user_query=questions[0],\n",
    "    model_id=model_id,\n",
    "     tenant_id=\"tenant2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a92762",
   "metadata": {},
   "source": [
    "In this cell, we run an additional test using the same tenant_id but with a different question about the same document. This demonstrates that prompt caching is functioning correctly for this tenant: since the cache is keyed by the hashed tenant_id and the document, the model retrieves context from the cache (as indicated by the cacheReadInputTokens metric) rather than reprocessing the document. This confirms that cache entries are isolated per tenant, and that subsequent queries for the same tenant and document benefit from reduced latency and token usage due to prompt caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5fec904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response text:\n",
      "Relevant quotes:\n",
      "\n",
      "[1] \"This is particularly useful for applications such as customer service assistants, where uncomplicated queries can be handled by smaller, faster, and more cost-effective models, and complex queries are routed to more capable models.\"\n",
      "\n",
      "[2] \"This is especially valuable for applications that repeatedly use the same context, such as document Q&A systems where users ask multiple questions about the same document or coding assistants that need to maintain context about code files.\"\n",
      "\n",
      "Answer:\n",
      "\n",
      "The article highlights several key use cases for Amazon Bedrock's Intelligent Prompt Routing and prompt caching:\n",
      "\n",
      "1. Customer Service Assistants\n",
      "- Route simple queries to smaller, more cost-effective models\n",
      "- Route complex queries to more advanced, capable models\n",
      "- Optimize both performance and cost\n",
      "\n",
      "2. Document Q&A Systems\n",
      "- Cache context from documents\n",
      "- Allow multiple follow-up questions about the same document\n",
      "- Reduce latency and costs for repeated queries about the same source material\n",
      "\n",
      "3. Coding Assistants\n",
      "- Maintain context about code files\n",
      "- Enable more efficient interactions by preserving previous context\n",
      "- Reduce computational overhead for repeated code-related queries\n",
      "\n",
      "The key benefits across these use cases include:\n",
      "- Reducing costs (up to 30% for prompt routing)\n",
      "- Reducing latency (up to 85% with prompt caching)\n",
      "- Optimizing model selection based on query complexity\n",
      "- Improving overall application performance and efficiency\n",
      "\n",
      "These capabilities make generative AI applications more economical and responsive by intelligently managing computational resources and context.\n",
      "Usage:\n",
      "{\n",
      "  \"inputTokens\": 10,\n",
      "  \"outputTokens\": 336,\n",
      "  \"totalTokens\": 37585,\n",
      "  \"cacheReadInputTokens\": 37239,\n",
      "  \"cacheWriteInputTokens\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chat_with_document(\n",
    "    document=blog,\n",
    "    user_query=questions[1],\n",
    "    model_id=model_id,\n",
    "     tenant_id=\"tenant2\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ae3bf18c-e87e-47a5-91a8-c781fd785e63",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook expolored Amazon Bedrock's prompt caching feature, demonstrating how it works, when to use it, and how to use it effectively. It's important to carefully evaluate whether your use case will benefit from this feature. It depends on thoughtful prompt structuring, understanding the distinction between static and dynamic content, and selecting appropriate caching strategies for your specific needs.\n",
    "\n",
    "For more information about working with prompt caching on Amazon Bedrock, see the [Amazon Bedrock User Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/prompt-caching.html). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws-samples",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
