{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e9c4ddc-72f1-4338-b814-e5362fb6ed39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.11/site-packages (2.232.2)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.11/site-packages (3.0.2)\n",
      "Collecting chess\n",
      "  Using cached chess-1.11.1-py3-none-any.whl\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.34.142 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.35.46)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: docker in /opt/conda/lib/python3.11/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (6.10.0)\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.21.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (24.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.2.2)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.3.2)\n",
      "Requirement already satisfied: platformdirs in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.3.6)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.25.3)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from sagemaker) (5.9.8)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.32.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.0.10)\n",
      "Requirement already satisfied: sagemaker-mlflow in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.1.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.11/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (2.0.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.11/site-packages (from sagemaker) (4.66.5)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /opt/conda/lib/python3.11/site-packages (from sagemaker) (1.26.19)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.11/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.11/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/conda/lib/python3.11/site-packages (from datasets) (0.24.5)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.46 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.35.46)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/conda/lib/python3.11/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (0.10.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.11/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.20.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->sagemaker) (2024.7.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=1.7.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (1.10.17)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (13.7.1)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /opt/conda/lib/python3.11/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from jsonschema->sagemaker) (0.20.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.11/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (1.7.6.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /opt/conda/lib/python3.11/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: mlflow>=2.8 in /opt/conda/lib/python3.11/site-packages (from sagemaker-mlflow->sagemaker) (2.15.1)\n",
      "Requirement already satisfied: Flask<4 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.0.3)\n",
      "Requirement already satisfied: alembic!=1.10.0,<2 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.13.2)\n",
      "Requirement already satisfied: cachetools<6,>=5.0.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (5.5.0)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (8.1.7)\n",
      "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.30.0)\n",
      "Requirement already satisfied: entrypoints<1 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.4)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.1.43)\n",
      "Requirement already satisfied: graphene<4 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.3)\n",
      "Requirement already satisfied: markdown<4,>=3.3 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.6)\n",
      "Requirement already satisfied: matplotlib<4 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.9.2)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.26.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.26.0)\n",
      "Requirement already satisfied: querystring-parser<2 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.2.4)\n",
      "Requirement already satisfied: scikit-learn<2 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.4.2)\n",
      "Requirement already satisfied: scipy<2 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.12.0)\n",
      "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.0.30)\n",
      "Requirement already satisfied: sqlparse<1,>=0.4.0 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.5.1)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.1.4)\n",
      "Requirement already satisfied: gunicorn<23 in /opt/conda/lib/python3.11/site-packages (from mlflow>=2.8->sagemaker-mlflow->sagemaker) (22.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.11/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (2.18.0)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.11/site-packages (from alembic!=1.10.0,<2->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.3.5)\n",
      "Requirement already satisfied: google-auth~=2.0 in /opt/conda/lib/python3.11/site-packages (from databricks-sdk<1,>=0.20.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.33.0)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.0.3)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.2.0)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /opt/conda/lib/python3.11/site-packages (from Flask<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.8.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.11/site-packages (from gitpython<4,>=3.1.9->mlflow>=2.8->sagemaker-mlflow->sagemaker) (4.0.11)\n",
      "Requirement already satisfied: graphql-core<3.3,>=3.1 in /opt/conda/lib/python3.11/site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.2.3)\n",
      "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /opt/conda/lib/python3.11/site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.2.0)\n",
      "Requirement already satisfied: aniso8601<10,>=8 in /opt/conda/lib/python3.11/site-packages (from graphene<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (9.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from Jinja2<4,>=2.11->mlflow>=2.8->sagemaker-mlflow->sagemaker) (2.1.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.11/site-packages (from matplotlib<4->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.1.2)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.2.14)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.47b0 in /opt/conda/lib/python3.11/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.47b0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn<2->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.5.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.11/site-packages (from sqlalchemy<3,>=1.4.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (3.0.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.11/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow>=2.8->sagemaker-mlflow->sagemaker) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.11/site-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow>=2.8->sagemaker-mlflow->sagemaker) (0.6.0)\n",
      "Installing collected packages: chess\n",
      "Successfully installed chess-1.11.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade sagemaker datasets chess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "656af867-994c-4216-a063-b714f7f2777c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>Jupyter.notebook.kernel.restart()</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1925d8cc-e1b9-4c91-97ca-0f731d027051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Sagemaker Version - 2.232.2\n",
      "Transformers version - 4.38.2\n",
      "sagemaker role arn: arn:aws:iam::769977401909:role/service-role/SageMaker-ExecutionRole-20231218T134780\n",
      "sagemaker bucket: sagemaker-us-east-1-769977401909\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import transformers\n",
    "print(f\"Sagemaker Version - {sagemaker.__version__}\")\n",
    "print(f\"Transformers version - {transformers.__version__}\")\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    " \n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    " \n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    " \n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ed25e4e1-1e1f-40b0-ac9f-019855a01b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-3-1-8b\", \"2.2.2\"\n",
    "train_test_data_location = f's3://{sess.default_bucket()}/datasets/chess-world-fide-championship'\n",
    "local_train_data_file = \"data/train.jsonl\"\n",
    "local_test_data_file = \"data/validation.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d70e7371-2a82-4ff4-abfb-b92adf381d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def interactive_sleep(seconds: int):\n",
    "    dots = ''\n",
    "    for i in range(seconds):\n",
    "        dots += '.'\n",
    "        print(dots, end='\\r')\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "44f63df4-45ae-46fc-985f-b27d4cefcc10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No instance type selected for inference hosting endpoint. Defaulting to ml.g5.4xlarge.\n",
      "INFO:sagemaker.jumpstart:No instance type selected for inference hosting endpoint. Defaulting to ml.g5.4xlarge.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "pretrained_model = JumpStartModel(model_id=model_id, model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b2bb3ed4-bea4-4c7e-b931-237614029c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File being processed - data/pgn/FideChamp1999.pgn\n",
      "File being processed - data/pgn/FideChamp2002.pgn\n",
      "File being processed - data/pgn/WorldChamp2006.pgn\n",
      "File being processed - data/pgn/WorldChamp2013.pgn\n",
      "File being processed - data/pgn/WorldChamp2004.pgn\n",
      "File being processed - data/pgn/FideChamp2004.pgn\n",
      "File being processed - data/pgn/WorldChamp2000.pgn\n",
      "File being processed - data/pgn/FideChamp2000.pgn\n",
      "File being processed - data/pgn/WorldChamp2008.pgn\n",
      "File being processed - data/pgn/WorldChamp2012.pgn\n",
      "File being processed - data/pgn/WorldChamp2021.pgn\n",
      "File being processed - data/pgn/FideChamp1996.pgn\n",
      "File being processed - data/pgn/WorldChamp2016.pgn\n",
      "File being processed - data/pgn/WorldChamp2007.pgn\n",
      "File being processed - data/pgn/WorldChamp2014.pgn\n",
      "File being processed - data/pgn/WorldChamp2023.pgn\n",
      "File being processed - data/pgn/FideChamp2005.pgn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:chess.pgn:illegal san: 'Bf4' in 8/8/R1p5/1p1k4/1Pr5/1K6/8/8 w - - 18 115 while parsing <Game at 0x7f7d4ddade90 ('Caruana, Fabiano' vs. 'Carlsen, Magnus', '2018.11.09' at 'London ENG')>\n",
      "ERROR:chess.pgn:illegal san: 'd4' in 8/8/R1p5/1p1k4/1Pr5/1K6/8/8 w - - 18 115 while parsing <Game at 0x7f7d4ddade90 ('Caruana, Fabiano' vs. 'Carlsen, Magnus', '2018.11.09' at 'London ENG', 1 errors)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File being processed - data/pgn/WorldChamp2010.pgn\n",
      "File being processed - data/pgn/WorldChamp2018.pgn\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a30ae6defb4ea19529fbb6ffdb5c31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema for dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'context', 'response'],\n",
      "        num_rows: 105373\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'context', 'response'],\n",
      "        num_rows: 45161\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f076fbb3f19942d78afa81e8df9f1bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/106 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0da029ec8434d0292d4b997812e034d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/46 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "33623397"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chess.pgn\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "instruction = '''In the FEN Lowercase letters describe the black pieces. \"p\" stands for pawn, \"r\" for rook, \"n\" for knight, \"b\" for bishop, \"q\" for queen, and \"k\" for king.\n",
    "The same letters are used for the white pieces, but they appear in uppercase.\n",
    "Empty squares are denoted by numbers from one to eight, depending on how many empty squares are between two pieces.\n",
    "Use the FEN to understand the position of the pieces on the chessboard and recommend legal moves accordingly and follow the rules of playing chess to recommend legal moves.'''\n",
    "\n",
    "pathlist = Path(\"data/pgn/\").glob('**/*.pgn')\n",
    "s = \",\"\n",
    "with open(\"data/pgn/data.json\", 'w') as f:\n",
    "    for path in pathlist:\n",
    "        print(f'File being processed - {path}')\n",
    "        pgn = open(path)\n",
    "        while True:\n",
    "            game = chess.pgn.read_game(pgn)\n",
    "            if game is None:\n",
    "                break\n",
    "            else:\n",
    "                event = game.headers[\"Event\"]\n",
    "                white_player = game.headers[\"White\"]\n",
    "                black_player = game.headers[\"Black\"]\n",
    "                board = game.board()\n",
    "                prev_moves = []\n",
    "                move_cnt = 0\n",
    "                for move in game.mainline_moves():\n",
    "                    move_cnt += 1\n",
    "                    next_move_color= \"WHITE\" if board.turn else \"BLACK\"\n",
    "                    move_json = {\n",
    "                        \"instruction\": instruction,\n",
    "                        \"context\": f'''You are a chess grandmaster. You are playing {next_move_color} color and the current chessboard FEN is {board.fen()}. ''',\n",
    "                        \"response\":move.uci()\n",
    "                    }\n",
    "                    prev_moves.append(f\"{move.uci()}\")\n",
    "                    board.push(move)\n",
    "                    f.write(json.dumps(move_json) + \"\\n\")\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=f\"{path.cwd()}/data/pgn/data.json\", split=\"train[:100%]\")\n",
    "dataset = dataset.train_test_split(test_size=0.3)\n",
    "print(f'Schema for dataset: {dataset}')\n",
    "\n",
    "dataset = dataset.shuffle()\n",
    "columns_to_remove = list(dataset[\"train\"].features)\n",
    "\n",
    "dataset[\"train\"].to_json(local_train_data_file, orient=\"records\", force_ascii=False)\n",
    "dataset[\"test\"].to_json(local_test_data_file, orient=\"records\", force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "79f5c469-977d-4d9e-aa7e-57563a8e32b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": (\n",
    "        \"What is your next move in UCI notation. \"\n",
    "        \"Provide only the UCI notation for your next move. Your next move should be a valid legal move only for the color you are playing.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\"\n",
    "    ),\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "\n",
    "with open(\"data/template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "76d50d9e-79a1-4a7d-aee8-d4dba26b645d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: s3://sagemaker-us-east-1-769977401909/datasets/chess-world-fide-championship\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "S3Uploader.upload(local_train_data_file, train_test_data_location)\n",
    "S3Uploader.upload(local_test_data_file, train_test_data_location)\n",
    "S3Uploader.upload(\"data/template.json\", train_test_data_location)\n",
    "\n",
    "print(f\"Training data: {train_test_data_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077696ae-c7aa-4aed-9998-94478f72614d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: meta-textgeneration-llama-3-1-8b-2024-10-23-23-03-38-240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-23 23:03:40 Starting - Starting the training job\n",
      "2024-10-23 23:03:40 Pending - Training job waiting for capacity......\n",
      "2024-10-23 23:04:33 Pending - Preparing the instances for training......\n",
      "2024-10-23 23:05:17 Downloading - Downloading input data..............................\n",
      "2024-10-23 23:10:35 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2024-10-23 23:10:37,977 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-10-23 23:10:38,014 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-23 23:10:38,024 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-10-23 23:10:38,026 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-10-23 23:10:47,484 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mProcessing ./lib/accelerate/accelerate-0.33.0-py3-none-any.whl (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/bitsandbytes/bitsandbytes-0.39.1-py3-none-any.whl (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/black/black-23.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/brotli/Brotli-1.0.9-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/datasets/datasets-2.14.1-py3-none-any.whl (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/docstring-parser/docstring_parser-0.16-py3-none-any.whl (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/fire/fire-0.5.0.tar.gz\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mProcessing ./lib/huggingface-hub/huggingface_hub-0.24.2-py3-none-any.whl (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/inflate64/inflate64-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 9))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/loralib/loralib-0.1.1-py3-none-any.whl (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/multivolumefile/multivolumefile-0.2.3-py3-none-any.whl (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/mypy-extensions/mypy_extensions-1.0.0-py3-none-any.whl (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cublas-cu12/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-cupti-cu12/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 14))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-nvrtc-cu12/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cuda-runtime-cu12/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cudnn-cu12/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cufft-cu12/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-curand-cu12/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 19))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusolver-cu12/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-cusparse-cu12/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 21))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nccl-cu12/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvjitlink-cu12/nvidia_nvjitlink_cu12-12.3.101-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/nvidia-nvtx-cu12/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pathspec/pathspec-0.11.1-py3-none-any.whl (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/peft/peft-0.4.0-py3-none-any.whl (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/py7zr/py7zr-0.20.5-py3-none-any.whl (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pybcj/pybcj-1.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pycryptodomex/pycryptodomex-3.18.0-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 29))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyppmd/pyppmd-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 30))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/pyzstd/pyzstd-0.15.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 31))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/safetensors/safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 32))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/scipy/scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 33))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/shtab/shtab-1.7.1-py3-none-any.whl (from -r requirements.txt (line 34))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/termcolor/termcolor-2.3.0-py3-none-any.whl (from -r requirements.txt (line 35))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/texttable/texttable-1.6.7-py2.py3-none-any.whl (from -r requirements.txt (line 36))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenize-rt/tokenize_rt-5.1.0-py2.py3-none-any.whl (from -r requirements.txt (line 37))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tokenizers/tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 38))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/torch/torch-2.2.0-cp310-cp310-manylinux1_x86_64.whl (from -r requirements.txt (line 39))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/transformers/transformers-4.43.1-py3-none-any.whl (from -r requirements.txt (line 40))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/triton/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from -r requirements.txt (line 41))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/trl/trl-0.8.1-py3-none-any.whl (from -r requirements.txt (line 42))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/typing-extensions/typing_extensions-4.8.0-py3-none-any.whl (from -r requirements.txt (line 43))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/tyro/tyro-0.7.3-py3-none-any.whl (from -r requirements.txt (line 44))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_script_utilities/sagemaker_jumpstart_script_utilities-1.1.9-py2.py3-none-any.whl (from -r requirements.txt (line 45))\u001b[0m\n",
      "\u001b[34mProcessing ./lib/sagemaker_jumpstart_huggingface_script_utilities/sagemaker_jumpstart_huggingface_script_utilities-1.2.7-py2.py3-none-any.whl (from -r requirements.txt (line 46))\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (1.24.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (5.9.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate==0.33.0->-r requirements.txt (line 1)) (6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (8.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs>=2 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (3.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from black==23.7.0->-r requirements.txt (line 3)) (2.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (14.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (2.31.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (4.65.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.11.1->datasets==2.14.1->-r requirements.txt (line 5)) (2023.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.1->-r requirements.txt (line 5)) (3.9.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.5.0->-r requirements.txt (line 7)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub==0.24.2->-r requirements.txt (line 8)) (3.12.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (1.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.2.0->-r requirements.txt (line 39)) (3.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers==4.43.1->-r requirements.txt (line 40)) (2023.12.25)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro==0.7.3->-r requirements.txt (line 44)) (13.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.3.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (23.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (6.0.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (1.9.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.1->-r requirements.txt (line 5)) (4.0.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (3.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (1.26.15)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.1->-r requirements.txt (line 5)) (2024.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (2.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.2.0->-r requirements.txt (line 39)) (2.1.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.1->-r requirements.txt (line 5)) (2023.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.2.0->-r requirements.txt (line 39)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro==0.7.3->-r requirements.txt (line 44)) (0.1.0)\u001b[0m\n",
      "\u001b[34mscipy is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=c9d024b58127ddc74e36fa9b6d60edc189d21c5e251d739b641d5f0055ed7b0c\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/db/3d/41/7e69dca5f61e37d109a4457082ffc5c6edb55ab633bafded38\u001b[0m\n",
      "\u001b[34mSuccessfully built fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: texttable, Brotli, bitsandbytes, typing-extensions, triton, tokenize-rt, termcolor, shtab, sagemaker-jumpstart-script-utilities, sagemaker-jumpstart-huggingface-script-utilities, safetensors, pyzstd, pyppmd, pycryptodomex, pybcj, pathspec, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, multivolumefile, loralib, inflate64, docstring-parser, py7zr, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, fire, black, tyro, tokenizers, nvidia-cusolver-cu12, transformers, torch, datasets, accelerate, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: typing-extensions\u001b[0m\n",
      "\u001b[34mFound existing installation: typing_extensions 4.7.1\u001b[0m\n",
      "\u001b[34mUninstalling typing_extensions-4.7.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled typing_extensions-4.7.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: triton\u001b[0m\n",
      "\u001b[34mFound existing installation: triton 2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mUninstalling triton-2.0.0.dev20221202:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled triton-2.0.0.dev20221202\u001b[0m\n",
      "\u001b[34mAttempting uninstall: huggingface-hub\u001b[0m\n",
      "\u001b[34mFound existing installation: huggingface-hub 0.20.3\u001b[0m\n",
      "\u001b[34mUninstalling huggingface-hub-0.20.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled huggingface-hub-0.20.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: tokenizers\u001b[0m\n",
      "\u001b[34mFound existing installation: tokenizers 0.13.3\u001b[0m\n",
      "\u001b[34mUninstalling tokenizers-0.13.3:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled tokenizers-0.13.3\u001b[0m\n",
      "\u001b[34mAttempting uninstall: transformers\u001b[0m\n",
      "\u001b[34mFound existing installation: transformers 4.28.1\u001b[0m\n",
      "\u001b[34mUninstalling transformers-4.28.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled transformers-4.28.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: torch\u001b[0m\n",
      "\u001b[34mFound existing installation: torch 2.0.0\u001b[0m\n",
      "\u001b[34mUninstalling torch-2.0.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled torch-2.0.0\u001b[0m\n",
      "\u001b[34mAttempting uninstall: datasets\u001b[0m\n",
      "\u001b[34mFound existing installation: datasets 2.16.1\u001b[0m\n",
      "\u001b[34mUninstalling datasets-2.16.1:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled datasets-2.16.1\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.19.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.19.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.19.0\u001b[0m\n",
      "\u001b[34mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\u001b[0m\n",
      "\u001b[34mfastai 2.7.12 requires torch<2.1,>=1.7, but you have torch 2.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[34mSuccessfully installed Brotli-1.0.9 accelerate-0.33.0 bitsandbytes-0.39.1 black-23.7.0 datasets-2.14.1 docstring-parser-0.16 fire-0.5.0 huggingface-hub-0.24.2 inflate64-0.3.1 loralib-0.1.1 multivolumefile-0.2.3 mypy-extensions-1.0.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.3.101 nvidia-nvtx-cu12-12.1.105 pathspec-0.11.1 peft-0.4.0 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.18.0 pyppmd-1.0.0 pyzstd-0.15.9 safetensors-0.4.2 sagemaker-jumpstart-huggingface-script-utilities-1.2.7 sagemaker-jumpstart-script-utilities-1.1.9 shtab-1.7.1 termcolor-2.3.0 texttable-1.6.7 tokenize-rt-5.1.0 tokenizers-0.19.1 torch-2.2.0 transformers-4.43.1 triton-2.2.0 trl-0.8.1 typing-extensions-4.8.0 tyro-0.7.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m2024-10-23 23:11:49,833 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-10-23 23:11:49,833 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-10-23 23:11:49,891 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-23 23:11:49,939 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-23 23:11:49,987 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-23 23:11:49,997 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"code\": \"/opt/ml/input/data/code\",\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.g5.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"add_input_output_demarcation_key\": \"True\",\n",
      "        \"chat_dataset\": \"False\",\n",
      "        \"chat_template\": \"Llama3.1\",\n",
      "        \"enable_fsdp\": \"True\",\n",
      "        \"epoch\": \"3\",\n",
      "        \"instruction_tuned\": true,\n",
      "        \"int8_quantization\": \"False\",\n",
      "        \"learning_rate\": \"0.0001\",\n",
      "        \"lora_alpha\": \"32\",\n",
      "        \"lora_dropout\": \"0.05\",\n",
      "        \"lora_r\": \"8\",\n",
      "        \"max_input_length\": \"1024\",\n",
      "        \"max_train_samples\": \"-1\",\n",
      "        \"max_val_samples\": \"-1\",\n",
      "        \"per_device_eval_batch_size\": \"1\",\n",
      "        \"per_device_train_batch_size\": \"4\",\n",
      "        \"preprocessing_num_workers\": \"None\",\n",
      "        \"seed\": \"10\",\n",
      "        \"target_modules\": \"q_proj,v_proj\",\n",
      "        \"train_data_split_seed\": \"0\",\n",
      "        \"validation_split_ratio\": \"0.2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"code\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.g5.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": true,\n",
      "    \"job_name\": \"meta-textgeneration-llama-3-1-8b-2024-10-23-23-03-38-240\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"/opt/ml/input/data/code/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"transfer_learning\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 4,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"transfer_learning.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"chat_template\":\"Llama3.1\",\"enable_fsdp\":\"True\",\"epoch\":\"3\",\"instruction_tuned\":true,\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=transfer_learning.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"code\",\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.g5.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=transfer_learning\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=/opt/ml/input/data/code/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"code\":\"/opt/ml/input/data/code\",\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.g5.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"add_input_output_demarcation_key\":\"True\",\"chat_dataset\":\"False\",\"chat_template\":\"Llama3.1\",\"enable_fsdp\":\"True\",\"epoch\":\"3\",\"instruction_tuned\":true,\"int8_quantization\":\"False\",\"learning_rate\":\"0.0001\",\"lora_alpha\":\"32\",\"lora_dropout\":\"0.05\",\"lora_r\":\"8\",\"max_input_length\":\"1024\",\"max_train_samples\":\"-1\",\"max_val_samples\":\"-1\",\"per_device_eval_batch_size\":\"1\",\"per_device_train_batch_size\":\"4\",\"preprocessing_num_workers\":\"None\",\"seed\":\"10\",\"target_modules\":\"q_proj,v_proj\",\"train_data_split_seed\":\"0\",\"validation_split_ratio\":\"0.2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"code\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":true,\"job_name\":\"meta-textgeneration-llama-3-1-8b-2024-10-23-23-03-38-240\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"/opt/ml/input/data/code/sourcedir.tar.gz\",\"module_name\":\"transfer_learning\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":4,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"transfer_learning.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--add_input_output_demarcation_key\",\"True\",\"--chat_dataset\",\"False\",\"--chat_template\",\"Llama3.1\",\"--enable_fsdp\",\"True\",\"--epoch\",\"3\",\"--instruction_tuned\",\"True\",\"--int8_quantization\",\"False\",\"--learning_rate\",\"0.0001\",\"--lora_alpha\",\"32\",\"--lora_dropout\",\"0.05\",\"--lora_r\",\"8\",\"--max_input_length\",\"1024\",\"--max_train_samples\",\"-1\",\"--max_val_samples\",\"-1\",\"--per_device_eval_batch_size\",\"1\",\"--per_device_train_batch_size\",\"4\",\"--preprocessing_num_workers\",\"None\",\"--seed\",\"10\",\"--target_modules\",\"q_proj,v_proj\",\"--train_data_split_seed\",\"0\",\"--validation_split_ratio\",\"0.2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_CODE=/opt/ml/input/data/code\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_ADD_INPUT_OUTPUT_DEMARCATION_KEY=True\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_DATASET=False\u001b[0m\n",
      "\u001b[34mSM_HP_CHAT_TEMPLATE=Llama3.1\u001b[0m\n",
      "\u001b[34mSM_HP_ENABLE_FSDP=True\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCH=3\u001b[0m\n",
      "\u001b[34mSM_HP_INSTRUCTION_TUNED=true\u001b[0m\n",
      "\u001b[34mSM_HP_INT8_QUANTIZATION=False\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=0.0001\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_ALPHA=32\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_DROPOUT=0.05\u001b[0m\n",
      "\u001b[34mSM_HP_LORA_R=8\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_INPUT_LENGTH=1024\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_TRAIN_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_VAL_SAMPLES=-1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_EVAL_BATCH_SIZE=1\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=4\u001b[0m\n",
      "\u001b[34mSM_HP_PREPROCESSING_NUM_WORKERS=None\u001b[0m\n",
      "\u001b[34mSM_HP_SEED=10\u001b[0m\n",
      "\u001b[34mSM_HP_TARGET_MODULES=q_proj,v_proj\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_DATA_SPLIT_SEED=0\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_SPLIT_RATIO=0.2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 transfer_learning.py --add_input_output_demarcation_key True --chat_dataset False --chat_template Llama3.1 --enable_fsdp True --epoch 3 --instruction_tuned True --int8_quantization False --learning_rate 0.0001 --lora_alpha 32 --lora_dropout 0.05 --lora_r 8 --max_input_length 1024 --max_train_samples -1 --max_val_samples -1 --per_device_eval_batch_size 1 --per_device_train_batch_size 4 --preprocessing_num_workers None --seed 10 --target_modules q_proj,v_proj --train_data_split_seed 0 --validation_split_ratio 0.2\u001b[0m\n",
      "\u001b[34m2024-10-23 23:11:50,029 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Using pre-trained artifacts in SAGEMAKER_ADDITIONAL_S3_DATA_PATH=/opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34mINFO:root:Invoking the training command ['torchrun', '--nnodes', '1', '--nproc_per_node', '4', 'llama_finetuning.py', '--model_name', '/opt/ml/additonals3data', '--num_gpus', '4', '--pure_bf16', '--dist_checkpoint_root_folder', 'model_checkpoints', '--dist_checkpoint_folder', 'fine-tuned', '--batch_size_training', '4', '--micro_batch_size', '4', '--train_file', '/opt/ml/input/data/training', '--lr', '0.0001', '--do_train', '--output_dir', 'saved_peft_model', '--num_epochs', '3', '--use_peft', '--peft_method', 'lora', '--max_train_samples', '-1', '--max_val_samples', '-1', '--seed', '10', '--per_device_eval_batch_size', '1', '--max_input_length', '1024', '--preprocessing_num_workers', '--None', '--validation_split_ratio', '0.2', '--train_data_split_seed', '0', '--num_workers_dataloader', '0', '--weight_decay', '0.1', '--lora_r', '8', '--lora_alpha', '32', '--lora_dropout', '0.05', '--target_modules', 'q_proj,v_proj', '--chat_template', 'Llama3.1', '--enable_fsdp', '--add_input_output_demarcation_key', '--instruction_tuned'].\u001b[0m\n",
      "\u001b[34m[2024-10-23 23:11:54,672] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-10-23 23:11:54,672] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-10-23 23:11:54,672] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-10-23 23:11:54,672] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m===================================BUG REPORT===================================\u001b[0m\n",
      "\u001b[34mWelcome to bitsandbytes. For bug reports, please run\u001b[0m\n",
      "\u001b[34mpython -m bitsandbytes\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\u001b[0m\n",
      "\u001b[34m================================================================================\u001b[0m\n",
      "\u001b[34mbin /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\u001b[0m\n",
      "\u001b[34mCUDA SETUP: CUDA runtime path found: /opt/conda/lib/libcudart.so.11.0\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Highest compute capability among GPUs detected: 8.6\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Detected CUDA version 118\u001b[0m\n",
      "\u001b[34mCUDA SETUP: Loading binary /opt/conda/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 2. Rank is 2\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 2\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 0. Rank is 0\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 0\u001b[0m\n",
      "\u001b[34m--> Running with torch dist debug set to detail\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 1. Rank is 1\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 1\u001b[0m\n",
      "\u001b[34mINFO:root:Local rank is 3. Rank is 3\u001b[0m\n",
      "\u001b[34mINFO:root:Setting torch device = 3\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mDownloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mDownloading data files: 100%|██████████| 1/1 [00:00<00:00, 11715.93it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mExtracting data files: 100%|██████████| 1/1 [00:00<00:00, 1164.11it/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the tokenizer.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \u001b[0m\n",
      "\u001b[34mThe tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \u001b[0m\n",
      "\u001b[34mThe class this function is called from is 'LlamaTokenizerFast'.\u001b[0m\n",
      "\u001b[34mGenerating train split: 28167 examples [00:00, 165048.83 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 56339 examples [00:00, 175547.41 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the data.\u001b[0m\n",
      "\u001b[34mGenerating train split: 84509 examples [00:00, 180119.67 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 119458 examples [00:00, 182926.93 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 147628 examples [00:00, 183475.40 examples/s]\u001b[0m\n",
      "\u001b[34mGenerating train split: 150534 examples [00:00, 180954.50 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/150534 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/150534 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/150534 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Adding demarkation key ### Response:\n",
      " between input and output.\u001b[0m\n",
      "\u001b[34mINFO:root:Test data is not identified. Splitting the data into train and test data respectively.\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/150534 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|          | 1783/150534 [00:00<00:08, 17728.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|          | 1761/150534 [00:00<00:08, 17496.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|          | 1759/150534 [00:00<00:08, 17487.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|          | 1793/150534 [00:00<00:08, 17827.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 3641/150534 [00:00<00:08, 18219.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 3567/150534 [00:00<00:08, 17825.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 3544/150534 [00:00<00:08, 17694.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 3631/150534 [00:00<00:08, 18145.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▎         | 5478/150534 [00:00<00:07, 18287.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▎         | 5374/150534 [00:00<00:08, 17929.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▎         | 5351/150534 [00:00<00:08, 17864.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▎         | 5486/150534 [00:00<00:07, 18326.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▍         | 7331/150534 [00:00<00:07, 18377.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▍         | 7159/150534 [00:00<00:07, 17945.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▍         | 7337/150534 [00:00<00:07, 18396.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▌         | 8054/150534 [00:00<00:07, 17890.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   6%|▌         | 8989/150534 [00:00<00:07, 18070.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 10071/150534 [00:00<00:07, 18319.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 9876/150534 [00:00<00:07, 17994.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 10089/150534 [00:00<00:07, 18371.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 10797/150534 [00:00<00:07, 18071.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   8%|▊         | 11942/150534 [00:00<00:07, 18364.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   8%|▊         | 11679/150534 [00:00<00:07, 18002.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   8%|▊         | 11992/150534 [00:00<00:07, 18569.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▉         | 13507/150534 [00:00<00:07, 18066.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 14701/150534 [00:00<00:07, 18368.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 14355/150534 [00:00<00:07, 17932.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 14748/150534 [00:00<00:07, 18487.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 16550/150534 [00:00<00:07, 18400.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 16189/150534 [00:00<00:07, 17994.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█▏        | 17023/150534 [00:00<00:07, 17873.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 17495/150534 [00:00<00:07, 18421.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 18394/150534 [00:01<00:07, 18407.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 18000/150534 [00:01<00:07, 17968.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 20239/150534 [00:01<00:07, 18413.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 19680/150534 [00:01<00:07, 17815.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 19836/150534 [00:01<00:07, 18074.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 20230/150534 [00:01<00:07, 18351.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 22085/150534 [00:01<00:06, 18422.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 22325/150534 [00:01<00:07, 17752.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 22550/150534 [00:01<00:07, 18078.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▌        | 22989/150534 [00:01<00:06, 18363.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 23948/150534 [00:01<00:06, 18480.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 24111/150534 [00:01<00:07, 17776.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 25255/150534 [00:01<00:06, 18061.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 25737/150534 [00:01<00:06, 18347.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 26702/150534 [00:01<00:06, 18427.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 25923/150534 [00:01<00:06, 17860.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 27065/150534 [00:01<00:06, 18068.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 27580/150534 [00:01<00:06, 18363.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 29457/150534 [00:01<00:06, 18401.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▉        | 28565/150534 [00:01<00:06, 17768.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▉        | 28892/150534 [00:01<00:06, 18119.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|██        | 30310/150534 [00:01<00:06, 18303.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|██        | 30358/150534 [00:01<00:06, 17805.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██▏       | 32226/150534 [00:01<00:06, 18418.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██        | 31602/150534 [00:01<00:06, 18096.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██▏       | 32161/150534 [00:01<00:06, 18349.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██▏       | 32152/150534 [00:01<00:06, 17839.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 34000/150534 [00:01<00:06, 18355.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 35000/150534 [00:01<00:06, 18393.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 33955/150534 [00:01<00:06, 17891.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 34306/150534 [00:01<00:06, 18071.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 35850/150534 [00:01<00:06, 18391.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 36861/150534 [00:02<00:06, 18443.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 36633/150534 [00:02<00:06, 17874.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 37012/150534 [00:02<00:06, 18058.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  26%|██▌       | 38589/150534 [00:02<00:06, 18338.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  26%|██▋       | 39609/150534 [00:02<00:06, 18398.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  26%|██▌       | 38846/150534 [00:02<00:06, 18122.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  26%|██▌       | 39281/150534 [00:02<00:06, 17795.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 40434/150534 [00:02<00:05, 18362.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  28%|██▊       | 41453/150534 [00:02<00:05, 18406.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 40690/150534 [00:02<00:06, 18197.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 41067/150534 [00:02<00:06, 17809.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 43321/150534 [00:02<00:05, 18476.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  28%|██▊       | 42558/150534 [00:02<00:05, 18324.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▊       | 43191/150534 [00:02<00:05, 18362.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  28%|██▊       | 42884/150534 [00:02<00:06, 17900.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|███       | 45218/150534 [00:02<00:05, 18604.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 44398/150534 [00:02<00:05, 18344.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 44722/150534 [00:02<00:05, 18029.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 45960/150534 [00:02<00:05, 18392.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███▏      | 47091/150534 [00:02<00:05, 18633.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███▏      | 47109/150534 [00:02<00:05, 18240.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 48984/150534 [00:02<00:05, 18715.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███▏      | 47395/150534 [00:02<00:05, 17949.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 48699/150534 [00:02<00:05, 18345.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 48943/150534 [00:02<00:05, 18264.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▎      | 50538/150534 [00:02<00:05, 18354.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 51753/150534 [00:02<00:05, 18612.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 50083/150534 [00:02<00:05, 17934.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 51653/150534 [00:02<00:05, 18191.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▍      | 52378/150534 [00:02<00:05, 18364.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 51913/150534 [00:02<00:05, 17975.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▌      | 54496/150534 [00:02<00:05, 18495.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▌      | 54363/150534 [00:03<00:05, 18144.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 55119/150534 [00:03<00:05, 18328.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▋      | 54595/150534 [00:03<00:05, 17939.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 57274/150534 [00:03<00:05, 18500.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 56189/150534 [00:03<00:05, 18171.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 56984/150534 [00:03<00:05, 18406.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 59127/150534 [00:03<00:04, 18505.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 57232/150534 [00:03<00:05, 17818.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▊      | 58040/150534 [00:03<00:05, 18255.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 59737/150534 [00:03<00:04, 18386.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 61000/150534 [00:03<00:04, 18538.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 59066/150534 [00:03<00:05, 17941.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 59929/150534 [00:03<00:04, 18421.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 61610/150534 [00:03<00:04, 18470.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 62876/150534 [00:03<00:04, 18593.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|████      | 60952/150534 [00:03<00:04, 18153.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 61799/150534 [00:03<00:04, 18494.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 63489/150534 [00:03<00:04, 18552.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▎     | 65652/150534 [00:03<00:04, 18558.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 63665/150534 [00:03<00:04, 18124.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 64547/150534 [00:03<00:04, 18426.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▍     | 66243/150534 [00:03<00:04, 18479.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▍     | 67525/150534 [00:03<00:04, 18598.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▍     | 66317/150534 [00:03<00:04, 17972.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▍     | 67337/150534 [00:03<00:04, 18483.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 69016/150534 [00:03<00:04, 18478.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 70321/150534 [00:03<00:04, 18608.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 68972/150534 [00:03<00:04, 17879.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 70059/150534 [00:03<00:04, 18368.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 70890/150534 [00:03<00:04, 18539.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  49%|████▊     | 73101/150534 [00:03<00:04, 18580.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 71906/150534 [00:03<00:04, 18389.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 71588/150534 [00:04<00:04, 17738.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  49%|████▉     | 73666/150534 [00:04<00:04, 18524.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 74968/150534 [00:04<00:04, 18600.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 74648/150534 [00:04<00:04, 18348.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  49%|████▉     | 74207/150534 [00:04<00:04, 17650.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████     | 76409/150534 [00:04<00:04, 18444.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 77726/150534 [00:04<00:03, 18522.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|█████     | 75981/150534 [00:04<00:04, 17668.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████▏    | 77375/150534 [00:04<00:03, 18290.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 78298/150534 [00:04<00:03, 18548.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 80490/150534 [00:04<00:03, 18488.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 78603/150534 [00:04<00:04, 17604.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 80093/150534 [00:04<00:03, 18233.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 81062/150534 [00:04<00:03, 18503.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▌    | 83247/150534 [00:04<00:03, 18451.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 80371/150534 [00:04<00:03, 17619.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 81935/150534 [00:04<00:03, 18271.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▌    | 82946/150534 [00:04<00:03, 18536.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▍    | 82181/150534 [00:04<00:03, 17738.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  57%|█████▋    | 86004/150534 [00:04<00:03, 18426.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 84668/150534 [00:04<00:03, 18251.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  57%|█████▋    | 85702/150534 [00:04<00:03, 18476.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 84000/150534 [00:04<00:03, 17829.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 87901/150534 [00:04<00:03, 18550.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 87421/150534 [00:04<00:03, 18280.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  59%|█████▉    | 88459/150534 [00:04<00:03, 18441.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 89763/150534 [00:04<00:03, 18565.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 86668/150534 [00:04<00:03, 17809.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  59%|█████▉    | 89294/150534 [00:04<00:03, 18383.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|██████    | 90347/150534 [00:04<00:03, 18544.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 91632/150534 [00:04<00:03, 18593.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  59%|█████▉    | 89316/150534 [00:05<00:03, 17754.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 92006/150534 [00:05<00:03, 18278.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 93094/150534 [00:05<00:03, 18461.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 94405/150534 [00:05<00:03, 18549.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 92000/150534 [00:05<00:03, 17775.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 93891/150534 [00:05<00:03, 18413.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 94985/150534 [00:05<00:02, 18568.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▍   | 97188/150534 [00:05<00:02, 18548.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 93806/150534 [00:05<00:03, 17842.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 96601/150534 [00:05<00:02, 18292.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▍   | 97717/150534 [00:05<00:02, 18446.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▋   | 99981/150534 [00:05<00:02, 18567.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 96479/150534 [00:05<00:03, 17830.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▌   | 99319/150534 [00:05<00:02, 18233.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 100461/150534 [00:05<00:02, 18393.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 98276/150534 [00:05<00:02, 17861.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 102725/150534 [00:05<00:02, 18480.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 102035/150534 [00:05<00:02, 18189.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▊   | 103203/150534 [00:05<00:02, 18352.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 100982/150534 [00:05<00:02, 17918.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 105492/150534 [00:05<00:02, 18465.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 104762/150534 [00:05<00:02, 18185.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 105960/150534 [00:05<00:02, 18359.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  71%|███████▏  | 107346/150534 [00:05<00:02, 18479.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▉   | 103665/150534 [00:05<00:02, 17906.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 105461/150534 [00:05<00:02, 17916.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  71%|███████▏  | 107486/150534 [00:05<00:02, 18174.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 108704/150534 [00:05<00:02, 18334.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 110099/150534 [00:05<00:02, 18433.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  71%|███████▏  | 107272/150534 [00:06<00:02, 17962.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 110541/150534 [00:06<00:02, 18341.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 111978/150534 [00:06<00:02, 18514.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 110196/150534 [00:06<00:02, 18137.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 109964/150534 [00:06<00:02, 17951.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 113265/150534 [00:06<00:02, 18276.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▌  | 114750/150534 [00:06<00:01, 18499.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 112937/150534 [00:06<00:02, 18135.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 111761/150534 [00:06<00:02, 17954.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 116006/150534 [00:06<00:01, 18272.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 117516/150534 [00:06<00:01, 18476.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 113578/150534 [00:06<00:02, 18008.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 115652/150534 [00:06<00:01, 18122.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 117861/150534 [00:06<00:01, 18334.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 120282/150534 [00:06<00:01, 18461.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 116253/150534 [00:06<00:01, 17941.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▊  | 118366/150534 [00:06<00:01, 18110.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 120601/150534 [00:06<00:01, 18306.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 123046/150534 [00:06<00:01, 18448.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 118941/150534 [00:06<00:01, 17930.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 121064/150534 [00:06<00:01, 18070.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 123332/150534 [00:06<00:01, 18271.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 124924/150534 [00:06<00:01, 18522.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 120738/150534 [00:06<00:01, 17937.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 122908/150534 [00:06<00:01, 18152.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 125161/150534 [00:06<00:01, 18272.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▍ | 127685/150534 [00:06<00:01, 18481.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 123434/150534 [00:06<00:01, 17945.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 125621/150534 [00:06<00:01, 18128.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▍ | 127000/150534 [00:06<00:01, 18271.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▌ | 128846/150534 [00:07<00:01, 18317.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  87%|████████▋ | 130448/150534 [00:07<00:01, 18459.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▍ | 126074/150534 [00:07<00:01, 17829.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 128328/150534 [00:07<00:01, 18099.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▍ | 127918/150534 [00:07<00:01, 17945.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▋ | 130141/150534 [00:07<00:01, 18104.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  87%|████████▋ | 131580/150534 [00:07<00:01, 18279.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 133200/150534 [00:07<00:00, 18420.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 131971/150534 [00:07<00:01, 18149.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 135055/150534 [00:07<00:00, 18447.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  87%|████████▋ | 130610/150534 [00:07<00:01, 17943.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  89%|████████▉ | 134310/150534 [00:07<00:00, 18249.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████ | 136913/150534 [00:07<00:00, 18478.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  89%|████████▉ | 134671/150534 [00:07<00:00, 18093.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  89%|████████▊ | 133289/150534 [00:07<00:00, 17913.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████ | 137035/150534 [00:07<00:00, 18220.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  93%|█████████▎| 139664/150534 [00:07<00:00, 18426.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████▏| 137377/150534 [00:07<00:00, 18070.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 138875/150534 [00:07<00:00, 18261.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 135984/150534 [00:07<00:00, 17927.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▍| 142440/150534 [00:07<00:00, 18449.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  93%|█████████▎| 140090/150534 [00:07<00:00, 18072.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 141607/150534 [00:07<00:00, 18243.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 138670/150534 [00:07<00:00, 17915.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 141923/150534 [00:07<00:00, 18117.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 145186/150534 [00:07<00:00, 18401.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▌| 144344/150534 [00:07<00:00, 18241.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 141347/150534 [00:07<00:00, 17891.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 147030/150534 [00:07<00:00, 18407.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▌| 144641/150534 [00:07<00:00, 18114.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 146178/150534 [00:07<00:00, 18261.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▌| 143142/150534 [00:08<00:00, 17902.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 148910/150534 [00:08<00:00, 18502.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 144964/150534 [00:08<00:00, 17978.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 147346/150534 [00:08<00:00, 18086.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 148933/150534 [00:08<00:00, 18291.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:08<00:00, 18466.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:08<00:00, 18358.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 147625/150534 [00:08<00:00, 17893.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 150032/150534 [00:08<00:00, 18024.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/150534 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:08<00:00, 18161.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 149420/150534 [00:08<00:00, 17905.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/150534 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|          | 1000/150534 [00:00<00:18, 8067.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:08<00:00, 17867.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/150534 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|          | 1000/150534 [00:00<00:20, 7303.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|▏         | 2000/150534 [00:00<00:17, 8314.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|          | 1000/150534 [00:00<00:19, 7650.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/150534 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|▏         | 2000/150534 [00:00<00:18, 7883.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 3000/150534 [00:00<00:18, 8176.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|▏         | 2000/150534 [00:00<00:18, 7840.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 3000/150534 [00:00<00:20, 7205.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 4000/150534 [00:00<00:18, 8107.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|          | 1000/150534 [00:00<00:27, 5364.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 3000/150534 [00:00<00:18, 8126.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 4000/150534 [00:00<00:21, 6929.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 5000/150534 [00:00<00:19, 7531.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 4000/150534 [00:00<00:17, 8463.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|▏         | 2000/150534 [00:00<00:34, 4350.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▍         | 6000/150534 [00:00<00:19, 7501.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 5000/150534 [00:00<00:17, 8224.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 5000/150534 [00:00<00:20, 7075.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▍         | 6000/150534 [00:00<00:18, 8021.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▍         | 6000/150534 [00:00<00:19, 7258.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 3000/150534 [00:00<00:28, 5182.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▍         | 7000/150534 [00:00<00:19, 7450.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▍         | 7000/150534 [00:00<00:18, 7721.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▍         | 7000/150534 [00:00<00:19, 7211.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 4000/150534 [00:00<00:25, 5792.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▌         | 8000/150534 [00:01<00:24, 5827.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 5000/150534 [00:00<00:23, 6273.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▌         | 8000/150534 [00:01<00:23, 6029.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▌         | 8000/150534 [00:01<00:24, 5807.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   6%|▌         | 9000/150534 [00:01<00:23, 6136.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▍         | 6000/150534 [00:00<00:21, 6824.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   6%|▌         | 9000/150534 [00:01<00:23, 6117.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   6%|▌         | 9000/150534 [00:01<00:22, 6255.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 10000/150534 [00:01<00:21, 6499.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▍         | 7000/150534 [00:01<00:20, 7016.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 10000/150534 [00:01<00:22, 6209.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 10000/150534 [00:01<00:22, 6290.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 11000/150534 [00:01<00:21, 6477.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▌         | 8000/150534 [00:01<00:25, 5509.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 11000/150534 [00:01<00:21, 6521.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 11000/150534 [00:01<00:21, 6573.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   8%|▊         | 12000/150534 [00:01<00:20, 6733.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   8%|▊         | 12000/150534 [00:01<00:19, 6931.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   8%|▊         | 12000/150534 [00:01<00:20, 6793.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▊         | 13000/150534 [00:01<00:19, 6994.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   6%|▌         | 9000/150534 [00:01<00:24, 5758.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▊         | 13000/150534 [00:01<00:19, 6883.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▊         | 13000/150534 [00:01<00:20, 6784.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 10000/150534 [00:01<00:23, 6053.42 examples/s]#015Map:   9%|▉         | 14000/150534 [00:02<00:19, 6908.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 11000/150534 [00:01<00:21, 6386.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▉         | 14000/150534 [00:01<00:19, 6895.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 15000/150534 [00:02<00:19, 6839.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▉         | 14000/150534 [00:02<00:20, 6598.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 15000/150534 [00:02<00:19, 6921.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 16000/150534 [00:02<00:19, 7018.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   8%|▊         | 12000/150534 [00:01<00:21, 6490.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 15000/150534 [00:02<00:19, 6952.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█▏        | 17000/150534 [00:02<00:19, 6936.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 16000/150534 [00:02<00:19, 6835.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▊         | 13000/150534 [00:02<00:21, 6548.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 16000/150534 [00:02<00:19, 6849.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 18000/150534 [00:02<00:19, 6953.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▉         | 14000/150534 [00:02<00:20, 6700.07 examples/s]#015Map:  11%|█▏        | 17000/150534 [00:02<00:19, 6881.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█▏        | 17000/150534 [00:02<00:19, 6927.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 19000/150534 [00:02<00:19, 6919.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 18000/150534 [00:02<00:19, 6902.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 18000/150534 [00:02<00:19, 6722.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 15000/150534 [00:02<00:20, 6551.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 20000/150534 [00:02<00:19, 6821.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 19000/150534 [00:02<00:19, 6832.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 19000/150534 [00:02<00:19, 6814.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 16000/150534 [00:02<00:20, 6664.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 20000/150534 [00:02<00:19, 6823.24 examples/s]#015Map:  14%|█▍        | 21000/150534 [00:03<00:19, 6805.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█▏        | 17000/150534 [00:02<00:19, 6793.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 20000/150534 [00:02<00:19, 6813.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  14%|█▍        | 21000/150534 [00:03<00:18, 6861.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 22000/150534 [00:03<00:18, 6800.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  14%|█▍        | 21000/150534 [00:03<00:19, 6778.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 18000/150534 [00:02<00:19, 6750.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▌        | 23000/150534 [00:03<00:18, 6907.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 22000/150534 [00:03<00:18, 6923.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 22000/150534 [00:03<00:18, 6867.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 19000/150534 [00:03<00:19, 6870.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 24000/150534 [00:03<00:18, 6761.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▌        | 23000/150534 [00:03<00:18, 6719.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▌        | 23000/150534 [00:03<00:19, 6658.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 20000/150534 [00:03<00:19, 6630.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 25000/150534 [00:03<00:18, 6727.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 24000/150534 [00:03<00:18, 6739.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 24000/150534 [00:03<00:18, 6756.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  14%|█▍        | 21000/150534 [00:03<00:19, 6738.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 25000/150534 [00:03<00:18, 6793.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 26000/150534 [00:03<00:18, 6720.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 25000/150534 [00:03<00:18, 6749.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 22000/150534 [00:03<00:19, 6733.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 26000/150534 [00:03<00:18, 6891.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 27000/150534 [00:03<00:18, 6510.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 26000/150534 [00:03<00:19, 6539.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▌        | 23000/150534 [00:03<00:19, 6505.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 27000/150534 [00:03<00:17, 7239.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▊        | 28000/150534 [00:04<00:18, 6579.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 27000/150534 [00:04<00:18, 6599.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 24000/150534 [00:03<00:19, 6618.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▊        | 28000/150534 [00:03<00:16, 7370.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▉        | 29000/150534 [00:04<00:17, 6756.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▊        | 28000/150534 [00:04<00:18, 6706.88 examples/s]#015Map:  17%|█▋        | 25000/150534 [00:03<00:18, 6701.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▉        | 29000/150534 [00:04<00:16, 7396.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 30000/150534 [00:04<00:17, 6913.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▉        | 29000/150534 [00:04<00:17, 6912.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 26000/150534 [00:04<00:17, 6940.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 30000/150534 [00:04<00:15, 7656.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 30000/150534 [00:04<00:16, 7134.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 27000/150534 [00:04<00:17, 7136.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██        | 31000/150534 [00:04<00:16, 7066.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██        | 31000/150534 [00:04<00:15, 7881.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██        | 31000/150534 [00:04<00:17, 6889.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▊        | 28000/150534 [00:04<00:17, 6888.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██▏       | 32000/150534 [00:04<00:17, 6850.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██▏       | 32000/150534 [00:04<00:14, 7999.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██▏       | 32000/150534 [00:04<00:17, 6923.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|██▏       | 33000/150534 [00:04<00:15, 7691.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|██▏       | 33000/150534 [00:04<00:17, 6861.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▉        | 29000/150534 [00:04<00:17, 6826.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 34000/150534 [00:04<00:15, 7488.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|██▏       | 33000/150534 [00:04<00:17, 6913.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 30000/150534 [00:04<00:17, 6941.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 34000/150534 [00:04<00:16, 6882.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 35000/150534 [00:04<00:15, 7436.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 34000/150534 [00:04<00:16, 7000.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██        | 31000/150534 [00:04<00:17, 7000.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 35000/150534 [00:05<00:16, 6937.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 35000/150534 [00:05<00:16, 6981.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██▏       | 32000/150534 [00:04<00:16, 6974.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 36000/150534 [00:05<00:19, 5804.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 36000/150534 [00:05<00:20, 5476.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|██▏       | 33000/150534 [00:05<00:16, 7064.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 36000/150534 [00:05<00:20, 5697.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 37000/150534 [00:05<00:18, 6082.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 37000/150534 [00:05<00:19, 5971.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 34000/150534 [00:05<00:16, 7256.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 37000/150534 [00:05<00:19, 5924.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▌       | 38000/150534 [00:05<00:17, 6303.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▌       | 38000/150534 [00:05<00:18, 6218.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 35000/150534 [00:05<00:16, 7182.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▌       | 38000/150534 [00:05<00:17, 6276.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  26%|██▌       | 39000/150534 [00:05<00:17, 6550.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  26%|██▌       | 39000/150534 [00:05<00:17, 6533.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 36000/150534 [00:05<00:20, 5673.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  26%|██▌       | 39000/150534 [00:05<00:16, 6570.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 40000/150534 [00:05<00:16, 6788.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 40000/150534 [00:05<00:16, 6793.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 41000/150534 [00:05<00:15, 6954.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 37000/150534 [00:05<00:18, 6031.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 41000/150534 [00:06<00:15, 6944.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 40000/150534 [00:05<00:16, 6695.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  28%|██▊       | 42000/150534 [00:06<00:15, 6812.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 41000/150534 [00:06<00:16, 6638.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▌       | 38000/150534 [00:05<00:18, 6114.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  28%|██▊       | 42000/150534 [00:06<00:16, 6744.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▊       | 43000/150534 [00:06<00:15, 6899.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  26%|██▌       | 39000/150534 [00:06<00:17, 6406.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▊       | 43000/150534 [00:06<00:15, 6827.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  28%|██▊       | 42000/150534 [00:06<00:16, 6704.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 44000/150534 [00:06<00:15, 6883.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 44000/150534 [00:06<00:15, 6901.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 40000/150534 [00:06<00:16, 6518.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▊       | 43000/150534 [00:06<00:15, 6779.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 45000/150534 [00:06<00:15, 6843.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 41000/150534 [00:06<00:16, 6538.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 45000/150534 [00:06<00:15, 6726.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 44000/150534 [00:06<00:15, 6688.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 46000/150534 [00:06<00:15, 6786.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  28%|██▊       | 42000/150534 [00:06<00:16, 6529.08 examples/s]#015Map:  31%|███       | 46000/150534 [00:06<00:15, 6659.51 examples/s]#015Map:  30%|██▉       | 45000/150534 [00:06<00:15, 6618.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 47000/150534 [00:06<00:15, 6696.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 47000/150534 [00:06<00:15, 6781.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▊       | 43000/150534 [00:06<00:16, 6612.50 examples/s]#015Map:  31%|███       | 46000/150534 [00:06<00:15, 6709.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 44000/150534 [00:06<00:15, 6800.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 48000/150534 [00:06<00:15, 6764.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 48000/150534 [00:07<00:15, 6751.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 47000/150534 [00:07<00:15, 6762.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 45000/150534 [00:06<00:15, 6689.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 49000/150534 [00:07<00:15, 6595.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 48000/150534 [00:07<00:15, 6698.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 49000/150534 [00:07<00:15, 6545.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 50000/150534 [00:07<00:14, 6779.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 50000/150534 [00:07<00:14, 6845.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 46000/150534 [00:07<00:15, 6688.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 49000/150534 [00:07<00:14, 6771.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 51000/150534 [00:07<00:14, 6905.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 51000/150534 [00:07<00:14, 6970.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 47000/150534 [00:07<00:15, 6836.16 examples/s]#015Map:  33%|███▎      | 50000/150534 [00:07<00:14, 6930.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▍      | 52000/150534 [00:07<00:14, 7018.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 48000/150534 [00:07<00:14, 6951.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 51000/150534 [00:07<00:14, 6996.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▍      | 52000/150534 [00:07<00:14, 6971.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▌      | 53000/150534 [00:07<00:14, 6846.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 49000/150534 [00:07<00:14, 6838.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▌      | 53000/150534 [00:07<00:14, 6894.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▍      | 52000/150534 [00:07<00:14, 6830.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▌      | 54000/150534 [00:07<00:13, 6975.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▌      | 53000/150534 [00:07<00:14, 6957.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 50000/150534 [00:07<00:14, 6878.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▌      | 54000/150534 [00:07<00:13, 6922.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 55000/150534 [00:07<00:14, 6787.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▌      | 54000/150534 [00:08<00:14, 6859.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 51000/150534 [00:07<00:14, 6755.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 55000/150534 [00:08<00:14, 6735.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 56000/150534 [00:08<00:13, 6811.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 55000/150534 [00:08<00:13, 6833.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▍      | 52000/150534 [00:07<00:14, 6710.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 56000/150534 [00:08<00:14, 6737.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 57000/150534 [00:08<00:13, 6747.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 56000/150534 [00:08<00:13, 6782.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 57000/150534 [00:08<00:13, 6837.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▌      | 53000/150534 [00:08<00:14, 6807.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▊      | 58000/150534 [00:08<00:13, 6890.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 57000/150534 [00:08<00:13, 6962.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▊      | 58000/150534 [00:08<00:13, 6990.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▌      | 54000/150534 [00:08<00:14, 6658.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 59000/150534 [00:08<00:13, 6795.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 59000/150534 [00:08<00:13, 6876.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▊      | 58000/150534 [00:08<00:13, 6777.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 55000/150534 [00:08<00:13, 6860.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 60000/150534 [00:08<00:13, 6863.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 60000/150534 [00:08<00:13, 6705.80 examples/s]#015Map:  39%|███▉      | 59000/150534 [00:08<00:13, 6689.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 56000/150534 [00:08<00:14, 6657.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 61000/150534 [00:08<00:13, 6816.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 57000/150534 [00:08<00:13, 6959.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 61000/150534 [00:08<00:13, 6783.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 60000/150534 [00:08<00:13, 6848.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▊      | 58000/150534 [00:08<00:13, 7073.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 62000/150534 [00:09<00:12, 6960.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 62000/150534 [00:08<00:12, 6942.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 61000/150534 [00:09<00:12, 6951.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 63000/150534 [00:09<00:12, 6933.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 59000/150534 [00:08<00:13, 7000.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 62000/150534 [00:09<00:12, 6920.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 63000/150534 [00:09<00:12, 6801.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 64000/150534 [00:09<00:12, 7075.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 63000/150534 [00:09<00:12, 7090.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 60000/150534 [00:09<00:12, 7056.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 64000/150534 [00:09<00:12, 6964.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 64000/150534 [00:09<00:12, 7124.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 61000/150534 [00:09<00:12, 7100.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 65000/150534 [00:09<00:14, 5769.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 65000/150534 [00:09<00:15, 5692.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 62000/150534 [00:09<00:12, 7148.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 65000/150534 [00:09<00:14, 5840.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▍     | 66000/150534 [00:09<00:13, 6092.68 examples/s]#015Map:  44%|████▍     | 66000/150534 [00:09<00:13, 6076.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 63000/150534 [00:09<00:11, 7356.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▍     | 66000/150534 [00:09<00:13, 6063.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▍     | 67000/150534 [00:09<00:13, 6304.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 64000/150534 [00:09<00:11, 7491.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▍     | 67000/150534 [00:09<00:13, 6391.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▌     | 68000/150534 [00:10<00:12, 6668.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▌     | 68000/150534 [00:09<00:12, 6580.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▍     | 67000/150534 [00:09<00:13, 6350.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 65000/150534 [00:09<00:15, 5695.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 69000/150534 [00:10<00:12, 6699.79 examples/s]#015Map:  46%|████▌     | 69000/150534 [00:10<00:12, 6730.38 examples/s]#015Map:  45%|████▌     | 68000/150534 [00:10<00:12, 6468.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 70000/150534 [00:10<00:11, 6802.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▍     | 66000/150534 [00:10<00:14, 5971.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 70000/150534 [00:10<00:11, 6792.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 69000/150534 [00:10<00:12, 6658.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 71000/150534 [00:10<00:11, 6973.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▍     | 67000/150534 [00:10<00:13, 6283.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 71000/150534 [00:10<00:11, 6905.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 70000/150534 [00:10<00:11, 6814.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 72000/150534 [00:10<00:11, 6897.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 71000/150534 [00:10<00:11, 6905.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▌     | 68000/150534 [00:10<00:12, 6444.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 72000/150534 [00:10<00:11, 6863.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 73000/150534 [00:10<00:11, 6824.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 72000/150534 [00:10<00:11, 6833.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 69000/150534 [00:10<00:12, 6488.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 73000/150534 [00:10<00:11, 6809.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  49%|████▉     | 74000/150534 [00:10<00:11, 6843.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 73000/150534 [00:10<00:11, 6812.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  49%|████▉     | 74000/150534 [00:10<00:11, 6845.60 examples/s]#015Map:  47%|████▋     | 70000/150534 [00:10<00:12, 6556.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 75000/150534 [00:10<00:10, 6918.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  49%|████▉     | 74000/150534 [00:10<00:11, 6933.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 71000/150534 [00:10<00:11, 6784.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 75000/150534 [00:11<00:10, 6962.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|█████     | 76000/150534 [00:11<00:10, 7078.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 72000/150534 [00:10<00:11, 7016.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 75000/150534 [00:11<00:10, 7029.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|█████     | 76000/150534 [00:11<00:11, 6732.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████     | 77000/150534 [00:11<00:10, 6947.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 73000/150534 [00:11<00:11, 6805.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|█████     | 76000/150534 [00:11<00:10, 6858.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████     | 77000/150534 [00:11<00:10, 7007.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 78000/150534 [00:11<00:10, 6928.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  49%|████▉     | 74000/150534 [00:11<00:11, 6885.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████     | 77000/150534 [00:11<00:10, 6927.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 78000/150534 [00:11<00:10, 6918.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 79000/150534 [00:11<00:10, 7047.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 78000/150534 [00:11<00:10, 7051.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 79000/150534 [00:11<00:09, 7158.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 75000/150534 [00:11<00:10, 7001.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 80000/150534 [00:11<00:09, 7106.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 80000/150534 [00:11<00:09, 7159.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 79000/150534 [00:11<00:10, 7035.33 examples/s]#015Map:  50%|█████     | 76000/150534 [00:11<00:10, 7018.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 81000/150534 [00:11<00:09, 7156.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 80000/150534 [00:11<00:09, 7157.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████     | 77000/150534 [00:11<00:10, 7103.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 81000/150534 [00:11<00:09, 7164.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 82000/150534 [00:11<00:09, 7151.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 82000/150534 [00:12<00:09, 7211.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 78000/150534 [00:11<00:10, 7153.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 81000/150534 [00:11<00:09, 7145.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▌    | 83000/150534 [00:12<00:09, 7042.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 82000/150534 [00:12<00:09, 7067.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▌    | 83000/150534 [00:12<00:09, 7036.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 79000/150534 [00:11<00:10, 6855.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 84000/150534 [00:12<00:09, 6973.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 84000/150534 [00:12<00:09, 6864.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▌    | 83000/150534 [00:12<00:09, 6897.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 80000/150534 [00:12<00:10, 6868.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 85000/150534 [00:12<00:09, 6828.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 85000/150534 [00:12<00:09, 6861.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 81000/150534 [00:12<00:10, 6896.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 84000/150534 [00:12<00:09, 6796.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  57%|█████▋    | 86000/150534 [00:12<00:09, 6890.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  57%|█████▋    | 86000/150534 [00:12<00:09, 6942.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 82000/150534 [00:12<00:09, 6932.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 85000/150534 [00:12<00:09, 6870.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 87000/150534 [00:12<00:09, 6992.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 87000/150534 [00:12<00:09, 6758.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▌    | 83000/150534 [00:12<00:10, 6720.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  57%|█████▋    | 86000/150534 [00:12<00:09, 6640.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 88000/150534 [00:12<00:09, 6805.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 88000/150534 [00:12<00:09, 6729.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 84000/150534 [00:12<00:10, 6616.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 87000/150534 [00:12<00:09, 6607.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  59%|█████▉    | 89000/150534 [00:12<00:09, 6728.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  59%|█████▉    | 89000/150534 [00:13<00:09, 6661.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 88000/150534 [00:13<00:09, 6544.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 85000/150534 [00:12<00:10, 6475.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 90000/150534 [00:13<00:08, 6731.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 90000/150534 [00:13<00:09, 6607.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  59%|█████▉    | 89000/150534 [00:13<00:09, 6766.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  57%|█████▋    | 86000/150534 [00:12<00:09, 6777.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|██████    | 91000/150534 [00:13<00:08, 6770.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|██████    | 91000/150534 [00:13<00:09, 6572.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 87000/150534 [00:13<00:09, 6754.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 90000/150534 [00:13<00:09, 6654.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 92000/150534 [00:13<00:08, 6749.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 92000/150534 [00:13<00:08, 6753.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 88000/150534 [00:13<00:09, 6845.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|██████    | 91000/150534 [00:13<00:08, 6832.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  59%|█████▉    | 89000/150534 [00:13<00:08, 6996.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 92000/150534 [00:13<00:08, 6995.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 93000/150534 [00:13<00:10, 5562.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 93000/150534 [00:13<00:10, 5582.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 90000/150534 [00:13<00:08, 7139.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 93000/150534 [00:13<00:09, 5798.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 94000/150534 [00:13<00:09, 5923.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 94000/150534 [00:13<00:09, 5945.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|██████    | 91000/150534 [00:13<00:08, 7305.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 95000/150534 [00:14<00:08, 6346.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 95000/150534 [00:13<00:08, 6289.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 94000/150534 [00:13<00:09, 6021.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 92000/150534 [00:13<00:08, 7122.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 96000/150534 [00:14<00:08, 6514.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 95000/150534 [00:14<00:08, 6263.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 96000/150534 [00:14<00:08, 6298.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 93000/150534 [00:14<00:09, 5760.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 97000/150534 [00:14<00:07, 6734.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 96000/150534 [00:14<00:08, 6405.58 examples/s]#015Map:  64%|██████▍   | 97000/150534 [00:14<00:08, 6479.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 94000/150534 [00:14<00:09, 5898.34 examples/s]#015Map:  65%|██████▌   | 98000/150534 [00:14<00:07, 6630.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 98000/150534 [00:14<00:07, 6783.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 97000/150534 [00:14<00:08, 6585.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▌   | 99000/150534 [00:14<00:07, 6591.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 95000/150534 [00:14<00:09, 6045.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 98000/150534 [00:14<00:08, 6556.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▌   | 99000/150534 [00:14<00:07, 6640.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 96000/150534 [00:14<00:08, 6316.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▋   | 100000/150534 [00:14<00:07, 6691.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▌   | 99000/150534 [00:14<00:07, 6486.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▋   | 100000/150534 [00:14<00:07, 6445.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 101000/150534 [00:14<00:07, 6677.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 97000/150534 [00:14<00:08, 6331.69 examples/s]#015Map:  67%|██████▋   | 101000/150534 [00:14<00:07, 6790.63 examples/s]#015Map:  66%|██████▋   | 100000/150534 [00:14<00:07, 6698.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 102000/150534 [00:14<00:07, 6649.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 98000/150534 [00:14<00:08, 6429.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 102000/150534 [00:15<00:07, 6547.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 101000/150534 [00:15<00:08, 6147.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 103000/150534 [00:15<00:07, 6629.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▌   | 99000/150534 [00:14<00:08, 6229.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 103000/150534 [00:15<00:07, 6373.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 102000/150534 [00:15<00:07, 6419.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▉   | 104000/150534 [00:15<00:06, 6682.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▋   | 100000/150534 [00:15<00:08, 6263.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▉   | 104000/150534 [00:15<00:07, 6268.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 103000/150534 [00:15<00:07, 6310.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 105000/150534 [00:15<00:06, 7043.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 101000/150534 [00:15<00:07, 6348.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 105000/150534 [00:15<00:06, 6528.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▉   | 104000/150534 [00:15<00:07, 6578.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 106000/150534 [00:15<00:06, 7355.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 102000/150534 [00:15<00:07, 6516.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 106000/150534 [00:15<00:06, 6665.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 105000/150534 [00:15<00:07, 6448.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  71%|███████   | 107000/150534 [00:15<00:05, 7686.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  71%|███████   | 107000/150534 [00:15<00:06, 6689.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 103000/150534 [00:15<00:07, 6513.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 108000/150534 [00:15<00:05, 7737.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 106000/150534 [00:15<00:06, 6695.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▉   | 104000/150534 [00:15<00:06, 6683.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 108000/150534 [00:16<00:06, 6761.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 109000/150534 [00:15<00:05, 7572.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  71%|███████   | 107000/150534 [00:15<00:06, 6848.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 110000/150534 [00:16<00:05, 7270.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 105000/150534 [00:15<00:06, 6570.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 109000/150534 [00:16<00:06, 6637.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 108000/150534 [00:16<00:06, 6713.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 110000/150534 [00:16<00:05, 6841.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▎  | 111000/150534 [00:16<00:05, 7133.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 109000/150534 [00:16<00:06, 6775.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 106000/150534 [00:16<00:06, 6519.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▎  | 111000/150534 [00:16<00:05, 6864.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 112000/150534 [00:16<00:05, 7096.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 110000/150534 [00:16<00:05, 6888.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  71%|███████   | 107000/150534 [00:16<00:06, 6851.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 113000/150534 [00:16<00:05, 7158.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 108000/150534 [00:16<00:06, 6944.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 112000/150534 [00:16<00:05, 6759.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▎  | 111000/150534 [00:16<00:05, 6813.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▌  | 114000/150534 [00:16<00:05, 6989.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 113000/150534 [00:16<00:05, 6871.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 109000/150534 [00:16<00:06, 6806.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 112000/150534 [00:16<00:05, 6837.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▋  | 115000/150534 [00:16<00:05, 6941.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▌  | 114000/150534 [00:16<00:05, 6717.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 113000/150534 [00:16<00:05, 6741.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 110000/150534 [00:16<00:06, 6677.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 116000/150534 [00:16<00:05, 6844.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▎  | 111000/150534 [00:16<00:05, 6803.48 examples/s]#015Map:  76%|███████▋  | 115000/150534 [00:17<00:05, 6764.74 examples/s]#015Map:  76%|███████▌  | 114000/150534 [00:16<00:05, 6809.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 117000/150534 [00:17<00:05, 6704.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 112000/150534 [00:16<00:05, 6658.83 examples/s]#015Map:  76%|███████▋  | 115000/150534 [00:17<00:05, 6681.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 116000/150534 [00:17<00:05, 6638.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 118000/150534 [00:17<00:04, 6761.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 117000/150534 [00:17<00:04, 6812.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 113000/150534 [00:17<00:05, 6799.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 116000/150534 [00:17<00:05, 6796.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 118000/150534 [00:17<00:04, 6903.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 119000/150534 [00:17<00:04, 6843.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 117000/150534 [00:17<00:04, 6903.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▌  | 114000/150534 [00:17<00:05, 6853.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 119000/150534 [00:17<00:04, 6811.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▋  | 115000/150534 [00:17<00:05, 6831.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 120000/150534 [00:17<00:04, 6732.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 118000/150534 [00:17<00:04, 6778.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 121000/150534 [00:17<00:04, 6871.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 119000/150534 [00:17<00:04, 6872.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 120000/150534 [00:17<00:04, 6825.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 116000/150534 [00:17<00:05, 6818.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 117000/150534 [00:17<00:04, 6886.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 120000/150534 [00:17<00:04, 6814.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 121000/150534 [00:17<00:04, 6757.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 122000/150534 [00:17<00:05, 5497.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 118000/150534 [00:17<00:04, 6960.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 121000/150534 [00:18<00:04, 6984.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 122000/150534 [00:18<00:05, 5531.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 123000/150534 [00:18<00:04, 5839.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 119000/150534 [00:17<00:04, 7182.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 122000/150534 [00:18<00:04, 5788.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 120000/150534 [00:18<00:04, 7222.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 123000/150534 [00:18<00:04, 5878.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 124000/150534 [00:18<00:04, 6121.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 123000/150534 [00:18<00:04, 5934.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 121000/150534 [00:18<00:04, 7239.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 125000/150534 [00:18<00:03, 6472.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 124000/150534 [00:18<00:04, 6162.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 124000/150534 [00:18<00:04, 6190.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▎ | 126000/150534 [00:18<00:03, 6604.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 125000/150534 [00:18<00:03, 6492.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 125000/150534 [00:18<00:03, 6535.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▍ | 127000/150534 [00:18<00:03, 6847.04 examples/s]#015Map:  84%|████████▎ | 126000/150534 [00:18<00:03, 6765.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 122000/150534 [00:18<00:05, 5507.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 128000/150534 [00:18<00:03, 6979.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▎ | 126000/150534 [00:18<00:03, 6640.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 123000/150534 [00:18<00:04, 5893.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▍ | 127000/150534 [00:18<00:03, 6739.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▍ | 127000/150534 [00:18<00:03, 6738.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▌ | 129000/150534 [00:18<00:03, 6868.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 128000/150534 [00:19<00:03, 6879.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 124000/150534 [00:18<00:04, 6179.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 128000/150534 [00:19<00:03, 6858.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 125000/150534 [00:18<00:03, 6453.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▌ | 129000/150534 [00:19<00:03, 6936.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▋ | 130000/150534 [00:19<00:02, 6882.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▌ | 129000/150534 [00:19<00:03, 6873.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▎ | 126000/150534 [00:19<00:03, 6585.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  87%|████████▋ | 131000/150534 [00:19<00:02, 6887.67 examples/s]#015Map:  86%|████████▋ | 130000/150534 [00:19<00:03, 6803.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  87%|████████▋ | 131000/150534 [00:19<00:02, 6978.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▋ | 130000/150534 [00:19<00:03, 6835.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 132000/150534 [00:19<00:02, 6949.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▍ | 127000/150534 [00:19<00:03, 6643.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  87%|████████▋ | 131000/150534 [00:19<00:02, 6756.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 128000/150534 [00:19<00:03, 6631.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 132000/150534 [00:19<00:02, 6819.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 133000/150534 [00:19<00:02, 6783.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  89%|████████▉ | 134000/150534 [00:19<00:02, 6854.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 133000/150534 [00:19<00:02, 6846.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 132000/150534 [00:19<00:02, 6756.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▌ | 129000/150534 [00:19<00:03, 6672.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 135000/150534 [00:19<00:02, 7011.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 133000/150534 [00:19<00:02, 6921.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▋ | 130000/150534 [00:19<00:02, 6880.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  89%|████████▉ | 134000/150534 [00:19<00:02, 6955.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 136000/150534 [00:19<00:02, 6761.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  89%|████████▉ | 134000/150534 [00:19<00:02, 6717.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  87%|████████▋ | 131000/150534 [00:19<00:02, 6689.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 135000/150534 [00:20<00:02, 6729.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████ | 137000/150534 [00:20<00:01, 6877.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 135000/150534 [00:20<00:02, 6818.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 132000/150534 [00:19<00:02, 6754.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 136000/150534 [00:20<00:02, 6783.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 138000/150534 [00:20<00:01, 7000.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████ | 137000/150534 [00:20<00:01, 7006.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 136000/150534 [00:20<00:02, 6946.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 133000/150534 [00:20<00:02, 6923.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████ | 137000/150534 [00:20<00:01, 6939.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 139000/150534 [00:20<00:01, 6867.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 138000/150534 [00:20<00:01, 6909.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  89%|████████▉ | 134000/150534 [00:20<00:02, 6816.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  93%|█████████▎| 140000/150534 [00:20<00:01, 6981.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 138000/150534 [00:20<00:01, 6967.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 135000/150534 [00:20<00:02, 6984.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 139000/150534 [00:20<00:01, 6945.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 141000/150534 [00:20<00:01, 6873.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  93%|█████████▎| 140000/150534 [00:20<00:01, 6873.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 139000/150534 [00:20<00:01, 6817.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 136000/150534 [00:20<00:02, 6779.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 142000/150534 [00:20<00:01, 6968.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  93%|█████████▎| 140000/150534 [00:20<00:01, 6978.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 141000/150534 [00:20<00:01, 6933.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████ | 137000/150534 [00:20<00:01, 6959.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▍| 143000/150534 [00:20<00:01, 7047.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 141000/150534 [00:20<00:01, 7012.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 142000/150534 [00:21<00:01, 7039.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 138000/150534 [00:20<00:01, 7041.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▌| 144000/150534 [00:21<00:00, 7082.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 139000/150534 [00:20<00:01, 7089.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▍| 143000/150534 [00:21<00:01, 7068.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 142000/150534 [00:21<00:01, 6999.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 145000/150534 [00:21<00:00, 7048.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  93%|█████████▎| 140000/150534 [00:21<00:01, 6918.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▌| 144000/150534 [00:21<00:00, 6878.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▍| 143000/150534 [00:21<00:01, 6799.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 146000/150534 [00:21<00:00, 6834.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▌| 144000/150534 [00:21<00:00, 6938.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 141000/150534 [00:21<00:01, 6894.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 145000/150534 [00:21<00:00, 6819.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 142000/150534 [00:21<00:01, 7063.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 145000/150534 [00:21<00:00, 6927.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 147000/150534 [00:21<00:00, 6692.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 146000/150534 [00:21<00:00, 6576.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 146000/150534 [00:21<00:00, 6826.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 147000/150534 [00:21<00:00, 6888.25 examples/s]#015Map:  95%|█████████▍| 143000/150534 [00:21<00:01, 6719.14 examples/s]#015Map:  98%|█████████▊| 148000/150534 [00:21<00:00, 6725.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 149000/150534 [00:21<00:00, 6799.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 147000/150534 [00:21<00:00, 6677.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▌| 144000/150534 [00:21<00:00, 6671.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 148000/150534 [00:21<00:00, 6748.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 148000/150534 [00:22<00:00, 6715.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 145000/150534 [00:21<00:00, 6694.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 149000/150534 [00:22<00:00, 6775.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 150000/150534 [00:22<00:00, 5434.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 146000/150534 [00:21<00:00, 6876.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 149000/150534 [00:22<00:00, 6853.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:22<00:00, 6800.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/150534 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 150000/150534 [00:22<00:00, 5578.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 147000/150534 [00:22<00:00, 6990.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:22<00:00, 6708.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/150534 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 150000/150534 [00:22<00:00, 5659.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 148000/150534 [00:22<00:00, 7103.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:22<00:00, 6691.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/150534 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|          | 1000/150534 [00:00<00:44, 3360.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 149000/150534 [00:22<00:00, 7467.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|          | 1000/150534 [00:00<00:45, 3296.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 150000/150534 [00:22<00:00, 6110.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|          | 1000/150534 [00:00<00:43, 3452.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|▏         | 2000/150534 [00:00<00:43, 3429.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:22<00:00, 6651.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   0%|          | 0/150534 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|▏         | 2000/150534 [00:00<00:44, 3364.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|▏         | 2000/150534 [00:00<00:42, 3515.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 3000/150534 [00:00<00:41, 3531.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|          | 1000/150534 [00:00<00:41, 3596.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 3000/150534 [00:00<00:42, 3439.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 3000/150534 [00:00<00:41, 3557.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 4000/150534 [00:01<00:40, 3597.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   1%|▏         | 2000/150534 [00:00<00:40, 3628.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 4000/150534 [00:01<00:42, 3478.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 4000/150534 [00:01<00:40, 3574.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 5000/150534 [00:01<00:40, 3619.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   2%|▏         | 3000/150534 [00:00<00:40, 3601.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 5000/150534 [00:01<00:41, 3503.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▍         | 6000/150534 [00:01<00:39, 3638.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 5000/150534 [00:01<00:40, 3560.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 4000/150534 [00:01<00:40, 3603.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▍         | 6000/150534 [00:01<00:41, 3502.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▍         | 7000/150534 [00:01<00:39, 3639.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▍         | 6000/150534 [00:01<00:40, 3582.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   3%|▎         | 5000/150534 [00:01<00:40, 3596.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▍         | 7000/150534 [00:02<00:40, 3506.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▌         | 8000/150534 [00:02<00:39, 3654.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▍         | 7000/150534 [00:01<00:39, 3597.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   4%|▍         | 6000/150534 [00:01<00:40, 3590.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▌         | 8000/150534 [00:02<00:40, 3507.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   6%|▌         | 9000/150534 [00:02<00:38, 3650.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▌         | 8000/150534 [00:02<00:39, 3589.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▍         | 7000/150534 [00:01<00:39, 3595.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   6%|▌         | 9000/150534 [00:02<00:40, 3517.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 10000/150534 [00:02<00:38, 3656.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   6%|▌         | 9000/150534 [00:02<00:39, 3599.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   5%|▌         | 8000/150534 [00:02<00:39, 3590.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 10000/150534 [00:02<00:39, 3521.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 11000/150534 [00:03<00:38, 3664.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 10000/150534 [00:02<00:38, 3606.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   6%|▌         | 9000/150534 [00:02<00:39, 3600.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 11000/150534 [00:03<00:39, 3510.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   8%|▊         | 12000/150534 [00:03<00:37, 3668.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 11000/150534 [00:03<00:38, 3600.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 10000/150534 [00:02<00:39, 3599.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   8%|▊         | 12000/150534 [00:03<00:39, 3503.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▊         | 13000/150534 [00:03<00:37, 3670.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   8%|▊         | 12000/150534 [00:03<00:38, 3568.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   7%|▋         | 11000/150534 [00:03<00:38, 3599.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▉         | 14000/150534 [00:03<00:37, 3665.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▊         | 13000/150534 [00:03<00:39, 3505.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▊         | 13000/150534 [00:03<00:38, 3573.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   8%|▊         | 12000/150534 [00:03<00:38, 3592.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 15000/150534 [00:04<00:36, 3674.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▉         | 14000/150534 [00:04<00:38, 3510.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▉         | 14000/150534 [00:03<00:38, 3573.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▊         | 13000/150534 [00:03<00:38, 3596.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 16000/150534 [00:04<00:36, 3666.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 15000/150534 [00:04<00:38, 3505.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 15000/150534 [00:04<00:37, 3579.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:   9%|▉         | 14000/150534 [00:03<00:37, 3602.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█▏        | 17000/150534 [00:04<00:36, 3660.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 16000/150534 [00:04<00:38, 3506.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 16000/150534 [00:04<00:37, 3593.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  10%|▉         | 15000/150534 [00:04<00:37, 3607.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 18000/150534 [00:04<00:36, 3672.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█▏        | 17000/150534 [00:04<00:38, 3499.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█▏        | 17000/150534 [00:04<00:37, 3590.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█         | 16000/150534 [00:04<00:37, 3601.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 19000/150534 [00:05<00:35, 3669.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 18000/150534 [00:05<00:37, 3515.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 18000/150534 [00:05<00:36, 3583.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  11%|█▏        | 17000/150534 [00:04<00:37, 3604.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 20000/150534 [00:05<00:35, 3654.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 19000/150534 [00:05<00:37, 3510.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 19000/150534 [00:05<00:36, 3575.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  12%|█▏        | 18000/150534 [00:04<00:36, 3603.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  14%|█▍        | 21000/150534 [00:05<00:35, 3653.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 20000/150534 [00:05<00:37, 3514.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 20000/150534 [00:05<00:36, 3586.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 19000/150534 [00:05<00:36, 3605.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 22000/150534 [00:06<00:35, 3661.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  14%|█▍        | 21000/150534 [00:05<00:36, 3584.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  14%|█▍        | 21000/150534 [00:06<00:36, 3509.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  13%|█▎        | 20000/150534 [00:05<00:36, 3608.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▌        | 23000/150534 [00:06<00:34, 3663.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 22000/150534 [00:06<00:35, 3585.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 22000/150534 [00:06<00:36, 3514.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  14%|█▍        | 21000/150534 [00:05<00:35, 3605.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 24000/150534 [00:06<00:34, 3654.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▌        | 23000/150534 [00:06<00:35, 3572.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▌        | 23000/150534 [00:06<00:36, 3515.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▍        | 22000/150534 [00:06<00:35, 3597.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 25000/150534 [00:06<00:34, 3652.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 24000/150534 [00:06<00:35, 3576.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 24000/150534 [00:06<00:35, 3522.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  15%|█▌        | 23000/150534 [00:06<00:35, 3594.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 26000/150534 [00:07<00:33, 3664.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 25000/150534 [00:06<00:35, 3582.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 25000/150534 [00:07<00:35, 3513.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  16%|█▌        | 24000/150534 [00:06<00:35, 3595.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 27000/150534 [00:07<00:33, 3664.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 26000/150534 [00:07<00:34, 3585.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 26000/150534 [00:07<00:35, 3527.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 25000/150534 [00:06<00:34, 3598.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▊        | 28000/150534 [00:07<00:33, 3663.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 27000/150534 [00:07<00:34, 3586.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 27000/150534 [00:07<00:34, 3543.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  17%|█▋        | 26000/150534 [00:07<00:34, 3608.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▉        | 29000/150534 [00:07<00:33, 3665.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▊        | 28000/150534 [00:07<00:34, 3593.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  18%|█▊        | 27000/150534 [00:07<00:34, 3607.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▊        | 28000/150534 [00:07<00:34, 3536.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 30000/150534 [00:08<00:32, 3671.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▉        | 29000/150534 [00:08<00:33, 3594.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▊        | 28000/150534 [00:07<00:33, 3606.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▉        | 29000/150534 [00:08<00:34, 3541.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██        | 31000/150534 [00:08<00:32, 3673.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 30000/150534 [00:08<00:33, 3600.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  19%|█▉        | 29000/150534 [00:08<00:33, 3613.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 30000/150534 [00:08<00:34, 3529.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██▏       | 32000/150534 [00:08<00:32, 3669.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██        | 31000/150534 [00:08<00:33, 3586.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  20%|█▉        | 30000/150534 [00:08<00:33, 3616.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██        | 31000/150534 [00:08<00:33, 3535.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|██▏       | 33000/150534 [00:09<00:32, 3662.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██▏       | 32000/150534 [00:08<00:33, 3588.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██        | 31000/150534 [00:08<00:33, 3619.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██▏       | 32000/150534 [00:09<00:33, 3527.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 34000/150534 [00:09<00:31, 3654.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|██▏       | 33000/150534 [00:09<00:32, 3583.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  21%|██▏       | 32000/150534 [00:08<00:32, 3610.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|██▏       | 33000/150534 [00:09<00:33, 3527.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 35000/150534 [00:09<00:31, 3664.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 34000/150534 [00:09<00:32, 3567.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  22%|██▏       | 33000/150534 [00:09<00:32, 3605.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 34000/150534 [00:09<00:33, 3528.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 36000/150534 [00:09<00:31, 3670.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 35000/150534 [00:09<00:32, 3576.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 34000/150534 [00:09<00:32, 3608.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 35000/150534 [00:09<00:32, 3525.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 37000/150534 [00:10<00:30, 3668.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 36000/150534 [00:10<00:32, 3570.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  23%|██▎       | 35000/150534 [00:09<00:32, 3607.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 36000/150534 [00:10<00:32, 3520.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▌       | 38000/150534 [00:10<00:30, 3659.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 37000/150534 [00:10<00:31, 3562.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  24%|██▍       | 36000/150534 [00:09<00:31, 3596.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 37000/150534 [00:10<00:32, 3520.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  26%|██▌       | 39000/150534 [00:10<00:30, 3663.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▌       | 38000/150534 [00:10<00:31, 3574.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▍       | 37000/150534 [00:10<00:31, 3600.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 40000/150534 [00:10<00:30, 3657.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▌       | 38000/150534 [00:10<00:32, 3508.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  26%|██▌       | 39000/150534 [00:10<00:31, 3587.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  25%|██▌       | 38000/150534 [00:10<00:31, 3610.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 41000/150534 [00:11<00:30, 3647.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  26%|██▌       | 39000/150534 [00:11<00:31, 3510.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  26%|██▌       | 39000/150534 [00:10<00:30, 3621.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 40000/150534 [00:11<00:30, 3591.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  28%|██▊       | 42000/150534 [00:11<00:29, 3650.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 40000/150534 [00:11<00:31, 3518.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 40000/150534 [00:11<00:30, 3612.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 41000/150534 [00:11<00:30, 3571.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▊       | 43000/150534 [00:11<00:29, 3652.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 41000/150534 [00:11<00:31, 3520.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  27%|██▋       | 41000/150534 [00:11<00:30, 3612.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  28%|██▊       | 42000/150534 [00:11<00:30, 3578.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 44000/150534 [00:12<00:29, 3662.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  28%|██▊       | 42000/150534 [00:11<00:30, 3507.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  28%|██▊       | 42000/150534 [00:11<00:29, 3639.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▊       | 43000/150534 [00:12<00:29, 3588.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 45000/150534 [00:12<00:28, 3672.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▊       | 43000/150534 [00:12<00:30, 3513.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▊       | 43000/150534 [00:11<00:29, 3628.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 44000/150534 [00:12<00:29, 3587.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 46000/150534 [00:12<00:28, 3671.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 44000/150534 [00:12<00:30, 3513.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  29%|██▉       | 44000/150534 [00:12<00:29, 3620.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 45000/150534 [00:12<00:29, 3577.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 47000/150534 [00:12<00:28, 3678.94 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 45000/150534 [00:12<00:29, 3526.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  30%|██▉       | 45000/150534 [00:12<00:29, 3626.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 46000/150534 [00:12<00:29, 3587.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 48000/150534 [00:13<00:27, 3668.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 46000/150534 [00:13<00:29, 3524.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 46000/150534 [00:12<00:28, 3624.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 47000/150534 [00:13<00:28, 3597.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 49000/150534 [00:13<00:27, 3671.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 47000/150534 [00:13<00:29, 3510.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  31%|███       | 47000/150534 [00:13<00:28, 3611.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 48000/150534 [00:13<00:28, 3592.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 50000/150534 [00:13<00:27, 3673.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 48000/150534 [00:13<00:29, 3500.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  32%|███▏      | 48000/150534 [00:13<00:28, 3599.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 51000/150534 [00:13<00:27, 3667.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 49000/150534 [00:13<00:28, 3575.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 49000/150534 [00:13<00:29, 3498.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 49000/150534 [00:13<00:28, 3610.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▍      | 52000/150534 [00:14<00:26, 3672.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 50000/150534 [00:13<00:28, 3570.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 50000/150534 [00:14<00:28, 3498.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  33%|███▎      | 50000/150534 [00:13<00:27, 3610.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▌      | 53000/150534 [00:14<00:26, 3669.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 51000/150534 [00:14<00:27, 3569.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 51000/150534 [00:14<00:28, 3505.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  34%|███▍      | 51000/150534 [00:14<00:27, 3618.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▌      | 54000/150534 [00:14<00:26, 3669.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▍      | 52000/150534 [00:14<00:27, 3580.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▍      | 52000/150534 [00:14<00:28, 3488.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▍      | 52000/150534 [00:14<00:27, 3621.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 55000/150534 [00:15<00:26, 3664.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▌      | 53000/150534 [00:14<00:27, 3582.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▌      | 53000/150534 [00:15<00:27, 3494.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  35%|███▌      | 53000/150534 [00:14<00:26, 3614.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 56000/150534 [00:15<00:25, 3665.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▌      | 54000/150534 [00:15<00:26, 3587.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▌      | 54000/150534 [00:15<00:27, 3508.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  36%|███▌      | 54000/150534 [00:14<00:26, 3622.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 57000/150534 [00:15<00:25, 3664.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 55000/150534 [00:15<00:26, 3592.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 55000/150534 [00:15<00:27, 3497.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 55000/150534 [00:15<00:26, 3630.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▊      | 58000/150534 [00:15<00:25, 3657.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 56000/150534 [00:15<00:26, 3597.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 56000/150534 [00:15<00:26, 3506.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  37%|███▋      | 56000/150534 [00:15<00:26, 3617.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 59000/150534 [00:16<00:25, 3655.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 57000/150534 [00:15<00:26, 3597.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 57000/150534 [00:16<00:26, 3502.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 60000/150534 [00:16<00:24, 3665.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  38%|███▊      | 57000/150534 [00:15<00:25, 3606.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▊      | 58000/150534 [00:16<00:25, 3592.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▊      | 58000/150534 [00:16<00:26, 3503.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 61000/150534 [00:16<00:24, 3657.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▊      | 58000/150534 [00:16<00:25, 3601.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 59000/150534 [00:16<00:25, 3596.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 59000/150534 [00:16<00:26, 3499.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 62000/150534 [00:16<00:24, 3666.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  39%|███▉      | 59000/150534 [00:16<00:25, 3603.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 60000/150534 [00:16<00:25, 3600.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 63000/150534 [00:17<00:23, 3674.24 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 60000/150534 [00:17<00:25, 3486.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  40%|███▉      | 60000/150534 [00:16<00:25, 3609.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 61000/150534 [00:17<00:24, 3592.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 64000/150534 [00:17<00:23, 3674.39 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 61000/150534 [00:16<00:24, 3605.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 61000/150534 [00:17<00:25, 3488.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 62000/150534 [00:17<00:24, 3584.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 65000/150534 [00:17<00:23, 3666.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 62000/150534 [00:17<00:24, 3611.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  41%|████      | 62000/150534 [00:17<00:25, 3493.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 63000/150534 [00:17<00:24, 3596.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▍     | 66000/150534 [00:18<00:23, 3665.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 63000/150534 [00:17<00:24, 3616.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  42%|████▏     | 63000/150534 [00:17<00:25, 3499.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 64000/150534 [00:17<00:24, 3599.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▍     | 67000/150534 [00:18<00:22, 3672.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 64000/150534 [00:17<00:23, 3618.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 64000/150534 [00:18<00:24, 3492.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 65000/150534 [00:18<00:23, 3586.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▌     | 68000/150534 [00:18<00:22, 3655.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 65000/150534 [00:18<00:23, 3614.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  43%|████▎     | 65000/150534 [00:18<00:24, 3490.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▍     | 66000/150534 [00:18<00:23, 3575.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 69000/150534 [00:18<00:22, 3654.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▍     | 66000/150534 [00:18<00:23, 3613.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  44%|████▍     | 66000/150534 [00:18<00:24, 3484.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▍     | 67000/150534 [00:18<00:23, 3584.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 70000/150534 [00:19<00:22, 3658.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▍     | 67000/150534 [00:18<00:23, 3608.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▍     | 67000/150534 [00:19<00:24, 3479.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▌     | 68000/150534 [00:18<00:23, 3582.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 71000/150534 [00:19<00:21, 3661.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▌     | 68000/150534 [00:18<00:22, 3609.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 69000/150534 [00:19<00:22, 3578.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  45%|████▌     | 68000/150534 [00:19<00:23, 3495.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 72000/150534 [00:19<00:21, 3653.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 69000/150534 [00:19<00:22, 3601.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 70000/150534 [00:19<00:22, 3580.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  46%|████▌     | 69000/150534 [00:19<00:23, 3508.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 73000/150534 [00:19<00:21, 3649.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 70000/150534 [00:19<00:22, 3600.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 71000/150534 [00:19<00:22, 3593.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 70000/150534 [00:19<00:23, 3500.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  49%|████▉     | 74000/150534 [00:20<00:20, 3661.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 71000/150534 [00:19<00:22, 3599.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 72000/150534 [00:20<00:21, 3587.60 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  47%|████▋     | 71000/150534 [00:20<00:22, 3499.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 75000/150534 [00:20<00:20, 3662.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 72000/150534 [00:19<00:21, 3601.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 73000/150534 [00:20<00:21, 3585.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 72000/150534 [00:20<00:22, 3501.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|█████     | 76000/150534 [00:20<00:20, 3672.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 73000/150534 [00:20<00:21, 3594.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  49%|████▉     | 74000/150534 [00:20<00:21, 3597.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  48%|████▊     | 73000/150534 [00:20<00:22, 3508.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████     | 77000/150534 [00:21<00:20, 3674.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  49%|████▉     | 74000/150534 [00:20<00:21, 3606.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 75000/150534 [00:20<00:21, 3583.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  49%|████▉     | 74000/150534 [00:21<00:21, 3511.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 78000/150534 [00:21<00:19, 3675.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 75000/150534 [00:20<00:20, 3603.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|█████     | 76000/150534 [00:21<00:20, 3599.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|████▉     | 75000/150534 [00:21<00:21, 3511.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 79000/150534 [00:21<00:19, 3675.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|█████     | 76000/150534 [00:21<00:20, 3610.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████     | 77000/150534 [00:21<00:20, 3610.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  50%|█████     | 76000/150534 [00:21<00:21, 3519.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 80000/150534 [00:21<00:19, 3677.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████     | 77000/150534 [00:21<00:20, 3615.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 78000/150534 [00:21<00:20, 3598.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  51%|█████     | 77000/150534 [00:21<00:21, 3481.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 81000/150534 [00:22<00:18, 3683.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 78000/150534 [00:21<00:19, 3628.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 79000/150534 [00:22<00:20, 3564.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 78000/150534 [00:22<00:20, 3483.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 82000/150534 [00:22<00:18, 3673.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 79000/150534 [00:21<00:19, 3610.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 80000/150534 [00:22<00:19, 3566.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  52%|█████▏    | 79000/150534 [00:22<00:20, 3492.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▌    | 83000/150534 [00:22<00:18, 3679.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 80000/150534 [00:22<00:19, 3605.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 81000/150534 [00:22<00:19, 3571.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 84000/150534 [00:22<00:18, 3671.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  53%|█████▎    | 80000/150534 [00:22<00:20, 3491.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 81000/150534 [00:22<00:19, 3609.19 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 82000/150534 [00:22<00:19, 3559.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 85000/150534 [00:23<00:17, 3673.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 81000/150534 [00:23<00:19, 3490.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 82000/150534 [00:22<00:19, 3596.07 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▌    | 83000/150534 [00:23<00:18, 3569.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  57%|█████▋    | 86000/150534 [00:23<00:17, 3671.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  54%|█████▍    | 82000/150534 [00:23<00:19, 3491.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▌    | 83000/150534 [00:23<00:18, 3603.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 84000/150534 [00:23<00:18, 3562.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 87000/150534 [00:23<00:17, 3665.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  55%|█████▌    | 83000/150534 [00:23<00:19, 3500.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 84000/150534 [00:23<00:18, 3611.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 85000/150534 [00:23<00:18, 3562.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 88000/150534 [00:24<00:17, 3658.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▌    | 84000/150534 [00:23<00:19, 3492.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 85000/150534 [00:23<00:18, 3616.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  57%|█████▋    | 86000/150534 [00:24<00:18, 3572.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  59%|█████▉    | 89000/150534 [00:24<00:16, 3660.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  56%|█████▋    | 85000/150534 [00:24<00:18, 3497.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  57%|█████▋    | 86000/150534 [00:23<00:17, 3603.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 87000/150534 [00:24<00:17, 3566.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 90000/150534 [00:24<00:16, 3656.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  57%|█████▋    | 86000/150534 [00:24<00:18, 3504.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 87000/150534 [00:24<00:17, 3599.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 88000/150534 [00:24<00:17, 3564.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|██████    | 91000/150534 [00:24<00:16, 3659.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 87000/150534 [00:24<00:18, 3503.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 88000/150534 [00:24<00:17, 3602.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  59%|█████▉    | 89000/150534 [00:24<00:17, 3573.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 92000/150534 [00:25<00:15, 3661.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  58%|█████▊    | 88000/150534 [00:25<00:17, 3508.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  59%|█████▉    | 89000/150534 [00:24<00:17, 3605.28 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 90000/150534 [00:25<00:16, 3566.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 93000/150534 [00:25<00:15, 3646.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  59%|█████▉    | 89000/150534 [00:25<00:17, 3500.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 90000/150534 [00:24<00:16, 3597.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|██████    | 91000/150534 [00:25<00:16, 3577.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 94000/150534 [00:25<00:15, 3649.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|█████▉    | 90000/150534 [00:25<00:17, 3495.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|██████    | 91000/150534 [00:25<00:16, 3607.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 92000/150534 [00:25<00:16, 3583.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 95000/150534 [00:25<00:15, 3638.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  60%|██████    | 91000/150534 [00:25<00:17, 3492.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 92000/150534 [00:25<00:16, 3615.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 96000/150534 [00:26<00:14, 3656.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 93000/150534 [00:25<00:16, 3573.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 93000/150534 [00:25<00:15, 3619.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  61%|██████    | 92000/150534 [00:26<00:16, 3478.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 97000/150534 [00:26<00:14, 3656.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 94000/150534 [00:26<00:15, 3594.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 94000/150534 [00:26<00:15, 3624.80 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 93000/150534 [00:26<00:16, 3483.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 98000/150534 [00:26<00:14, 3648.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 95000/150534 [00:26<00:15, 3594.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 95000/150534 [00:26<00:15, 3632.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  62%|██████▏   | 94000/150534 [00:26<00:16, 3487.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▌   | 99000/150534 [00:27<00:14, 3653.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 96000/150534 [00:26<00:15, 3590.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 96000/150534 [00:26<00:15, 3631.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  63%|██████▎   | 95000/150534 [00:27<00:15, 3487.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▋   | 100000/150534 [00:27<00:13, 3657.08 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 97000/150534 [00:27<00:14, 3593.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 97000/150534 [00:26<00:14, 3627.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 96000/150534 [00:27<00:15, 3494.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 101000/150534 [00:27<00:13, 3658.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 98000/150534 [00:27<00:14, 3584.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 98000/150534 [00:27<00:14, 3629.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  64%|██████▍   | 97000/150534 [00:27<00:15, 3501.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 102000/150534 [00:27<00:13, 3639.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▌   | 99000/150534 [00:27<00:14, 3582.62 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▌   | 99000/150534 [00:27<00:14, 3624.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  65%|██████▌   | 98000/150534 [00:27<00:14, 3511.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 103000/150534 [00:28<00:13, 3638.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▋   | 100000/150534 [00:27<00:14, 3586.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▋   | 100000/150534 [00:27<00:13, 3621.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▌   | 99000/150534 [00:28<00:14, 3510.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▉   | 104000/150534 [00:28<00:12, 3650.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 101000/150534 [00:28<00:13, 3581.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 101000/150534 [00:27<00:13, 3622.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  66%|██████▋   | 100000/150534 [00:28<00:14, 3511.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 105000/150534 [00:28<00:12, 3651.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 102000/150534 [00:28<00:13, 3577.82 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 102000/150534 [00:28<00:13, 3624.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  67%|██████▋   | 101000/150534 [00:28<00:14, 3505.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 106000/150534 [00:28<00:12, 3652.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 103000/150534 [00:28<00:13, 3573.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 103000/150534 [00:28<00:13, 3619.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 102000/150534 [00:29<00:13, 3500.48 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  71%|███████   | 107000/150534 [00:29<00:11, 3656.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▉   | 104000/150534 [00:29<00:13, 3574.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▉   | 104000/150534 [00:28<00:12, 3615.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 108000/150534 [00:29<00:11, 3654.53 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  68%|██████▊   | 103000/150534 [00:29<00:13, 3501.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 105000/150534 [00:29<00:12, 3570.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 105000/150534 [00:29<00:12, 3603.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 109000/150534 [00:29<00:11, 3651.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  69%|██████▉   | 104000/150534 [00:29<00:13, 3495.25 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 106000/150534 [00:29<00:12, 3567.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 106000/150534 [00:29<00:12, 3608.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 110000/150534 [00:30<00:11, 3662.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|██████▉   | 105000/150534 [00:29<00:13, 3502.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  71%|███████   | 107000/150534 [00:29<00:12, 3573.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  71%|███████   | 107000/150534 [00:29<00:12, 3606.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▎  | 111000/150534 [00:30<00:10, 3678.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  70%|███████   | 106000/150534 [00:30<00:12, 3503.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 108000/150534 [00:30<00:11, 3566.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 108000/150534 [00:29<00:11, 3609.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 112000/150534 [00:30<00:10, 3670.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  71%|███████   | 107000/150534 [00:30<00:12, 3501.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 109000/150534 [00:30<00:11, 3566.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 109000/150534 [00:30<00:11, 3604.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 113000/150534 [00:30<00:10, 3678.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 108000/150534 [00:30<00:12, 3494.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 110000/150534 [00:30<00:11, 3570.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 110000/150534 [00:30<00:11, 3589.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▌  | 114000/150534 [00:31<00:09, 3685.04 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  72%|███████▏  | 109000/150534 [00:31<00:11, 3496.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▎  | 111000/150534 [00:30<00:11, 3579.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▎  | 111000/150534 [00:30<00:11, 3593.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▋  | 115000/150534 [00:31<00:09, 3684.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  73%|███████▎  | 110000/150534 [00:31<00:11, 3495.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 112000/150534 [00:31<00:10, 3574.26 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 112000/150534 [00:31<00:10, 3590.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 116000/150534 [00:31<00:09, 3678.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▎  | 111000/150534 [00:31<00:11, 3487.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 113000/150534 [00:31<00:10, 3573.90 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 113000/150534 [00:31<00:10, 3598.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 117000/150534 [00:31<00:09, 3680.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▌  | 114000/150534 [00:31<00:10, 3593.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  74%|███████▍  | 112000/150534 [00:31<00:11, 3497.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▌  | 114000/150534 [00:31<00:10, 3606.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 118000/150534 [00:32<00:08, 3675.29 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▋  | 115000/150534 [00:32<00:09, 3606.38 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  75%|███████▌  | 113000/150534 [00:32<00:10, 3501.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▋  | 115000/150534 [00:31<00:09, 3605.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 119000/150534 [00:32<00:08, 3670.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 116000/150534 [00:32<00:09, 3574.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▌  | 114000/150534 [00:32<00:10, 3505.76 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 116000/150534 [00:32<00:09, 3584.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 120000/150534 [00:32<00:08, 3668.81 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 117000/150534 [00:32<00:09, 3570.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  76%|███████▋  | 115000/150534 [00:32<00:10, 3517.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 117000/150534 [00:32<00:09, 3597.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 121000/150534 [00:33<00:08, 3674.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 118000/150534 [00:32<00:09, 3578.42 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  77%|███████▋  | 116000/150534 [00:33<00:09, 3524.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 118000/150534 [00:32<00:09, 3605.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 122000/150534 [00:33<00:07, 3683.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 119000/150534 [00:33<00:08, 3576.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 117000/150534 [00:33<00:09, 3528.74 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 119000/150534 [00:32<00:08, 3601.30 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 123000/150534 [00:33<00:07, 3662.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 120000/150534 [00:33<00:08, 3586.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  78%|███████▊  | 118000/150534 [00:33<00:09, 3524.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 120000/150534 [00:33<00:08, 3599.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 124000/150534 [00:33<00:07, 3667.47 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 121000/150534 [00:33<00:08, 3589.20 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  79%|███████▉  | 119000/150534 [00:33<00:08, 3524.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 125000/150534 [00:34<00:06, 3674.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 121000/150534 [00:33<00:08, 3603.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 122000/150534 [00:34<00:07, 3595.98 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|███████▉  | 120000/150534 [00:34<00:08, 3510.72 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▎ | 126000/150534 [00:34<00:06, 3674.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 122000/150534 [00:33<00:07, 3608.35 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 123000/150534 [00:34<00:07, 3602.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  80%|████████  | 121000/150534 [00:34<00:08, 3506.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▍ | 127000/150534 [00:34<00:06, 3674.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 123000/150534 [00:34<00:07, 3591.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 124000/150534 [00:34<00:07, 3599.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  81%|████████  | 122000/150534 [00:34<00:08, 3503.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 128000/150534 [00:34<00:06, 3675.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 124000/150534 [00:34<00:07, 3601.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 125000/150534 [00:34<00:07, 3597.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▌ | 129000/150534 [00:35<00:05, 3681.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 123000/150534 [00:35<00:07, 3503.50 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 125000/150534 [00:34<00:07, 3602.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▎ | 126000/150534 [00:35<00:06, 3588.11 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▋ | 130000/150534 [00:35<00:05, 3677.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  82%|████████▏ | 124000/150534 [00:35<00:07, 3496.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▎ | 126000/150534 [00:34<00:06, 3611.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▍ | 127000/150534 [00:35<00:06, 3602.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  87%|████████▋ | 131000/150534 [00:35<00:05, 3674.57 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  83%|████████▎ | 125000/150534 [00:35<00:07, 3512.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▍ | 127000/150534 [00:35<00:06, 3606.88 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 128000/150534 [00:35<00:06, 3619.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 132000/150534 [00:36<00:05, 3675.51 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▎ | 126000/150534 [00:35<00:06, 3510.96 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 128000/150534 [00:35<00:06, 3598.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▌ | 129000/150534 [00:35<00:05, 3634.77 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 133000/150534 [00:36<00:04, 3661.93 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▌ | 129000/150534 [00:35<00:05, 3591.58 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  84%|████████▍ | 127000/150534 [00:36<00:06, 3514.31 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▋ | 130000/150534 [00:36<00:05, 3615.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  89%|████████▉ | 134000/150534 [00:36<00:04, 3668.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▋ | 130000/150534 [00:36<00:05, 3596.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  85%|████████▌ | 128000/150534 [00:36<00:06, 3517.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  87%|████████▋ | 131000/150534 [00:36<00:05, 3611.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 135000/150534 [00:36<00:04, 3667.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  87%|████████▋ | 131000/150534 [00:36<00:05, 3593.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▌ | 129000/150534 [00:36<00:06, 3514.54 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 132000/150534 [00:36<00:05, 3603.56 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 136000/150534 [00:37<00:03, 3672.84 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 132000/150534 [00:36<00:05, 3589.67 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  86%|████████▋ | 130000/150534 [00:37<00:05, 3499.52 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 133000/150534 [00:37<00:04, 3607.70 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████ | 137000/150534 [00:37<00:03, 3658.95 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 133000/150534 [00:36<00:04, 3584.85 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  87%|████████▋ | 131000/150534 [00:37<00:05, 3501.16 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  89%|████████▉ | 134000/150534 [00:37<00:04, 3547.68 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 138000/150534 [00:37<00:03, 3652.17 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  89%|████████▉ | 134000/150534 [00:37<00:04, 3585.87 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 132000/150534 [00:37<00:05, 3503.05 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 135000/150534 [00:37<00:04, 3576.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 139000/150534 [00:37<00:03, 3650.34 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 135000/150534 [00:37<00:04, 3587.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  88%|████████▊ | 133000/150534 [00:37<00:05, 3460.41 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 136000/150534 [00:37<00:04, 3570.99 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  93%|█████████▎| 140000/150534 [00:38<00:02, 3647.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 136000/150534 [00:37<00:04, 3600.14 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  89%|████████▉ | 134000/150534 [00:38<00:04, 3475.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████ | 137000/150534 [00:38<00:03, 3576.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 141000/150534 [00:38<00:02, 3662.79 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████ | 137000/150534 [00:37<00:03, 3603.12 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|████████▉ | 135000/150534 [00:38<00:04, 3486.43 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 142000/150534 [00:38<00:02, 3675.10 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 138000/150534 [00:38<00:03, 3587.23 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 138000/150534 [00:38<00:03, 3597.63 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  90%|█████████ | 136000/150534 [00:38<00:04, 3500.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▍| 143000/150534 [00:39<00:02, 3665.66 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 139000/150534 [00:38<00:03, 3581.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 139000/150534 [00:38<00:03, 3614.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  91%|█████████ | 137000/150534 [00:39<00:03, 3487.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▌| 144000/150534 [00:39<00:01, 3654.09 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  93%|█████████▎| 140000/150534 [00:39<00:02, 3590.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  93%|█████████▎| 140000/150534 [00:38<00:02, 3605.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 138000/150534 [00:39<00:03, 3510.03 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 145000/150534 [00:39<00:01, 3658.83 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 141000/150534 [00:39<00:02, 3580.45 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 141000/150534 [00:39<00:02, 3602.59 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  92%|█████████▏| 139000/150534 [00:39<00:03, 3522.33 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 146000/150534 [00:39<00:01, 3658.61 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 142000/150534 [00:39<00:02, 3580.55 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 142000/150534 [00:39<00:02, 3592.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  93%|█████████▎| 140000/150534 [00:39<00:02, 3526.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 147000/150534 [00:40<00:00, 3675.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▍| 143000/150534 [00:39<00:02, 3589.00 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▍| 143000/150534 [00:39<00:02, 3596.69 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▎| 141000/150534 [00:40<00:02, 3519.64 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 148000/150534 [00:40<00:00, 3662.71 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▌| 144000/150534 [00:40<00:01, 3594.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▌| 144000/150534 [00:39<00:01, 3601.91 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  94%|█████████▍| 142000/150534 [00:40<00:02, 3512.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 149000/150534 [00:40<00:00, 3666.21 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 145000/150534 [00:40<00:01, 3598.89 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 145000/150534 [00:40<00:01, 3598.75 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  95%|█████████▍| 143000/150534 [00:40<00:02, 3511.06 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 150000/150534 [00:40<00:00, 3665.37 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 146000/150534 [00:40<00:01, 3602.49 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 146000/150534 [00:40<00:01, 3597.73 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:41<00:00, 3660.02 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:41<00:00, 3660.92 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▌| 144000/150534 [00:41<00:01, 3501.46 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 147000/150534 [00:41<00:00, 3617.25 examples/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 147000/150534 [00:40<00:00, 3590.22 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  96%|█████████▋| 145000/150534 [00:41<00:01, 3503.36 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 148000/150534 [00:41<00:00, 3620.92 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 148000/150534 [00:41<00:00, 3608.44 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  97%|█████████▋| 146000/150534 [00:41<00:01, 3506.18 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 149000/150534 [00:41<00:00, 3628.78 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 149000/150534 [00:41<00:00, 3596.86 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 147000/150534 [00:41<00:01, 3510.40 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 150000/150534 [00:41<00:00, 3631.27 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 150000/150534 [00:41<00:00, 3596.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:41<00:00, 3625.32 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:41<00:00, 3585.18 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:41<00:00, 3598.01 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:41<00:00, 3605.97 examples/s]\u001b[0m\n",
      "\u001b[34mMap:  98%|█████████▊| 148000/150534 [00:42<00:00, 3505.17 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mMap:  99%|█████████▉| 149000/150534 [00:42<00:00, 3516.65 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|█████████▉| 150000/150534 [00:42<00:00, 3516.13 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:42<00:00, 3515.15 examples/s]\u001b[0m\n",
      "\u001b[34mMap: 100%|██████████| 150534/150534 [00:42<00:00, 3504.28 examples/s]\u001b[0m\n",
      "\u001b[34mINFO:root:Loading the pre-trained model.\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:06<00:19,  6.60s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:17,  5.95s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:15,  5.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  25%|██▌       | 1/4 [00:05<00:17,  5.89s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:13<00:13,  6.51s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.28s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:11<00:11,  5.95s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 2/4 [00:12<00:12,  6.26s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:05,  5.98s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:19<00:06,  6.61s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:17<00:06,  6.05s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  75%|███████▌  | 3/4 [00:18<00:06,  6.22s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.27s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.94s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  4.52s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:21<00:00,  5.27s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.18s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.79s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.29s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 4/4 [00:19<00:00,  4.98s/it]\u001b[0m\n",
      "\u001b[34m--> Model /opt/ml/additonals3data\u001b[0m\n",
      "\u001b[34m--> /opt/ml/additonals3data has 8030.261248 Million params\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34mbFloat16 enabled for mixed precision - using bfSixteen policy\u001b[0m\n",
      "\u001b[34mtrainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/1792 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/1792 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34mINFO:root:--> Training Set Length = 28680\u001b[0m\n",
      "\u001b[34mINFO:root:--> Validation Set Length = 7170\u001b[0m\n",
      "\u001b[34m--> applying fsdp activation checkpointing...\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/1792 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/torch/cuda/memory.py:330: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.\n",
      "  warnings.warn(\u001b[0m\n",
      "\u001b[34mNCCL version 2.19.3+cuda12.3\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 0/1792 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34m`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\u001b[0m\n",
      "\u001b[34mstep 0 is completed and loss is 0.7451974153518677\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 1/1792 [00:11<5:41:00, 11.42s/it]#015Training Epoch0:   0%|#033[34m          #033[0m| 1/1792 [00:10<5:25:16, 10.90s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 1/1792 [00:11<5:41:49, 11.45s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 1/1792 [00:10<5:22:02, 10.79s/it]\u001b[0m\n",
      "\u001b[34mstep 1 is completed and loss is 0.8092890381813049\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 2/1792 [00:21<5:11:54, 10.46s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 2/1792 [00:21<5:18:23, 10.67s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 2/1792 [00:21<5:18:44, 10.68s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 2/1792 [00:20<5:10:33, 10.41s/it]\u001b[0m\n",
      "\u001b[34mstep 2 is completed and loss is 0.7706774473190308\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 3/1792 [00:31<5:07:25, 10.31s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 3/1792 [00:31<5:10:56, 10.43s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 3/1792 [00:31<5:06:41, 10.29s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 3/1792 [00:31<5:11:08, 10.44s/it]\u001b[0m\n",
      "\u001b[34mstep 3 is completed and loss is 0.7766274213790894\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 4/1792 [00:41<5:05:12, 10.24s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 4/1792 [00:41<5:07:20, 10.31s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 4/1792 [00:41<5:07:27, 10.32s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 4/1792 [00:41<5:04:45, 10.23s/it]\u001b[0m\n",
      "\u001b[34mstep 4 is completed and loss is 0.7169676423072815\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 5/1792 [00:51<5:03:46, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 5/1792 [00:51<5:05:08, 10.25s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 5/1792 [00:51<5:05:12, 10.25s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 5/1792 [00:51<5:03:29, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 5 is completed and loss is 0.7474589943885803\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 6/1792 [01:01<5:03:08, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 6/1792 [01:02<5:04:02, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 6/1792 [01:02<5:04:04, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 6/1792 [01:01<5:02:57, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 7/1792 [01:12<5:03:18, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 6 is completed and loss is 0.7224909663200378\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 7/1792 [01:11<5:02:42, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 7/1792 [01:12<5:03:21, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 7/1792 [01:11<5:02:35, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 7 is completed and loss is 0.7327941060066223\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 8/1792 [01:22<5:02:42, 10.18s/it]#015Training Epoch0:   0%|#033[34m          #033[0m| 8/1792 [01:21<5:02:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 8/1792 [01:21<5:02:12, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   0%|#033[34m          #033[0m| 8/1792 [01:22<5:02:43, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 8 is completed and loss is 0.6911863684654236\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 9/1792 [01:32<5:01:55, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 9/1792 [01:32<5:02:12, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 9/1792 [01:32<5:02:13, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 9/1792 [01:31<5:01:52, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 9 is completed and loss is 0.652065634727478\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 10/1792 [01:42<5:01:39, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 10/1792 [01:42<5:01:50, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 10/1792 [01:42<5:01:50, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 10/1792 [01:42<5:01:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 11/1792 [01:52<5:02:34, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 10 is completed and loss is 0.6646425724029541\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 11/1792 [01:52<5:02:26, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 11/1792 [01:52<5:02:24, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 11/1792 [01:53<5:02:35, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 12/1792 [02:03<5:02:01, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 11 is completed and loss is 0.6255306601524353\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 12/1792 [02:02<5:01:56, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 12/1792 [02:02<5:01:54, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 12/1792 [02:03<5:02:02, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 12 is completed and loss is 0.6531057953834534\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 13/1792 [02:13<5:01:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 13/1792 [02:12<5:01:20, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 13/1792 [02:13<5:01:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 13/1792 [02:12<5:01:19, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 13 is completed and loss is 0.5975217819213867\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 14/1792 [02:22<5:01:01, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 14/1792 [02:23<5:01:04, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 14/1792 [02:23<5:01:04, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 14/1792 [02:22<5:01:01, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 15/1792 [02:33<5:00:50, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 14 is completed and loss is 0.5981851816177368\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 15/1792 [02:33<5:00:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 15/1792 [02:32<5:00:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 15/1792 [02:33<5:00:50, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 16/1792 [02:43<5:00:32, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 15 is completed and loss is 0.5751044154167175\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 16/1792 [02:43<5:00:30, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 16/1792 [02:43<5:00:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 16/1792 [02:43<5:00:32, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 17/1792 [02:53<5:00:13, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 17/1792 [02:53<5:00:13, 10.15s/it]#015Training Epoch0:   1%|#033[34m          #033[0m| 17/1792 [02:53<5:00:12, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 16 is completed and loss is 0.5988911986351013\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 17/1792 [02:53<5:00:13, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 18/1792 [03:03<4:59:52, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 18/1792 [03:04<4:59:52, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 18/1792 [03:03<4:59:52, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 17 is completed and loss is 0.5502201914787292\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 18/1792 [03:03<4:59:52, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 19/1792 [03:14<4:59:39, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 19/1792 [03:13<4:59:38, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 18 is completed and loss is 0.5088809132575989\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 19/1792 [03:13<4:59:39, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 19/1792 [03:14<4:59:39, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 20/1792 [03:24<4:59:24, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 20/1792 [03:23<4:59:23, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 20/1792 [03:24<4:59:23, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 19 is completed and loss is 0.5074148774147034\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 20/1792 [03:23<4:59:24, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 21/1792 [03:34<4:59:13, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 21/1792 [03:34<4:59:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 20 is completed and loss is 0.462846577167511\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 21/1792 [03:33<4:59:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 21/1792 [03:33<4:59:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 22/1792 [03:44<5:00:04, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 21 is completed and loss is 0.49953874945640564\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 22/1792 [03:44<5:00:04, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 22/1792 [03:44<5:00:04, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m          #033[0m| 22/1792 [03:43<5:00:04, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 23/1792 [03:54<4:59:30, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 22 is completed and loss is 0.42523524165153503\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 23/1792 [03:54<4:59:30, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 23/1792 [03:54<4:59:30, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 23/1792 [03:54<4:59:30, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 24/1792 [04:04<4:59:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 23 is completed and loss is 0.39902031421661377\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 24/1792 [04:04<4:59:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 24/1792 [04:04<4:59:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 24/1792 [04:04<4:59:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 25/1792 [04:15<4:58:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 25/1792 [04:15<4:58:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 25/1792 [04:14<4:58:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 24 is completed and loss is 0.4043336510658264\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 25/1792 [04:14<4:58:54, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 26/1792 [04:25<4:58:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 26/1792 [04:25<4:58:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 25 is completed and loss is 0.4093165695667267\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 26/1792 [04:24<4:58:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   1%|#033[34m▏         #033[0m| 26/1792 [04:24<4:58:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 27/1792 [04:35<4:58:19, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 26 is completed and loss is 0.39898568391799927\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 27/1792 [04:35<4:58:19, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 27/1792 [04:34<4:58:19, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 27/1792 [04:34<4:58:19, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 28/1792 [04:45<4:58:14, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 27 is completed and loss is 0.3992118537425995\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 28/1792 [04:45<4:58:14, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 28/1792 [04:44<4:58:14, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 28/1792 [04:44<4:58:14, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 29/1792 [04:55<4:58:03, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 28 is completed and loss is 0.3776323199272156\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 29/1792 [04:55<4:58:03, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 29/1792 [04:55<4:58:03, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 29/1792 [04:54<4:58:03, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 30/1792 [05:05<4:57:51, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 29 is completed and loss is 0.2927330732345581\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 30/1792 [05:05<4:57:50, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 30/1792 [05:05<4:57:51, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 30/1792 [05:05<4:57:51, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 31/1792 [05:15<4:57:38, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 30 is completed and loss is 0.36268436908721924\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 31/1792 [05:15<4:57:37, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 31/1792 [05:15<4:57:37, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 31/1792 [05:15<4:57:37, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 32/1792 [05:26<4:57:22, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 31 is completed and loss is 0.2903626561164856\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 32/1792 [05:26<4:57:21, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 32/1792 [05:25<4:57:21, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 32/1792 [05:25<4:57:22, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 32 is completed and loss is 0.3371563255786896\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 33/1792 [05:35<4:57:24, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 33/1792 [05:36<4:57:24, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 33/1792 [05:36<4:57:25, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 33/1792 [05:35<4:57:24, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 33 is completed and loss is 0.2958677113056183\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 34/1792 [05:45<4:57:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 34/1792 [05:46<4:57:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 34/1792 [05:46<4:57:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 34/1792 [05:45<4:57:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 34 is completed and loss is 0.3097277283668518\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 35/1792 [05:55<4:56:45, 10.13s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 35/1792 [05:55<4:56:46, 10.13s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 35/1792 [05:56<4:56:47, 10.13s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 35/1792 [05:56<4:56:46, 10.13s/it]\u001b[0m\n",
      "\u001b[34mstep 35 is completed and loss is 0.2917325496673584\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 36/1792 [06:06<4:56:39, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 36/1792 [06:06<4:56:40, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 36/1792 [06:06<4:56:40, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 36/1792 [06:05<4:56:40, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 36 is completed and loss is 0.31996071338653564\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 37/1792 [06:16<4:56:37, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 37/1792 [06:16<4:56:37, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 37/1792 [06:16<4:56:37, 10.14s/it]#015Training Epoch0:   2%|#033[34m▏         #033[0m| 37/1792 [06:16<4:56:37, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 37 is completed and loss is 0.27774012088775635\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 38/1792 [06:26<4:56:27, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 38/1792 [06:26<4:56:27, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 38/1792 [06:26<4:56:28, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 38/1792 [06:26<4:56:28, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 38 is completed and loss is 0.32559934258461\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 39/1792 [06:36<4:56:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 39/1792 [06:36<4:56:13, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 39/1792 [06:37<4:56:13, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 39/1792 [06:36<4:56:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 39 is completed and loss is 0.3105592429637909\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 40/1792 [06:46<4:56:04, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 40/1792 [06:47<4:56:03, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 40/1792 [06:46<4:56:04, 10.14s/it]#015Training Epoch0:   2%|#033[34m▏         #033[0m| 40/1792 [06:47<4:56:04, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 40 is completed and loss is 0.3216148018836975\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 41/1792 [06:56<4:55:48, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 41/1792 [06:56<4:55:48, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 41/1792 [06:57<4:55:48, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 41/1792 [06:57<4:55:48, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 41 is completed and loss is 0.2753901779651642\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 42/1792 [07:06<4:55:41, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 42/1792 [07:06<4:55:41, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 42/1792 [07:07<4:55:41, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 42/1792 [07:07<4:55:41, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 42 is completed and loss is 0.29972007870674133\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 43/1792 [07:17<4:56:37, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 43/1792 [07:17<4:56:37, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 43/1792 [07:17<4:56:37, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 43/1792 [07:17<4:56:37, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 43 is completed and loss is 0.2533224821090698\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 44/1792 [07:27<4:56:09, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 44/1792 [07:27<4:56:09, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 44/1792 [07:27<4:56:09, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   2%|#033[34m▏         #033[0m| 44/1792 [07:27<4:56:09, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 44 is completed and loss is 0.2645668387413025\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 45/1792 [07:37<4:55:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 45/1792 [07:37<4:55:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 45/1792 [07:37<4:55:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 45/1792 [07:37<4:55:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 45 is completed and loss is 0.2975248694419861\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 46/1792 [07:47<4:55:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 46/1792 [07:48<4:55:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 46/1792 [07:48<4:55:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 46/1792 [07:47<4:55:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 46 is completed and loss is 0.2961593568325043\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 47/1792 [07:57<4:55:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 47/1792 [07:58<4:55:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 47/1792 [07:57<4:55:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 47/1792 [07:58<4:55:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 47 is completed and loss is 0.33405596017837524\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 48/1792 [08:07<4:54:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 48/1792 [08:08<4:54:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 48/1792 [08:08<4:54:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 48/1792 [08:07<4:54:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 48 is completed and loss is 0.25804853439331055\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 49/1792 [08:18<4:54:48, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 49/1792 [08:18<4:54:48, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 49/1792 [08:18<4:54:48, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 49/1792 [08:17<4:54:48, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 49 is completed and loss is 0.3046489953994751\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 50/1792 [08:28<4:54:27, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 50/1792 [08:28<4:54:27, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 50/1792 [08:28<4:54:27, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 50/1792 [08:28<4:54:27, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 50 is completed and loss is 0.3003709316253662\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 51/1792 [08:38<4:54:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 51/1792 [08:38<4:54:22, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 51/1792 [08:38<4:54:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 51/1792 [08:38<4:54:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 51 is completed and loss is 0.2840413451194763\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 52/1792 [08:48<4:54:11, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 52/1792 [08:48<4:54:11, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 52/1792 [08:48<4:54:11, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 52/1792 [08:48<4:54:11, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 52 is completed and loss is 0.28381115198135376\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 53/1792 [08:58<4:54:00, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 53/1792 [08:59<4:54:00, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 53/1792 [08:59<4:54:00, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 53/1792 [08:58<4:54:00, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 53 is completed and loss is 0.28304389119148254\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 54/1792 [09:08<4:54:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 54/1792 [09:09<4:54:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 54/1792 [09:09<4:54:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 54/1792 [09:08<4:54:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 54 is completed and loss is 0.30577972531318665\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 55/1792 [09:18<4:54:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 55/1792 [09:19<4:54:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 55/1792 [09:18<4:54:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 55/1792 [09:19<4:54:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 55 is completed and loss is 0.2774852514266968\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 56/1792 [09:29<4:53:56, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 56/1792 [09:29<4:53:56, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 56/1792 [09:28<4:53:56, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 56/1792 [09:29<4:53:56, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 56 is completed and loss is 0.24646742641925812\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 57/1792 [09:39<4:53:44, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 57/1792 [09:39<4:53:44, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 57/1792 [09:39<4:53:44, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 57/1792 [09:39<4:53:44, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 57 is completed and loss is 0.3201758563518524\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 58/1792 [09:49<4:53:40, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 58/1792 [09:49<4:53:39, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 58/1792 [09:49<4:53:40, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 58/1792 [09:49<4:53:39, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 58 is completed and loss is 0.26852133870124817\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 59/1792 [09:59<4:53:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 59/1792 [10:00<4:53:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 59/1792 [10:00<4:53:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 59/1792 [09:59<4:53:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 59 is completed and loss is 0.28693607449531555\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 60/1792 [10:09<4:52:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 60/1792 [10:10<4:52:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 60/1792 [10:09<4:52:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 60/1792 [10:10<4:52:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 60 is completed and loss is 0.29870328307151794\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 61/1792 [10:19<4:52:39, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 61/1792 [10:20<4:52:38, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 61/1792 [10:20<4:52:39, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 61/1792 [10:19<4:52:39, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 61 is completed and loss is 0.28624430298805237\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 62/1792 [10:29<4:52:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 62/1792 [10:30<4:52:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 62/1792 [10:30<4:52:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   3%|#033[34m▎         #033[0m| 62/1792 [10:29<4:52:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 62 is completed and loss is 0.26658913493156433\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 63/1792 [10:40<4:52:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 63/1792 [10:40<4:52:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 63/1792 [10:40<4:52:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 63/1792 [10:40<4:52:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 63 is completed and loss is 0.2785007059574127\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 64/1792 [10:50<4:52:11, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 64/1792 [10:50<4:52:11, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 64/1792 [10:50<4:52:11, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 64/1792 [10:50<4:52:11, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 64 is completed and loss is 0.2898111641407013\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 65/1792 [11:00<4:53:10, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 65/1792 [11:01<4:53:10, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 65/1792 [11:00<4:53:10, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 65/1792 [11:01<4:53:11, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 65 is completed and loss is 0.2717457115650177\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 66/1792 [11:10<4:52:33, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 66/1792 [11:11<4:52:33, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 66/1792 [11:11<4:52:33, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 66/1792 [11:10<4:52:33, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 66 is completed and loss is 0.29285749793052673\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 67/1792 [11:20<4:52:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 67/1792 [11:21<4:52:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 67/1792 [11:21<4:52:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▎         #033[0m| 67/1792 [11:20<4:52:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 67 is completed and loss is 0.2824038565158844\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 68/1792 [11:30<4:51:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 68/1792 [11:31<4:51:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 68/1792 [11:31<4:51:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 68/1792 [11:30<4:51:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 68 is completed and loss is 0.28924959897994995\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 69/1792 [11:41<4:51:17, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 69/1792 [11:41<4:51:16, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 69/1792 [11:41<4:51:17, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 69/1792 [11:40<4:51:17, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 69 is completed and loss is 0.27884235978126526\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 70/1792 [11:51<4:51:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 70/1792 [11:51<4:51:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 70/1792 [11:51<4:51:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 70/1792 [11:51<4:51:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 70 is completed and loss is 0.27212831377983093\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 71/1792 [12:01<4:50:49, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 71/1792 [12:01<4:50:48, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 71/1792 [12:01<4:50:48, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 71/1792 [12:01<4:50:49, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 71 is completed and loss is 0.2577652335166931\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 72/1792 [12:11<4:50:41, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 72/1792 [12:11<4:50:41, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 72/1792 [12:12<4:50:41, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 72/1792 [12:12<4:50:42, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 72 is completed and loss is 0.2812502384185791\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 73/1792 [12:21<4:50:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 73/1792 [12:22<4:50:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 73/1792 [12:22<4:50:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 73/1792 [12:21<4:50:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 73 is completed and loss is 0.26136961579322815\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 74/1792 [12:31<4:50:16, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 74/1792 [12:32<4:50:16, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 74/1792 [12:32<4:50:17, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 74/1792 [12:31<4:50:17, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 74 is completed and loss is 0.23878906667232513\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 75/1792 [12:41<4:50:14, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 75/1792 [12:42<4:50:14, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 75/1792 [12:42<4:50:15, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 75/1792 [12:41<4:50:14, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 75 is completed and loss is 0.25366470217704773\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 76/1792 [12:52<4:50:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 76/1792 [12:52<4:50:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 76/1792 [12:52<4:50:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 76/1792 [12:51<4:50:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 76 is completed and loss is 0.31036046147346497\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 77/1792 [13:02<4:50:01, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 77/1792 [13:02<4:50:01, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 77/1792 [13:02<4:50:01, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 77/1792 [13:02<4:50:01, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 77 is completed and loss is 0.2523806095123291\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 78/1792 [13:12<4:49:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 78/1792 [13:12<4:49:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 78/1792 [13:12<4:49:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 78/1792 [13:12<4:49:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 78 is completed and loss is 0.2942736744880676\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 79/1792 [13:22<4:49:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 79/1792 [13:23<4:49:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 79/1792 [13:23<4:49:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 79/1792 [13:22<4:49:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 79 is completed and loss is 0.2909735441207886\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 80/1792 [13:32<4:49:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 80/1792 [13:33<4:49:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 80/1792 [13:33<4:49:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   4%|#033[34m▍         #033[0m| 80/1792 [13:32<4:49:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 80 is completed and loss is 0.24323616921901703\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 81/1792 [13:42<4:49:33, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 81/1792 [13:43<4:49:32, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 81/1792 [13:43<4:49:32, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 81/1792 [13:42<4:49:33, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 81 is completed and loss is 0.272919625043869\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 82/1792 [13:52<4:49:12, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 82/1792 [13:53<4:49:12, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 82/1792 [13:53<4:49:12, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 82/1792 [13:52<4:49:12, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 82 is completed and loss is 0.24494963884353638\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 83/1792 [14:03<4:49:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 83/1792 [14:03<4:49:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 83/1792 [14:03<4:49:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 83/1792 [14:03<4:49:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 83 is completed and loss is 0.2672450840473175\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 84/1792 [14:13<4:48:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 84/1792 [14:13<4:48:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 84/1792 [14:13<4:48:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 84/1792 [14:13<4:48:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 85/1792 [14:23<4:48:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 84 is completed and loss is 0.2386859506368637\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 85/1792 [14:23<4:48:40, 10.15s/it]#015Training Epoch0:   5%|#033[34m▍         #033[0m| 85/1792 [14:23<4:48:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 85/1792 [14:23<4:48:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 86/1792 [14:34<4:49:31, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 86/1792 [14:34<4:49:31, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 86/1792 [14:33<4:49:30, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 85 is completed and loss is 0.27099692821502686\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 86/1792 [14:33<4:49:31, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 87/1792 [14:44<4:48:56, 10.17s/it]#015Training Epoch0:   5%|#033[34m▍         #033[0m| 87/1792 [14:44<4:48:56, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 86 is completed and loss is 0.2618812322616577\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 87/1792 [14:43<4:48:56, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 87/1792 [14:43<4:48:56, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 88/1792 [14:53<4:48:29, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 87 is completed and loss is 0.26924702525138855\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 88/1792 [14:54<4:48:30, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 88/1792 [14:53<4:48:30, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 88/1792 [14:54<4:48:30, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 88 is completed and loss is 0.295878142118454\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 89/1792 [15:04<4:48:06, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 89/1792 [15:04<4:48:06, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 89/1792 [15:03<4:48:06, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▍         #033[0m| 89/1792 [15:04<4:48:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 90/1792 [15:14<4:47:50, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 89 is completed and loss is 0.30618295073509216\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 90/1792 [15:14<4:47:50, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 90/1792 [15:14<4:47:50, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 90/1792 [15:14<4:47:50, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 91/1792 [15:24<4:47:33, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 90 is completed and loss is 0.2434074878692627\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 91/1792 [15:24<4:47:33, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 91/1792 [15:24<4:47:33, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 91/1792 [15:24<4:47:34, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 92/1792 [15:34<4:47:22, 10.14s/it]#015Training Epoch0:   5%|#033[34m▌         #033[0m| 92/1792 [15:35<4:47:22, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 92/1792 [15:35<4:47:23, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 91 is completed and loss is 0.2808666527271271\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 92/1792 [15:34<4:47:23, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 93/1792 [15:45<4:47:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 93/1792 [15:44<4:47:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 93/1792 [15:45<4:47:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 92 is completed and loss is 0.2995931804180145\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 93/1792 [15:44<4:47:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 93 is completed and loss is 0.25762706995010376\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 94/1792 [15:54<4:47:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 94/1792 [15:55<4:47:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 94/1792 [15:55<4:47:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 94/1792 [15:54<4:47:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 94 is completed and loss is 0.24924564361572266\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 95/1792 [16:04<4:46:57, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 95/1792 [16:04<4:46:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 95/1792 [16:05<4:46:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 95/1792 [16:05<4:46:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 95 is completed and loss is 0.24242621660232544\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 96/1792 [16:15<4:46:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 96/1792 [16:15<4:46:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 96/1792 [16:14<4:46:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 96/1792 [16:15<4:46:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 96 is completed and loss is 0.284465491771698\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 97/1792 [16:25<4:47:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 97/1792 [16:25<4:47:42, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 97/1792 [16:25<4:47:42, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 97/1792 [16:25<4:47:42, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 97 is completed and loss is 0.293664813041687\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 98/1792 [16:35<4:47:08, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 98/1792 [16:36<4:47:08, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 98/1792 [16:36<4:47:08, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   5%|#033[34m▌         #033[0m| 98/1792 [16:35<4:47:09, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 98 is completed and loss is 0.2764715254306793\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 99/1792 [16:45<4:46:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 99/1792 [16:46<4:46:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 99/1792 [16:45<4:46:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 99/1792 [16:46<4:46:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 99 is completed and loss is 0.2519460618495941\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 100/1792 [16:55<4:46:22, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 100/1792 [16:56<4:46:22, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 100/1792 [16:56<4:46:22, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 100/1792 [16:55<4:46:22, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 100 is completed and loss is 0.26724904775619507\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 101/1792 [17:05<4:46:06, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 101/1792 [17:06<4:46:06, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 101/1792 [17:05<4:46:06, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 101/1792 [17:06<4:46:06, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 101 is completed and loss is 0.2898626923561096\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 102/1792 [17:16<4:45:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 102/1792 [17:16<4:45:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 102/1792 [17:16<4:45:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 102/1792 [17:15<4:45:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 102 is completed and loss is 0.23658381402492523\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 103/1792 [17:26<4:45:34, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 103/1792 [17:26<4:45:34, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 103/1792 [17:26<4:45:34, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 103/1792 [17:26<4:45:34, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 103 is completed and loss is 0.26755833625793457\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 104/1792 [17:36<4:45:18, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 104/1792 [17:36<4:45:18, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 104/1792 [17:36<4:45:18, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 104/1792 [17:36<4:45:18, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 104 is completed and loss is 0.23070085048675537\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 105/1792 [17:46<4:45:11, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 105/1792 [17:47<4:45:11, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 105/1792 [17:47<4:45:11, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 105/1792 [17:46<4:45:11, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 105 is completed and loss is 0.24993981420993805\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 106/1792 [17:56<4:46:00, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 106/1792 [17:57<4:46:00, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 106/1792 [17:57<4:46:00, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 106/1792 [17:56<4:46:01, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 106 is completed and loss is 0.2575882375240326\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 107/1792 [18:06<4:45:31, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 107/1792 [18:07<4:45:31, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 107/1792 [18:07<4:45:31, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 107/1792 [18:06<4:45:31, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 107 is completed and loss is 0.2736315429210663\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 108/1792 [18:17<4:46:16, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 108/1792 [18:17<4:46:16, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 108/1792 [18:17<4:46:16, 10.20s/it]#015Training Epoch0:   6%|#033[34m▌         #033[0m| 108/1792 [18:17<4:46:16, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 108 is completed and loss is 0.2737133800983429\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 109/1792 [18:27<4:45:36, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 109/1792 [18:27<4:45:36, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 109/1792 [18:27<4:45:36, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 109/1792 [18:27<4:45:36, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 109 is completed and loss is 0.2482760101556778\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 110/1792 [18:37<4:45:07, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 110/1792 [18:38<4:45:07, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 110/1792 [18:37<4:45:07, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 110/1792 [18:37<4:45:07, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 110 is completed and loss is 0.2284054160118103\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 111/1792 [18:47<4:44:42, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 111/1792 [18:48<4:44:43, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 111/1792 [18:48<4:44:43, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▌         #033[0m| 111/1792 [18:47<4:44:43, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 111 is completed and loss is 0.2483649104833603\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 112/1792 [18:57<4:44:23, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 112/1792 [18:58<4:44:23, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 112/1792 [18:58<4:44:23, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 112/1792 [18:57<4:44:23, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 112 is completed and loss is 0.3134951889514923\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 113/1792 [19:07<4:44:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 113/1792 [19:08<4:44:12, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 113/1792 [19:08<4:44:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 113/1792 [19:07<4:44:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 113 is completed and loss is 0.2255859225988388\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 114/1792 [19:18<4:43:54, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 114/1792 [19:17<4:43:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 114/1792 [19:18<4:43:54, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 114/1792 [19:18<4:43:54, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 114 is completed and loss is 0.2906777262687683\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 115/1792 [19:28<4:43:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 115/1792 [19:28<4:43:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 115/1792 [19:28<4:43:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 115/1792 [19:28<4:43:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 115 is completed and loss is 0.2772379517555237\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 116/1792 [19:38<4:43:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 116/1792 [19:38<4:43:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 116/1792 [19:38<4:43:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   6%|#033[34m▋         #033[0m| 116/1792 [19:38<4:43:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 116 is completed and loss is 0.2755046784877777\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 117/1792 [19:48<4:44:18, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 117/1792 [19:49<4:44:18, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 117/1792 [19:48<4:44:18, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 117/1792 [19:49<4:44:18, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 117 is completed and loss is 0.2988388240337372\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 118/1792 [19:59<4:43:48, 10.17s/it]#015Training Epoch0:   7%|#033[34m▋         #033[0m| 118/1792 [19:58<4:43:48, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 118/1792 [19:59<4:43:48, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 118/1792 [19:58<4:43:48, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 118 is completed and loss is 0.24289773404598236\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 119/1792 [20:08<4:43:33, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 119/1792 [20:08<4:43:33, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 119/1792 [20:09<4:43:34, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 119/1792 [20:09<4:43:34, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 119 is completed and loss is 0.2776067852973938\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 120/1792 [20:19<4:43:04, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 120/1792 [20:18<4:43:03, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 120/1792 [20:19<4:43:04, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 120/1792 [20:19<4:43:04, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 121/1792 [20:29<4:42:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 120 is completed and loss is 0.2591243088245392\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 121/1792 [20:29<4:42:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 121/1792 [20:29<4:42:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 121/1792 [20:29<4:42:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 121 is completed and loss is 0.2316398173570633\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 122/1792 [20:39<4:42:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 122/1792 [20:39<4:42:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 122/1792 [20:39<4:42:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 122/1792 [20:39<4:42:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 122 is completed and loss is 0.26483410596847534\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 123/1792 [20:49<4:42:21, 10.15s/it]#015Training Epoch0:   7%|#033[34m▋         #033[0m| 123/1792 [20:49<4:42:22, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 123/1792 [20:50<4:42:22, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 123/1792 [20:50<4:42:22, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 123 is completed and loss is 0.2500630021095276\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 124/1792 [20:59<4:42:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 124/1792 [20:59<4:42:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 124/1792 [21:00<4:42:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 124/1792 [21:00<4:42:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 124 is completed and loss is 0.29346296191215515\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 125/1792 [21:09<4:41:58, 10.15s/it]#015Training Epoch0:   7%|#033[34m▋         #033[0m| 125/1792 [21:09<4:41:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 125/1792 [21:10<4:41:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 125/1792 [21:10<4:41:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 126/1792 [21:19<4:42:56, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 125 is completed and loss is 0.2364121526479721\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 126/1792 [21:20<4:42:56, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 126/1792 [21:20<4:42:56, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 126/1792 [21:20<4:42:56, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 127/1792 [21:30<4:42:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 127/1792 [21:30<4:42:18, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 126 is completed and loss is 0.25991296768188477\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 127/1792 [21:30<4:42:19, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 127/1792 [21:30<4:42:19, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 128/1792 [21:40<4:42:53, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 128/1792 [21:40<4:42:54, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 127 is completed and loss is 0.24397356808185577\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 128/1792 [21:40<4:42:54, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 128/1792 [21:41<4:42:53, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 129/1792 [21:50<4:43:07, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 129/1792 [21:51<4:43:07, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 129/1792 [21:51<4:43:07, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 128 is completed and loss is 0.28250378370285034\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 129/1792 [21:50<4:43:08, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 130/1792 [22:00<4:42:19, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 129 is completed and loss is 0.25217297673225403\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 130/1792 [22:01<4:42:19, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 130/1792 [22:00<4:42:19, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 130/1792 [22:01<4:42:19, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 131/1792 [22:11<4:41:39, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 131/1792 [22:10<4:41:40, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 130 is completed and loss is 0.2568853199481964\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 131/1792 [22:10<4:41:39, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 131/1792 [22:11<4:41:40, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 132/1792 [22:20<4:41:06, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 131 is completed and loss is 0.23879064619541168\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 132/1792 [22:21<4:41:06, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 132/1792 [22:21<4:41:06, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 132/1792 [22:21<4:41:06, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 133/1792 [22:31<4:40:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 132 is completed and loss is 0.2877104878425598\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 133/1792 [22:31<4:40:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 133/1792 [22:31<4:40:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 133/1792 [22:31<4:40:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 133 is completed and loss is 0.2703353762626648\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 134/1792 [22:41<4:40:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 134/1792 [22:41<4:40:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 134/1792 [22:41<4:40:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   7%|#033[34m▋         #033[0m| 134/1792 [22:41<4:40:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 134 is completed and loss is 0.27145758271217346\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 135/1792 [22:51<4:40:05, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 135/1792 [22:51<4:40:06, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 135/1792 [22:52<4:40:06, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 135/1792 [22:52<4:40:05, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 135 is completed and loss is 0.23936963081359863\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 136/1792 [23:02<4:39:51, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 136/1792 [23:01<4:39:51, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 136/1792 [23:01<4:39:51, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 136/1792 [23:02<4:39:52, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 136 is completed and loss is 0.26102322340011597\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 137/1792 [23:12<4:40:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 137/1792 [23:11<4:40:42, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 137/1792 [23:11<4:40:42, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 137/1792 [23:12<4:40:42, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 137 is completed and loss is 0.26332348585128784\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 138/1792 [23:22<4:40:16, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 138/1792 [23:22<4:40:16, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 138/1792 [23:21<4:40:16, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 138/1792 [23:22<4:40:16, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 138 is completed and loss is 0.26062437891960144\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 139/1792 [23:32<4:40:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 139/1792 [23:32<4:40:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 139/1792 [23:32<4:40:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 139/1792 [23:32<4:40:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 139 is completed and loss is 0.2419876605272293\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 140/1792 [23:42<4:39:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 140/1792 [23:42<4:39:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 140/1792 [23:42<4:39:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 140/1792 [23:42<4:39:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 140 is completed and loss is 0.2696252763271332\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 141/1792 [23:52<4:39:26, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 141/1792 [23:52<4:39:26, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 141/1792 [23:53<4:39:26, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 141/1792 [23:53<4:39:27, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 141 is completed and loss is 0.2772282361984253\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 142/1792 [24:02<4:39:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 142/1792 [24:03<4:39:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 142/1792 [24:02<4:39:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 142/1792 [24:03<4:39:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 143/1792 [24:13<4:38:50, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 143/1792 [24:12<4:38:50, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 143/1792 [24:13<4:38:49, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 142 is completed and loss is 0.24588853120803833\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 143/1792 [24:12<4:38:51, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 144/1792 [24:23<4:38:33, 10.14s/it]#015Training Epoch0:   8%|#033[34m▊         #033[0m| 144/1792 [24:22<4:38:33, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 143 is completed and loss is 0.2690308690071106\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 144/1792 [24:23<4:38:34, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 144/1792 [24:22<4:38:34, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 145/1792 [24:32<4:38:19, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 145/1792 [24:33<4:38:19, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 144 is completed and loss is 0.24241571128368378\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 145/1792 [24:33<4:38:19, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 145/1792 [24:33<4:38:19, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 146/1792 [24:43<4:38:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 146/1792 [24:43<4:38:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 146/1792 [24:43<4:38:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 145 is completed and loss is 0.28863421082496643\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 146/1792 [24:43<4:38:13, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 147/1792 [24:53<4:38:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 147/1792 [24:53<4:38:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 147/1792 [24:53<4:38:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 146 is completed and loss is 0.2724919319152832\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 147/1792 [24:53<4:38:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 148/1792 [25:04<4:38:55, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 147 is completed and loss is 0.26444554328918457\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 148/1792 [25:04<4:38:55, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 148/1792 [25:03<4:38:55, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 148/1792 [25:03<4:38:56, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 149/1792 [25:14<4:39:35, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 149/1792 [25:13<4:39:35, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 149/1792 [25:14<4:39:35, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 148 is completed and loss is 0.28895315527915955\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 149/1792 [25:13<4:39:36, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 150/1792 [25:24<4:38:44, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 150/1792 [25:23<4:38:44, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 150/1792 [25:24<4:38:44, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 149 is completed and loss is 0.2800767123699188\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 150/1792 [25:24<4:38:44, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 151/1792 [25:34<4:38:14, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 151/1792 [25:34<4:38:14, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 150 is completed and loss is 0.27320998907089233\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 151/1792 [25:34<4:38:14, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 151/1792 [25:34<4:38:14, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 152/1792 [25:44<4:37:46, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 152/1792 [25:44<4:37:46, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 152/1792 [25:44<4:37:46, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 151 is completed and loss is 0.23717762529850006\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   8%|#033[34m▊         #033[0m| 152/1792 [25:44<4:37:46, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 153/1792 [25:54<4:37:24, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 153/1792 [25:54<4:37:24, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 153/1792 [25:54<4:37:24, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 152 is completed and loss is 0.2526540756225586\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 153/1792 [25:54<4:37:24, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 154/1792 [26:05<4:37:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 154/1792 [26:04<4:37:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 153 is completed and loss is 0.25551488995552063\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 154/1792 [26:04<4:37:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 154/1792 [26:05<4:37:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 155/1792 [26:15<4:36:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 155/1792 [26:15<4:36:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 154 is completed and loss is 0.220463365316391\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 155/1792 [26:14<4:36:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 155/1792 [26:14<4:36:54, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 156/1792 [26:25<4:36:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 156/1792 [26:25<4:36:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 155 is completed and loss is 0.31479495763778687\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 156/1792 [26:24<4:36:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▊         #033[0m| 156/1792 [26:24<4:36:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 157/1792 [26:35<4:36:32, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 157/1792 [26:35<4:36:32, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 157/1792 [26:34<4:36:32, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 156 is completed and loss is 0.25021523237228394\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 157/1792 [26:35<4:36:32, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 158/1792 [26:45<4:36:16, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 158/1792 [26:45<4:36:16, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 158/1792 [26:45<4:36:16, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 157 is completed and loss is 0.24276170134544373\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 158/1792 [26:45<4:36:16, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 159/1792 [26:55<4:36:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 158 is completed and loss is 0.2518460750579834\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 159/1792 [26:55<4:36:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 159/1792 [26:55<4:36:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 159/1792 [26:55<4:36:11, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 160/1792 [27:06<4:36:52, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 159 is completed and loss is 0.2472667098045349\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 160/1792 [27:05<4:36:52, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 160/1792 [27:06<4:36:53, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 160/1792 [27:05<4:36:53, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 161/1792 [27:16<4:36:14, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 161/1792 [27:15<4:36:14, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 160 is completed and loss is 0.2598336637020111\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 161/1792 [27:15<4:36:14, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 161/1792 [27:16<4:36:14, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 162/1792 [27:26<4:35:57, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 161 is completed and loss is 0.26231294870376587\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 162/1792 [27:25<4:35:57, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 162/1792 [27:25<4:35:57, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 162/1792 [27:26<4:35:57, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 163/1792 [27:36<4:35:32, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 162 is completed and loss is 0.2596290409564972\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 163/1792 [27:35<4:35:32, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 163/1792 [27:36<4:35:32, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 163/1792 [27:35<4:35:32, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 164/1792 [27:46<4:35:13, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 163 is completed and loss is 0.29394638538360596\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 164/1792 [27:46<4:35:13, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 164/1792 [27:46<4:35:13, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 164/1792 [27:45<4:35:13, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 165/1792 [27:56<4:35:01, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 165/1792 [27:56<4:35:00, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 165/1792 [27:56<4:35:01, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 164 is completed and loss is 0.24002858996391296\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 165/1792 [27:56<4:35:01, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 166/1792 [28:06<4:34:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 165 is completed and loss is 0.2771315574645996\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 166/1792 [28:06<4:34:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 166/1792 [28:06<4:34:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 166/1792 [28:06<4:34:55, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 167/1792 [28:17<4:34:43, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 166 is completed and loss is 0.2456330806016922\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 167/1792 [28:16<4:34:43, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 167/1792 [28:17<4:34:43, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 167/1792 [28:16<4:34:43, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 168/1792 [28:27<4:34:38, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 167 is completed and loss is 0.24658678472042084\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 168/1792 [28:26<4:34:38, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 168/1792 [28:26<4:34:38, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 168/1792 [28:27<4:34:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 169/1792 [28:37<4:35:15, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 169/1792 [28:37<4:35:15, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 169/1792 [28:36<4:35:15, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 168 is completed and loss is 0.20982028543949127\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 169/1792 [28:36<4:35:15, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 170/1792 [28:47<4:34:46, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 170/1792 [28:47<4:34:46, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 170/1792 [28:46<4:34:46, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 169 is completed and loss is 0.20621179044246674\u001b[0m\n",
      "\u001b[34mTraining Epoch0:   9%|#033[34m▉         #033[0m| 170/1792 [28:47<4:34:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 171/1792 [28:57<4:35:40, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 171/1792 [28:57<4:35:40, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 171/1792 [28:57<4:35:41, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 170 is completed and loss is 0.2575150728225708\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 171/1792 [28:57<4:35:41, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 172/1792 [29:08<4:35:00, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 172/1792 [29:08<4:35:01, 10.19s/it]#015Training Epoch0:  10%|#033[34m▉         #033[0m| 172/1792 [29:07<4:35:00, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 171 is completed and loss is 0.2523931562900543\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 172/1792 [29:07<4:35:00, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 173/1792 [29:18<4:34:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 172 is completed and loss is 0.27235838770866394\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 173/1792 [29:18<4:34:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 173/1792 [29:17<4:34:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 173/1792 [29:17<4:34:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 174/1792 [29:28<4:33:59, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 173 is completed and loss is 0.2611405551433563\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 174/1792 [29:27<4:33:59, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 174/1792 [29:27<4:33:59, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 174/1792 [29:28<4:33:59, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 175/1792 [29:38<4:33:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 175/1792 [29:38<4:33:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 174 is completed and loss is 0.2330600470304489\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 175/1792 [29:37<4:33:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 175/1792 [29:37<4:33:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 176/1792 [29:48<4:33:14, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 176/1792 [29:48<4:33:14, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 175 is completed and loss is 0.2762947082519531\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 176/1792 [29:48<4:33:14, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 176/1792 [29:47<4:33:14, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 177/1792 [29:58<4:33:06, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 176 is completed and loss is 0.25302016735076904\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 177/1792 [29:58<4:33:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 177/1792 [29:58<4:33:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 177/1792 [29:58<4:33:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 178/1792 [30:08<4:32:50, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 177 is completed and loss is 0.24055427312850952\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 178/1792 [30:08<4:32:50, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 178/1792 [30:08<4:32:50, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 178/1792 [30:08<4:32:50, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 179/1792 [30:18<4:32:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 178 is completed and loss is 0.2029615193605423\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 179/1792 [30:18<4:32:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m▉         #033[0m| 179/1792 [30:19<4:32:45, 10.15s/it]#015Training Epoch0:  10%|#033[34m▉         #033[0m| 179/1792 [30:18<4:32:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 180/1792 [30:29<4:33:35, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 179 is completed and loss is 0.24259230494499207\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 180/1792 [30:28<4:33:35, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 180/1792 [30:29<4:33:35, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 180/1792 [30:28<4:33:35, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 181/1792 [30:39<4:33:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 181/1792 [30:39<4:33:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 180 is completed and loss is 0.26271650195121765\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 181/1792 [30:38<4:33:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 181/1792 [30:38<4:33:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 182/1792 [30:49<4:32:50, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 181 is completed and loss is 0.24121183156967163\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 182/1792 [30:49<4:32:50, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 182/1792 [30:49<4:32:50, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 182/1792 [30:48<4:32:51, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 183/1792 [30:59<4:32:23, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 182 is completed and loss is 0.24342377483844757\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 183/1792 [30:59<4:32:22, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 183/1792 [30:59<4:32:22, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 183/1792 [30:59<4:32:23, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 184/1792 [31:09<4:32:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 183 is completed and loss is 0.26779666543006897\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 184/1792 [31:09<4:32:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 184/1792 [31:09<4:32:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 184/1792 [31:09<4:32:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 185/1792 [31:19<4:31:51, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 185/1792 [31:20<4:31:51, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 184 is completed and loss is 0.2543759346008301\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 185/1792 [31:19<4:31:51, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 185/1792 [31:19<4:31:51, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 186/1792 [31:30<4:31:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 186/1792 [31:30<4:31:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 185 is completed and loss is 0.26788750290870667\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 186/1792 [31:29<4:31:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 186/1792 [31:29<4:31:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 187/1792 [31:40<4:31:30, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 187/1792 [31:39<4:31:29, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 187/1792 [31:40<4:31:30, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 186 is completed and loss is 0.26800549030303955\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 187/1792 [31:39<4:31:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 188/1792 [31:50<4:31:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 188/1792 [31:50<4:31:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 187 is completed and loss is 0.2560296356678009\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 188/1792 [31:49<4:31:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  10%|#033[34m█         #033[0m| 188/1792 [31:49<4:31:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 189/1792 [32:00<4:32:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 189/1792 [32:00<4:32:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 188 is completed and loss is 0.2670351266860962\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 189/1792 [32:00<4:32:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 189/1792 [32:00<4:32:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 189 is completed and loss is 0.2385796755552292\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 190/1792 [32:10<4:31:38, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 190/1792 [32:10<4:31:38, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 190/1792 [32:10<4:31:39, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 190/1792 [32:10<4:31:40, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 191/1792 [32:21<4:32:23, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 191/1792 [32:21<4:32:23, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 190 is completed and loss is 0.22959469258785248\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 191/1792 [32:20<4:32:23, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 191/1792 [32:20<4:32:23, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 191 is completed and loss is 0.25695502758026123\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 192/1792 [32:30<4:32:29, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 192/1792 [32:31<4:32:29, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 192/1792 [32:30<4:32:29, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 192/1792 [32:31<4:32:30, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 192 is completed and loss is 0.2548273503780365\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 193/1792 [32:40<4:31:38, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 193/1792 [32:41<4:31:39, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 193/1792 [32:41<4:31:39, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 193/1792 [32:40<4:31:39, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 194/1792 [32:51<4:30:59, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 193 is completed and loss is 0.25790977478027344\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 194/1792 [32:51<4:30:59, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 194/1792 [32:50<4:30:59, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 194/1792 [32:51<4:31:00, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 195/1792 [33:01<4:30:27, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 194 is completed and loss is 0.19319844245910645\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 195/1792 [33:01<4:30:28, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 195/1792 [33:01<4:30:28, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 195/1792 [33:01<4:30:28, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 195 is completed and loss is 0.21976590156555176\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 196/1792 [33:11<4:30:17, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 196/1792 [33:11<4:30:18, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 196/1792 [33:11<4:30:18, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 196/1792 [33:11<4:30:18, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 196 is completed and loss is 0.2685530483722687\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 197/1792 [33:21<4:29:59, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 197/1792 [33:22<4:29:59, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 197/1792 [33:22<4:29:59, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 197/1792 [33:21<4:29:59, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 197 is completed and loss is 0.27214789390563965\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 198/1792 [33:31<4:29:49, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 198/1792 [33:32<4:29:49, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 198/1792 [33:32<4:29:50, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 198/1792 [33:31<4:29:50, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 198 is completed and loss is 0.2567421495914459\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 199/1792 [33:41<4:29:39, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 199/1792 [33:42<4:29:39, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 199/1792 [33:41<4:29:39, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 199/1792 [33:42<4:29:39, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 199 is completed and loss is 0.22157922387123108\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 200/1792 [33:52<4:30:27, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 200/1792 [33:52<4:30:27, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 200/1792 [33:52<4:30:27, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 200/1792 [33:52<4:30:27, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 200 is completed and loss is 0.1705818921327591\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 201/1792 [34:02<4:29:50, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 201/1792 [34:02<4:29:50, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 201/1792 [34:02<4:29:50, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█         #033[0m| 201/1792 [34:02<4:29:50, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 201 is completed and loss is 0.2770748436450958\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 202/1792 [34:12<4:29:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 202/1792 [34:12<4:29:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 202/1792 [34:12<4:29:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 202/1792 [34:12<4:29:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 202 is completed and loss is 0.2442765235900879\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 203/1792 [34:22<4:30:12, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 203/1792 [34:23<4:30:11, 10.20s/it]#015Training Epoch0:  11%|#033[34m█▏        #033[0m| 203/1792 [34:23<4:30:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 203/1792 [34:22<4:30:12, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 203 is completed and loss is 0.22403401136398315\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 204/1792 [34:32<4:29:31, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 204/1792 [34:33<4:29:30, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 204/1792 [34:33<4:29:31, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 204/1792 [34:32<4:29:30, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 205/1792 [34:43<4:29:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 204 is completed and loss is 0.26003825664520264\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 205/1792 [34:42<4:29:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 205/1792 [34:42<4:29:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 205/1792 [34:43<4:29:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 206/1792 [34:53<4:28:32, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 206/1792 [34:53<4:28:31, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 205 is completed and loss is 0.2173238843679428\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 206/1792 [34:53<4:28:31, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  11%|#033[34m█▏        #033[0m| 206/1792 [34:53<4:28:32, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 207/1792 [35:03<4:28:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 206 is completed and loss is 0.24827872216701508\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 207/1792 [35:03<4:28:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 207/1792 [35:03<4:28:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 207/1792 [35:03<4:28:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 207 is completed and loss is 0.26657634973526\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 208/1792 [35:13<4:27:56, 10.15s/it]#015Training Epoch0:  12%|#033[34m█▏        #033[0m| 208/1792 [35:13<4:27:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 208/1792 [35:13<4:27:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 208/1792 [35:13<4:27:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 209/1792 [35:24<4:27:43, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 209/1792 [35:23<4:27:44, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 209/1792 [35:24<4:27:44, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 208 is completed and loss is 0.2549227476119995\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 209/1792 [35:23<4:27:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 210/1792 [35:34<4:27:25, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 209 is completed and loss is 0.2367766797542572\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 210/1792 [35:33<4:27:26, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 210/1792 [35:33<4:27:26, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 210/1792 [35:34<4:27:26, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 211/1792 [35:44<4:28:10, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 211/1792 [35:43<4:28:10, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 210 is completed and loss is 0.2619703412055969\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 211/1792 [35:43<4:28:11, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 211/1792 [35:44<4:28:11, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 212/1792 [35:54<4:28:40, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 211 is completed and loss is 0.30705970525741577\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 212/1792 [35:54<4:28:40, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 212/1792 [35:54<4:28:40, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 212/1792 [35:54<4:28:40, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 213/1792 [36:04<4:28:04, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 213/1792 [36:04<4:28:05, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 212 is completed and loss is 0.28254202008247375\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 213/1792 [36:04<4:28:04, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 213/1792 [36:04<4:28:05, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 213 is completed and loss is 0.22117136418819427\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 214/1792 [36:15<4:28:30, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 214/1792 [36:14<4:28:30, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 214/1792 [36:14<4:28:31, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 214/1792 [36:15<4:28:31, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 214 is completed and loss is 0.24875345826148987\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 215/1792 [36:24<4:27:45, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 215/1792 [36:24<4:27:45, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 215/1792 [36:25<4:27:45, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 215/1792 [36:25<4:27:45, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 215 is completed and loss is 0.23619435727596283\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 216/1792 [36:35<4:27:11, 10.17s/it]#015Training Epoch0:  12%|#033[34m█▏        #033[0m| 216/1792 [36:35<4:27:12, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 216/1792 [36:34<4:27:11, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 216/1792 [36:34<4:27:11, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 216 is completed and loss is 0.2528822422027588\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 217/1792 [36:45<4:26:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 217/1792 [36:45<4:26:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 217/1792 [36:44<4:26:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 217/1792 [36:45<4:26:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 217 is completed and loss is 0.2134096622467041\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 218/1792 [36:55<4:26:37, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 218/1792 [36:55<4:26:37, 10.16s/it]#015Training Epoch0:  12%|#033[34m█▏        #033[0m| 218/1792 [36:55<4:26:37, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 218/1792 [36:55<4:26:37, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 219/1792 [37:05<4:26:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 219/1792 [37:05<4:26:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 219/1792 [37:05<4:26:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 218 is completed and loss is 0.27773982286453247\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 219/1792 [37:05<4:26:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 219 is completed and loss is 0.27643153071403503\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 220/1792 [37:15<4:25:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 220/1792 [37:15<4:25:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 220/1792 [37:15<4:25:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 220/1792 [37:16<4:25:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 221/1792 [37:25<4:25:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 221/1792 [37:26<4:25:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 220 is completed and loss is 0.22464004158973694\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 221/1792 [37:25<4:25:40, 10.15s/it]#015Training Epoch0:  12%|#033[34m█▏        #033[0m| 221/1792 [37:26<4:25:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 222/1792 [37:36<4:25:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 222/1792 [37:35<4:25:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 221 is completed and loss is 0.26370665431022644\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 222/1792 [37:35<4:25:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 222/1792 [37:36<4:25:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 223/1792 [37:46<4:26:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 223/1792 [37:46<4:26:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 223/1792 [37:45<4:26:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 222 is completed and loss is 0.2644503116607666\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▏        #033[0m| 223/1792 [37:46<4:26:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▎        #033[0m| 224/1792 [37:56<4:25:47, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▎        #033[0m| 224/1792 [37:56<4:25:47, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 223 is completed and loss is 0.2761056125164032\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▎        #033[0m| 224/1792 [37:56<4:25:47, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  12%|#033[34m█▎        #033[0m| 224/1792 [37:56<4:25:47, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 225/1792 [38:06<4:25:32, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 224 is completed and loss is 0.26877009868621826\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 225/1792 [38:06<4:25:32, 10.17s/it]#015Training Epoch0:  13%|#033[34m█▎        #033[0m| 225/1792 [38:06<4:25:32, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 225/1792 [38:06<4:25:32, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 226/1792 [38:16<4:25:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 226/1792 [38:17<4:25:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 225 is completed and loss is 0.2777789831161499\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 226/1792 [38:16<4:25:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 226/1792 [38:16<4:25:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 227/1792 [38:27<4:24:41, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 227/1792 [38:27<4:24:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 227/1792 [38:26<4:24:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 226 is completed and loss is 0.25874030590057373\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 227/1792 [38:26<4:24:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 228/1792 [38:37<4:24:29, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 227 is completed and loss is 0.2548959255218506\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 228/1792 [38:36<4:24:29, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 228/1792 [38:37<4:24:29, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 228/1792 [38:36<4:24:30, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 229/1792 [38:47<4:24:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 228 is completed and loss is 0.2721169590950012\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 229/1792 [38:47<4:24:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 229/1792 [38:46<4:24:12, 10.14s/it]#015Training Epoch0:  13%|#033[34m█▎        #033[0m| 229/1792 [38:46<4:24:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 230/1792 [38:57<4:23:58, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 229 is completed and loss is 0.25924152135849\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 230/1792 [38:57<4:23:58, 10.14s/it]#015Training Epoch0:  13%|#033[34m█▎        #033[0m| 230/1792 [38:56<4:23:58, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 230/1792 [38:56<4:23:58, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 231/1792 [39:07<4:23:45, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 230 is completed and loss is 0.298301100730896\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 231/1792 [39:07<4:23:45, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 231/1792 [39:07<4:23:45, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 231/1792 [39:07<4:23:45, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 231 is completed and loss is 0.2630050778388977\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 232/1792 [39:17<4:24:27, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 232/1792 [39:17<4:24:27, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 232/1792 [39:17<4:24:27, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 232/1792 [39:17<4:24:27, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 232 is completed and loss is 0.24987898766994476\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 233/1792 [39:27<4:24:09, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 233/1792 [39:28<4:24:09, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 233/1792 [39:27<4:24:09, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 233/1792 [39:28<4:24:09, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 233 is completed and loss is 0.2294318974018097\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 234/1792 [39:38<4:24:42, 10.19s/it]#015Training Epoch0:  13%|#033[34m█▎        #033[0m| 234/1792 [39:38<4:24:42, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 234/1792 [39:37<4:24:42, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 234/1792 [39:37<4:24:42, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 234 is completed and loss is 0.2677507996559143\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 235/1792 [39:48<4:25:01, 10.21s/it]#015Training Epoch0:  13%|#033[34m█▎        #033[0m| 235/1792 [39:48<4:25:01, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 235/1792 [39:48<4:25:01, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 235/1792 [39:47<4:25:01, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 235 is completed and loss is 0.2739482820034027\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 236/1792 [39:58<4:24:28, 10.20s/it]#015Training Epoch0:  13%|#033[34m█▎        #033[0m| 236/1792 [39:58<4:24:28, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 236/1792 [39:58<4:24:28, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 236/1792 [39:58<4:24:28, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 237/1792 [40:08<4:23:50, 10.18s/it]#015Training Epoch0:  13%|#033[34m█▎        #033[0m| 237/1792 [40:08<4:23:50, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 236 is completed and loss is 0.23881644010543823\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 237/1792 [40:08<4:23:50, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 237/1792 [40:08<4:23:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 238/1792 [40:19<4:23:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 238/1792 [40:19<4:23:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 237 is completed and loss is 0.2678295969963074\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 238/1792 [40:18<4:23:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 238/1792 [40:18<4:23:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 239/1792 [40:29<4:23:00, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 239/1792 [40:28<4:22:59, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 238 is completed and loss is 0.2491314858198166\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 239/1792 [40:28<4:23:00, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 239/1792 [40:29<4:23:00, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 240/1792 [40:39<4:22:34, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 240/1792 [40:38<4:22:34, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 239 is completed and loss is 0.1941196620464325\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 240/1792 [40:39<4:22:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 240/1792 [40:38<4:22:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 241/1792 [40:48<4:22:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 241/1792 [40:49<4:22:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 240 is completed and loss is 0.25809627771377563\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 241/1792 [40:49<4:22:18, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  13%|#033[34m█▎        #033[0m| 241/1792 [40:48<4:22:18, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 242/1792 [40:59<4:22:06, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 241 is completed and loss is 0.2360016256570816\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 242/1792 [40:58<4:22:06, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 242/1792 [40:59<4:22:06, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 242/1792 [40:59<4:22:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 243/1792 [41:09<4:21:59, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 243/1792 [41:09<4:21:59, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 243/1792 [41:09<4:22:00, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 242 is completed and loss is 0.26960742473602295\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 243/1792 [41:09<4:22:00, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 244/1792 [41:19<4:21:50, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 244/1792 [41:19<4:21:50, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 244/1792 [41:19<4:21:50, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 243 is completed and loss is 0.22661899030208588\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 244/1792 [41:19<4:21:50, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 245/1792 [41:30<4:21:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 245/1792 [41:29<4:21:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 244 is completed and loss is 0.26329633593559265\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 245/1792 [41:29<4:21:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 245/1792 [41:30<4:21:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 246/1792 [41:40<4:21:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 246/1792 [41:39<4:21:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 245 is completed and loss is 0.22212964296340942\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 246/1792 [41:40<4:21:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▎        #033[0m| 246/1792 [41:39<4:21:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 247/1792 [41:49<4:21:17, 10.15s/it]#015Training Epoch0:  14%|#033[34m█▍        #033[0m| 247/1792 [41:50<4:21:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 247/1792 [41:50<4:21:18, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 246 is completed and loss is 0.2730030417442322\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 247/1792 [41:49<4:21:18, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 248/1792 [42:00<4:21:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 248/1792 [41:59<4:21:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 247 is completed and loss is 0.2471446543931961\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 248/1792 [41:59<4:21:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 248/1792 [42:00<4:21:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 249/1792 [42:10<4:20:51, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 248 is completed and loss is 0.23873531818389893\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 249/1792 [42:10<4:20:51, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 249/1792 [42:10<4:20:51, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 249/1792 [42:09<4:20:52, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 250/1792 [42:20<4:20:36, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 250/1792 [42:20<4:20:36, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 249 is completed and loss is 0.26508548855781555\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 250/1792 [42:20<4:20:36, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 250/1792 [42:20<4:20:36, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 251/1792 [42:30<4:20:19, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 251/1792 [42:30<4:20:19, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 250 is completed and loss is 0.2318202406167984\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 251/1792 [42:30<4:20:19, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 251/1792 [42:30<4:20:19, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 252/1792 [42:41<4:21:05, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 252/1792 [42:40<4:21:05, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 251 is completed and loss is 0.27595800161361694\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 252/1792 [42:41<4:21:05, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 252/1792 [42:40<4:21:05, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 253/1792 [42:50<4:20:38, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 253/1792 [42:51<4:20:39, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 253/1792 [42:51<4:20:39, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 252 is completed and loss is 0.2502620816230774\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 253/1792 [42:50<4:20:39, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 254/1792 [43:01<4:20:19, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 254/1792 [43:00<4:20:19, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 254/1792 [43:01<4:20:19, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 253 is completed and loss is 0.24470652639865875\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 254/1792 [43:00<4:20:19, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 255/1792 [43:11<4:21:01, 10.19s/it]#015Training Epoch0:  14%|#033[34m█▍        #033[0m| 255/1792 [43:11<4:21:01, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 255/1792 [43:11<4:21:02, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 254 is completed and loss is 0.2777404189109802\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 255/1792 [43:11<4:21:02, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 256/1792 [43:21<4:20:29, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 256/1792 [43:21<4:20:29, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 255 is completed and loss is 0.24459289014339447\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 256/1792 [43:21<4:20:29, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 256/1792 [43:21<4:20:29, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 257/1792 [43:31<4:19:53, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 257/1792 [43:31<4:19:53, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 256 is completed and loss is 0.2509321868419647\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 257/1792 [43:31<4:19:53, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 257/1792 [43:31<4:19:53, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 258/1792 [43:42<4:19:34, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 258/1792 [43:41<4:19:34, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 257 is completed and loss is 0.2552621066570282\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 258/1792 [43:41<4:19:34, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 258/1792 [43:42<4:19:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 259/1792 [43:52<4:19:20, 10.15s/it]#015Training Epoch0:  14%|#033[34m█▍        #033[0m| 259/1792 [43:51<4:19:20, 10.15s/it]#015Training Epoch0:  14%|#033[34m█▍        #033[0m| 259/1792 [43:52<4:19:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 258 is completed and loss is 0.2589748799800873\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  14%|#033[34m█▍        #033[0m| 259/1792 [43:51<4:19:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 260/1792 [44:02<4:19:04, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 260/1792 [44:01<4:19:04, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 260/1792 [44:02<4:19:04, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 259 is completed and loss is 0.270397812128067\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 260/1792 [44:01<4:19:04, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 261/1792 [44:12<4:18:45, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 261/1792 [44:11<4:18:46, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 261/1792 [44:12<4:18:46, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 260 is completed and loss is 0.2527356445789337\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 261/1792 [44:11<4:18:46, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 262/1792 [44:22<4:18:40, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 261 is completed and loss is 0.2566351294517517\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 262/1792 [44:22<4:18:40, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 262/1792 [44:21<4:18:41, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 262/1792 [44:22<4:18:41, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 263/1792 [44:32<4:19:20, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 263/1792 [44:32<4:19:20, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 262 is completed and loss is 0.25395214557647705\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 263/1792 [44:32<4:19:20, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 263/1792 [44:32<4:19:20, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 264/1792 [44:43<4:18:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 264/1792 [44:42<4:18:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 263 is completed and loss is 0.2532900273799896\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 264/1792 [44:42<4:18:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 264/1792 [44:43<4:18:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 264 is completed and loss is 0.25443965196609497\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 265/1792 [44:52<4:18:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 265/1792 [44:53<4:18:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 265/1792 [44:52<4:18:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 265/1792 [44:53<4:18:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 266/1792 [45:03<4:18:21, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 265 is completed and loss is 0.24933908879756927\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 266/1792 [45:02<4:18:21, 10.16s/it]#015Training Epoch0:  15%|#033[34m█▍        #033[0m| 266/1792 [45:02<4:18:21, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 266/1792 [45:03<4:18:21, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 267/1792 [45:13<4:17:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 266 is completed and loss is 0.28193530440330505\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 267/1792 [45:12<4:17:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 267/1792 [45:12<4:17:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 267/1792 [45:13<4:17:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 268/1792 [45:23<4:17:37, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 267 is completed and loss is 0.2566637694835663\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 268/1792 [45:23<4:17:37, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 268/1792 [45:23<4:17:38, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▍        #033[0m| 268/1792 [45:22<4:17:38, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 268 is completed and loss is 0.26019319891929626\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 269/1792 [45:33<4:17:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 269/1792 [45:33<4:17:30, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 269/1792 [45:33<4:17:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 269/1792 [45:33<4:17:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 270/1792 [45:43<4:17:15, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 269 is completed and loss is 0.21951594948768616\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 270/1792 [45:43<4:17:15, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 270/1792 [45:43<4:17:15, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 270/1792 [45:43<4:17:15, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 271/1792 [45:54<4:16:55, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 270 is completed and loss is 0.2589816451072693\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 271/1792 [45:53<4:16:55, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 271/1792 [45:53<4:16:55, 10.14s/it]#015Training Epoch0:  15%|#033[34m█▌        #033[0m| 271/1792 [45:53<4:16:55, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 272/1792 [46:04<4:16:49, 10.14s/it]#015Training Epoch0:  15%|#033[34m█▌        #033[0m| 272/1792 [46:03<4:16:49, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 271 is completed and loss is 0.23127593100070953\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 272/1792 [46:04<4:16:49, 10.14s/it]#015Training Epoch0:  15%|#033[34m█▌        #033[0m| 272/1792 [46:03<4:16:49, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 273/1792 [46:14<4:16:35, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 272 is completed and loss is 0.25249454379081726\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 273/1792 [46:14<4:16:35, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 273/1792 [46:13<4:16:35, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 273/1792 [46:13<4:16:35, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 274/1792 [46:24<4:17:26, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 273 is completed and loss is 0.2689993977546692\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 274/1792 [46:23<4:17:26, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 274/1792 [46:23<4:17:26, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 274/1792 [46:24<4:17:27, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 275/1792 [46:34<4:17:51, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 274 is completed and loss is 0.23869766294956207\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 275/1792 [46:34<4:17:51, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 275/1792 [46:34<4:17:51, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 275/1792 [46:34<4:17:51, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 275 is completed and loss is 0.2294217050075531\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 276/1792 [46:44<4:17:09, 10.18s/it]#015Training Epoch0:  15%|#033[34m█▌        #033[0m| 276/1792 [46:44<4:17:09, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 276/1792 [46:44<4:17:09, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 276/1792 [46:44<4:17:09, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 276 is completed and loss is 0.2397652119398117\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 277/1792 [46:54<4:16:40, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 277/1792 [46:55<4:16:41, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  15%|#033[34m█▌        #033[0m| 277/1792 [46:55<4:16:41, 10.17s/it]#015Training Epoch0:  15%|#033[34m█▌        #033[0m| 277/1792 [46:54<4:16:41, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 278/1792 [47:05<4:16:13, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 277 is completed and loss is 0.2945839762687683\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 278/1792 [47:04<4:16:14, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 278/1792 [47:05<4:16:14, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 278/1792 [47:04<4:16:14, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 278 is completed and loss is 0.24970315396785736\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 279/1792 [47:15<4:15:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 279/1792 [47:14<4:15:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 279/1792 [47:14<4:15:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 279/1792 [47:15<4:15:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 279 is completed and loss is 0.24679702520370483\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 280/1792 [47:25<4:15:41, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 280/1792 [47:24<4:15:41, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 280/1792 [47:25<4:15:41, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 280/1792 [47:24<4:15:41, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 280 is completed and loss is 0.2756456434726715\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 281/1792 [47:35<4:15:29, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 281/1792 [47:35<4:15:29, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 281/1792 [47:35<4:15:29, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 281/1792 [47:34<4:15:29, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 281 is completed and loss is 0.2509159743785858\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 282/1792 [47:45<4:15:14, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 282/1792 [47:45<4:15:15, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 282/1792 [47:45<4:15:14, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 282/1792 [47:45<4:15:15, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 282 is completed and loss is 0.27961963415145874\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 283/1792 [47:55<4:15:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 283/1792 [47:55<4:15:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 283/1792 [47:55<4:15:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 283/1792 [47:55<4:15:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 284/1792 [48:06<4:14:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 283 is completed and loss is 0.23641102015972137\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 284/1792 [48:05<4:14:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 284/1792 [48:06<4:14:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 284/1792 [48:05<4:14:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 284 is completed and loss is 0.2252509593963623\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 285/1792 [48:15<4:14:42, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 285/1792 [48:15<4:14:42, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 285/1792 [48:16<4:14:42, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 285/1792 [48:16<4:14:43, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 285 is completed and loss is 0.23161567747592926\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 286/1792 [48:25<4:15:32, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 286/1792 [48:26<4:15:32, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 286/1792 [48:25<4:15:32, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 286/1792 [48:26<4:15:32, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 286 is completed and loss is 0.2883060872554779\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 287/1792 [48:36<4:14:59, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 287/1792 [48:36<4:14:59, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 287/1792 [48:36<4:14:59, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 287/1792 [48:35<4:14:59, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 287 is completed and loss is 0.23180918395519257\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 288/1792 [48:46<4:14:48, 10.17s/it]#015Training Epoch0:  16%|#033[34m█▌        #033[0m| 288/1792 [48:46<4:14:48, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 288/1792 [48:46<4:14:48, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 288/1792 [48:46<4:14:49, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 289/1792 [48:56<4:14:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 288 is completed and loss is 0.28243008255958557\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 289/1792 [48:56<4:14:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 289/1792 [48:56<4:14:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 289/1792 [48:56<4:14:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 289 is completed and loss is 0.2865302562713623\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 290/1792 [49:06<4:14:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 290/1792 [49:06<4:14:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 290/1792 [49:07<4:14:14, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 290/1792 [49:06<4:14:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 290 is completed and loss is 0.23687106370925903\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 291/1792 [49:16<4:13:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 291/1792 [49:17<4:13:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 291/1792 [49:17<4:13:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▌        #033[0m| 291/1792 [49:16<4:13:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 291 is completed and loss is 0.23457950353622437\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 292/1792 [49:26<4:13:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 292/1792 [49:27<4:13:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 292/1792 [49:27<4:13:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 292/1792 [49:26<4:13:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 292 is completed and loss is 0.2809922695159912\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 293/1792 [49:36<4:13:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 293/1792 [49:37<4:13:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 293/1792 [49:37<4:13:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 293/1792 [49:36<4:13:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 293 is completed and loss is 0.2613550126552582\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 294/1792 [49:47<4:13:19, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 294/1792 [49:47<4:13:19, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 294/1792 [49:47<4:13:19, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 294/1792 [49:46<4:13:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 295/1792 [49:57<4:14:04, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 294 is completed and loss is 0.24230672419071198\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 295/1792 [49:57<4:14:05, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 295/1792 [49:57<4:14:05, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  16%|#033[34m█▋        #033[0m| 295/1792 [49:57<4:14:05, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 296/1792 [50:07<4:13:33, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 295 is completed and loss is 0.2130882889032364\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 296/1792 [50:07<4:13:34, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 296/1792 [50:07<4:13:34, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 296/1792 [50:07<4:13:34, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 297/1792 [50:18<4:14:05, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 296 is completed and loss is 0.2663646340370178\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 297/1792 [50:18<4:14:05, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 297/1792 [50:17<4:14:05, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 297/1792 [50:17<4:14:05, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 298/1792 [50:28<4:13:30, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 298/1792 [50:28<4:13:30, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 297 is completed and loss is 0.20089201629161835\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 298/1792 [50:27<4:13:30, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 298/1792 [50:27<4:13:31, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 299/1792 [50:38<4:13:02, 10.17s/it]#015Training Epoch0:  17%|#033[34m█▋        #033[0m| 299/1792 [50:38<4:13:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 298 is completed and loss is 0.23429226875305176\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 299/1792 [50:37<4:13:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 299/1792 [50:37<4:13:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 300/1792 [50:48<4:12:36, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 300/1792 [50:48<4:12:36, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 300/1792 [50:48<4:12:36, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 299 is completed and loss is 0.22970055043697357\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 300/1792 [50:48<4:12:37, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 301/1792 [50:58<4:12:19, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 301/1792 [50:58<4:12:19, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 300 is completed and loss is 0.2611217796802521\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 301/1792 [50:58<4:12:19, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 301/1792 [50:58<4:12:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 302/1792 [51:08<4:12:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 302/1792 [51:08<4:12:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 302/1792 [51:08<4:12:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 301 is completed and loss is 0.2084806114435196\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 302/1792 [51:08<4:12:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 303/1792 [51:19<4:11:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 302 is completed and loss is 0.2536929249763489\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 303/1792 [51:18<4:11:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 303/1792 [51:19<4:11:57, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 303/1792 [51:18<4:11:57, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 304/1792 [51:29<4:11:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 303 is completed and loss is 0.2514212727546692\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 304/1792 [51:28<4:11:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 304/1792 [51:29<4:11:43, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 304/1792 [51:28<4:11:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 305/1792 [51:39<4:11:30, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 304 is completed and loss is 0.24728398025035858\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 305/1792 [51:38<4:11:30, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 305/1792 [51:39<4:11:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 305/1792 [51:38<4:11:31, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 306/1792 [51:49<4:11:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 305 is completed and loss is 0.25599658489227295\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 306/1792 [51:48<4:11:15, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 306/1792 [51:48<4:11:15, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 306/1792 [51:49<4:11:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 307/1792 [51:59<4:11:05, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 306 is completed and loss is 0.23272958397865295\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 307/1792 [51:59<4:11:05, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 307/1792 [51:59<4:11:05, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 307/1792 [51:59<4:11:05, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 308/1792 [52:09<4:10:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 307 is completed and loss is 0.26089850068092346\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 308/1792 [52:09<4:10:57, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 308/1792 [52:09<4:10:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 308/1792 [52:09<4:10:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 309/1792 [52:19<4:10:44, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 309/1792 [52:19<4:10:43, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 308 is completed and loss is 0.2496703416109085\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 309/1792 [52:19<4:10:43, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 309/1792 [52:19<4:10:43, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 310/1792 [52:30<4:10:28, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 309 is completed and loss is 0.27898699045181274\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 310/1792 [52:29<4:10:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 310/1792 [52:30<4:10:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 310/1792 [52:29<4:10:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 311/1792 [52:40<4:10:25, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 310 is completed and loss is 0.2818261981010437\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 311/1792 [52:40<4:10:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 311/1792 [52:39<4:10:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 311/1792 [52:39<4:10:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 312/1792 [52:50<4:10:04, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 312/1792 [52:50<4:10:05, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 311 is completed and loss is 0.2683296799659729\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 312/1792 [52:49<4:10:05, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 312/1792 [52:49<4:10:05, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 313/1792 [53:00<4:09:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 313/1792 [53:00<4:09:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 313/1792 [52:59<4:09:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 312 is completed and loss is 0.2544632852077484\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  17%|#033[34m█▋        #033[0m| 313/1792 [52:59<4:09:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 314/1792 [53:10<4:09:45, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 314/1792 [53:10<4:09:45, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 314/1792 [53:10<4:09:45, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 313 is completed and loss is 0.2608639597892761\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 314/1792 [53:10<4:09:45, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 315/1792 [53:20<4:10:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 314 is completed and loss is 0.23395714163780212\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 315/1792 [53:20<4:10:25, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 315/1792 [53:20<4:10:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 315/1792 [53:20<4:10:25, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 316/1792 [53:31<4:09:54, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 316/1792 [53:30<4:09:55, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 315 is completed and loss is 0.24383550882339478\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 316/1792 [53:31<4:09:55, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 316/1792 [53:30<4:09:55, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 317/1792 [53:40<4:09:40, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 317/1792 [53:41<4:09:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 316 is completed and loss is 0.265117883682251\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 317/1792 [53:40<4:09:40, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 317/1792 [53:41<4:09:40, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 318/1792 [53:51<4:10:16, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 318/1792 [53:50<4:10:16, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 317 is completed and loss is 0.23394522070884705\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 318/1792 [53:50<4:10:16, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 318/1792 [53:51<4:10:16, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 319/1792 [54:01<4:09:44, 10.17s/it]#015Training Epoch0:  18%|#033[34m█▊        #033[0m| 319/1792 [54:00<4:09:43, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 319/1792 [54:01<4:09:43, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 318 is completed and loss is 0.25554394721984863\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 319/1792 [54:01<4:09:44, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 320/1792 [54:11<4:09:19, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 320/1792 [54:11<4:09:19, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 320/1792 [54:11<4:09:19, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 319 is completed and loss is 0.27831801772117615\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 320/1792 [54:11<4:09:19, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 321/1792 [54:21<4:08:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 321/1792 [54:21<4:08:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 320 is completed and loss is 0.27680540084838867\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 321/1792 [54:21<4:08:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 321/1792 [54:21<4:08:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 322/1792 [54:31<4:08:44, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 322/1792 [54:32<4:08:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 322/1792 [54:32<4:08:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 321 is completed and loss is 0.23912598192691803\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 322/1792 [54:31<4:08:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 323/1792 [54:42<4:08:24, 10.15s/it]#015Training Epoch0:  18%|#033[34m█▊        #033[0m| 323/1792 [54:41<4:08:24, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 322 is completed and loss is 0.2374122440814972\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 323/1792 [54:41<4:08:24, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 323/1792 [54:42<4:08:24, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 324/1792 [54:52<4:08:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 324/1792 [54:51<4:08:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 323 is completed and loss is 0.23418785631656647\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 324/1792 [54:52<4:08:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 324/1792 [54:51<4:08:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 325/1792 [55:02<4:08:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 325/1792 [55:01<4:08:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 324 is completed and loss is 0.23996636271476746\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 325/1792 [55:02<4:08:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 325/1792 [55:01<4:08:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 326/1792 [55:11<4:07:52, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 326/1792 [55:12<4:07:52, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 325 is completed and loss is 0.24695700407028198\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 326/1792 [55:12<4:07:52, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 326/1792 [55:12<4:07:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 327/1792 [55:22<4:07:38, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 326 is completed and loss is 0.25006988644599915\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 327/1792 [55:22<4:07:38, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 327/1792 [55:22<4:07:38, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 327/1792 [55:22<4:07:39, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 328/1792 [55:32<4:07:31, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 327 is completed and loss is 0.2636776864528656\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 328/1792 [55:32<4:07:31, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 328/1792 [55:32<4:07:31, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 328/1792 [55:32<4:07:31, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 329/1792 [55:42<4:08:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 328 is completed and loss is 0.27557486295700073\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 329/1792 [55:43<4:08:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 329/1792 [55:42<4:08:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 329/1792 [55:43<4:08:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 330/1792 [55:52<4:07:39, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 329 is completed and loss is 0.28277385234832764\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 330/1792 [55:53<4:07:40, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 330/1792 [55:53<4:07:40, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 330/1792 [55:52<4:07:40, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 331/1792 [56:02<4:07:17, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 330 is completed and loss is 0.3032093644142151\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 331/1792 [56:03<4:07:17, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 331/1792 [56:02<4:07:17, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  18%|#033[34m█▊        #033[0m| 331/1792 [56:03<4:07:17, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 332/1792 [56:12<4:06:57, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 331 is completed and loss is 0.25298136472702026\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 332/1792 [56:12<4:06:57, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 332/1792 [56:13<4:06:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 332/1792 [56:13<4:06:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 333/1792 [56:23<4:06:51, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 332 is completed and loss is 0.2595673203468323\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 333/1792 [56:23<4:06:51, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 333/1792 [56:23<4:06:51, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 333/1792 [56:23<4:06:51, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 334/1792 [56:33<4:06:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 333 is completed and loss is 0.22861257195472717\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 334/1792 [56:33<4:06:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 334/1792 [56:33<4:06:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 334/1792 [56:33<4:06:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 335/1792 [56:43<4:07:09, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 334 is completed and loss is 0.2603324055671692\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 335/1792 [56:44<4:07:09, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 335/1792 [56:43<4:07:09, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▊        #033[0m| 335/1792 [56:44<4:07:09, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 336/1792 [56:53<4:06:40, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 336/1792 [56:54<4:06:40, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 335 is completed and loss is 0.2346709817647934\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 336/1792 [56:53<4:06:41, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 336/1792 [56:54<4:06:41, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 337/1792 [57:03<4:06:32, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 337/1792 [57:04<4:06:31, 10.17s/it]#015Training Epoch0:  19%|#033[34m█▉        #033[0m| 337/1792 [57:04<4:06:31, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 336 is completed and loss is 0.23526287078857422\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 337/1792 [57:03<4:06:32, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 338/1792 [57:14<4:07:10, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 337 is completed and loss is 0.22442980110645294\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 338/1792 [57:14<4:07:10, 10.20s/it]#015Training Epoch0:  19%|#033[34m█▉        #033[0m| 338/1792 [57:14<4:07:10, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 338/1792 [57:14<4:07:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 338 is completed and loss is 0.21879205107688904\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 339/1792 [57:24<4:06:35, 10.18s/it]#015Training Epoch0:  19%|#033[34m█▉        #033[0m| 339/1792 [57:24<4:06:35, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 339/1792 [57:24<4:06:36, 10.18s/it]#015Training Epoch0:  19%|#033[34m█▉        #033[0m| 339/1792 [57:24<4:06:36, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 340/1792 [57:35<4:07:02, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 340/1792 [57:34<4:07:02, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 339 is completed and loss is 0.23758940398693085\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 340/1792 [57:35<4:07:02, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 340/1792 [57:34<4:07:02, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 340 is completed and loss is 0.23979343473911285\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 341/1792 [57:44<4:06:24, 10.19s/it]#015Training Epoch0:  19%|#033[34m█▉        #033[0m| 341/1792 [57:45<4:06:24, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 341/1792 [57:44<4:06:24, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 341/1792 [57:45<4:06:24, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 341 is completed and loss is 0.2266489714384079\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 342/1792 [57:54<4:05:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 342/1792 [57:55<4:05:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 342/1792 [57:55<4:05:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 342/1792 [57:54<4:05:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 342 is completed and loss is 0.26507288217544556\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 343/1792 [58:05<4:05:27, 10.16s/it]#015Training Epoch0:  19%|#033[34m█▉        #033[0m| 343/1792 [58:05<4:05:28, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 343/1792 [58:04<4:05:28, 10.16s/it]#015Training Epoch0:  19%|#033[34m█▉        #033[0m| 343/1792 [58:04<4:05:28, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 343 is completed and loss is 0.21895936131477356\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 344/1792 [58:15<4:05:15, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 344/1792 [58:15<4:05:15, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 344/1792 [58:15<4:05:15, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 344/1792 [58:15<4:05:15, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 345/1792 [58:25<4:05:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 344 is completed and loss is 0.24785754084587097\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 345/1792 [58:25<4:05:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 345/1792 [58:25<4:05:08, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 345/1792 [58:25<4:05:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 345 is completed and loss is 0.2650168240070343\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 346/1792 [58:35<4:04:49, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 346/1792 [58:35<4:04:49, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 346/1792 [58:35<4:04:49, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 346/1792 [58:35<4:04:50, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 347/1792 [58:46<4:04:30, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 346 is completed and loss is 0.25194546580314636\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 347/1792 [58:45<4:04:30, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 347/1792 [58:45<4:04:30, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 347/1792 [58:46<4:04:30, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 348/1792 [58:56<4:04:19, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 347 is completed and loss is 0.22444181144237518\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 348/1792 [58:56<4:04:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 348/1792 [58:55<4:04:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 348/1792 [58:55<4:04:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 349/1792 [59:06<4:04:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 349/1792 [59:06<4:04:07, 10.15s/it]#015Training Epoch0:  19%|#033[34m█▉        #033[0m| 349/1792 [59:05<4:04:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 348 is completed and loss is 0.24433986842632294\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  19%|#033[34m█▉        #033[0m| 349/1792 [59:05<4:04:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 350/1792 [59:16<4:03:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 349 is completed and loss is 0.2345908284187317\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 350/1792 [59:16<4:03:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 350/1792 [59:16<4:03:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 350/1792 [59:15<4:03:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 351/1792 [59:26<4:03:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 350 is completed and loss is 0.28323012590408325\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 351/1792 [59:26<4:03:42, 10.15s/it]#015Training Epoch0:  20%|#033[34m█▉        #033[0m| 351/1792 [59:26<4:03:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 351/1792 [59:26<4:03:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 352/1792 [59:36<4:03:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 351 is completed and loss is 0.26158496737480164\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 352/1792 [59:36<4:03:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 352/1792 [59:36<4:03:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 352/1792 [59:36<4:03:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 353/1792 [59:47<4:03:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 352 is completed and loss is 0.2758992910385132\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 353/1792 [59:46<4:03:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 353/1792 [59:46<4:03:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 353/1792 [59:46<4:03:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 354/1792 [59:57<4:03:03, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 354/1792 [59:57<4:03:03, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 353 is completed and loss is 0.26533883810043335\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 354/1792 [59:56<4:03:04, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 354/1792 [59:56<4:03:03, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 355/1792 [1:00:07<4:03:41, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 354 is completed and loss is 0.24897341430187225\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 355/1792 [1:00:06<4:03:41, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 355/1792 [1:00:07<4:03:41, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 355/1792 [1:00:06<4:03:41, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 356/1792 [1:00:17<4:03:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 355 is completed and loss is 0.24566414952278137\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 356/1792 [1:00:16<4:03:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 356/1792 [1:00:17<4:03:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 356/1792 [1:00:16<4:03:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 357/1792 [1:00:27<4:02:47, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 356 is completed and loss is 0.2620851993560791\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 357/1792 [1:00:27<4:02:47, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 357/1792 [1:00:27<4:02:47, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 357/1792 [1:00:26<4:02:47, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 358/1792 [1:00:37<4:03:26, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 358/1792 [1:00:37<4:03:26, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 357 is completed and loss is 0.23520399630069733\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 358/1792 [1:00:37<4:03:26, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m█▉        #033[0m| 358/1792 [1:00:37<4:03:26, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 359/1792 [1:00:48<4:02:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 358 is completed and loss is 0.2439194619655609\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 359/1792 [1:00:47<4:02:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 359/1792 [1:00:48<4:02:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 359/1792 [1:00:47<4:02:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 360/1792 [1:00:58<4:02:29, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 359 is completed and loss is 0.21396519243717194\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 360/1792 [1:00:57<4:02:29, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 360/1792 [1:00:58<4:02:29, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 360/1792 [1:00:57<4:02:29, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 361/1792 [1:01:08<4:03:02, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 360 is completed and loss is 0.23850184679031372\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 361/1792 [1:01:08<4:03:01, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 361/1792 [1:01:07<4:03:02, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 361/1792 [1:01:07<4:03:02, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 362/1792 [1:01:18<4:02:32, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 361 is completed and loss is 0.22492516040802002\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 362/1792 [1:01:18<4:02:32, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 362/1792 [1:01:18<4:02:32, 10.18s/it]#015Training Epoch0:  20%|#033[34m██        #033[0m| 362/1792 [1:01:17<4:02:32, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 363/1792 [1:01:28<4:02:09, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 363/1792 [1:01:28<4:02:09, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 362 is completed and loss is 0.2103201150894165\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 363/1792 [1:01:28<4:02:09, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 363/1792 [1:01:28<4:02:09, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 364/1792 [1:01:38<4:01:46, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 364/1792 [1:01:38<4:01:45, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 363 is completed and loss is 0.27801787853240967\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 364/1792 [1:01:38<4:01:46, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 364/1792 [1:01:38<4:01:46, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 365/1792 [1:01:49<4:01:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 364 is completed and loss is 0.2339833378791809\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 365/1792 [1:01:48<4:01:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 365/1792 [1:01:48<4:01:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 365/1792 [1:01:48<4:01:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 366/1792 [1:01:59<4:02:02, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 365 is completed and loss is 0.27100670337677\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 366/1792 [1:01:58<4:02:02, 10.18s/it]#015Training Epoch0:  20%|#033[34m██        #033[0m| 366/1792 [1:01:59<4:02:02, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 366/1792 [1:01:58<4:02:02, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 367/1792 [1:02:09<4:01:35, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 366 is completed and loss is 0.25994712114334106\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 367/1792 [1:02:09<4:01:35, 10.17s/it]#015Training Epoch0:  20%|#033[34m██        #033[0m| 367/1792 [1:02:08<4:01:35, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  20%|#033[34m██        #033[0m| 367/1792 [1:02:08<4:01:35, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 368/1792 [1:02:19<4:01:14, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 367 is completed and loss is 0.2697870433330536\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 368/1792 [1:02:19<4:01:14, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 368/1792 [1:02:19<4:01:14, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 368/1792 [1:02:18<4:01:14, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 369/1792 [1:02:29<4:01:51, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 369/1792 [1:02:29<4:01:51, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 368 is completed and loss is 0.24104684591293335\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 369/1792 [1:02:29<4:01:51, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 369/1792 [1:02:29<4:01:51, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 370/1792 [1:02:39<4:01:13, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 369 is completed and loss is 0.2775905728340149\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 370/1792 [1:02:39<4:01:13, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 370/1792 [1:02:39<4:01:13, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 370/1792 [1:02:39<4:01:13, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 371/1792 [1:02:50<4:00:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 370 is completed and loss is 0.2580185830593109\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 371/1792 [1:02:49<4:00:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 371/1792 [1:02:50<4:00:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 371/1792 [1:02:49<4:00:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 372/1792 [1:03:00<4:01:21, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 372/1792 [1:03:00<4:01:21, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 371 is completed and loss is 0.2304655909538269\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 372/1792 [1:02:59<4:01:21, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 372/1792 [1:02:59<4:01:21, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 373/1792 [1:03:10<4:00:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 373/1792 [1:03:09<4:00:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 372 is completed and loss is 0.24507439136505127\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 373/1792 [1:03:10<4:00:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 373/1792 [1:03:09<4:00:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 374/1792 [1:03:19<4:00:09, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 374/1792 [1:03:20<4:00:09, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 373 is completed and loss is 0.23565146327018738\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 374/1792 [1:03:20<4:00:09, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 374/1792 [1:03:20<4:00:09, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 375/1792 [1:03:30<3:59:48, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 375/1792 [1:03:30<3:59:49, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 374 is completed and loss is 0.23517419397830963\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 375/1792 [1:03:30<3:59:49, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 375/1792 [1:03:30<3:59:49, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 376/1792 [1:03:40<3:59:33, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 376/1792 [1:03:40<3:59:33, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 376/1792 [1:03:40<3:59:33, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 375 is completed and loss is 0.2414819598197937\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 376/1792 [1:03:40<3:59:33, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 377/1792 [1:03:51<4:00:17, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 377/1792 [1:03:50<4:00:17, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 377/1792 [1:03:51<4:00:17, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 376 is completed and loss is 0.2543637454509735\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 377/1792 [1:03:50<4:00:17, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 378/1792 [1:04:01<3:59:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 378/1792 [1:04:00<3:59:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 377 is completed and loss is 0.2336343228816986\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 378/1792 [1:04:01<3:59:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 378/1792 [1:04:00<3:59:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 379/1792 [1:04:11<3:59:20, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 379/1792 [1:04:10<3:59:20, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 379/1792 [1:04:11<3:59:20, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 378 is completed and loss is 0.23963035643100739\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 379/1792 [1:04:10<3:59:20, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 380/1792 [1:04:21<3:59:53, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 380/1792 [1:04:21<3:59:53, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 380/1792 [1:04:21<3:59:52, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 379 is completed and loss is 0.23969101905822754\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██        #033[0m| 380/1792 [1:04:21<3:59:52, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 381/1792 [1:04:31<3:59:16, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 381/1792 [1:04:31<3:59:16, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 380 is completed and loss is 0.20788449048995972\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 381/1792 [1:04:31<3:59:16, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 381/1792 [1:04:31<3:59:16, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 382/1792 [1:04:41<3:58:47, 10.16s/it]#015Training Epoch0:  21%|#033[34m██▏       #033[0m| 382/1792 [1:04:41<3:58:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 381 is completed and loss is 0.21293140947818756\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 382/1792 [1:04:41<3:58:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 382/1792 [1:04:41<3:58:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 383/1792 [1:04:52<3:59:23, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 383/1792 [1:04:51<3:59:23, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 382 is completed and loss is 0.2557404041290283\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 383/1792 [1:04:52<3:59:23, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 383/1792 [1:04:51<3:59:23, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 384/1792 [1:05:02<3:58:48, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 384/1792 [1:05:01<3:58:48, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 383 is completed and loss is 0.2347726821899414\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 384/1792 [1:05:01<3:58:47, 10.18s/it]#015Training Epoch0:  21%|#033[34m██▏       #033[0m| 384/1792 [1:05:02<3:58:48, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 385/1792 [1:05:11<3:58:27, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 385/1792 [1:05:12<3:58:27, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 384 is completed and loss is 0.25998151302337646\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 385/1792 [1:05:12<3:58:27, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  21%|#033[34m██▏       #033[0m| 385/1792 [1:05:12<3:58:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 386/1792 [1:05:22<3:58:03, 10.16s/it]#015Training Epoch0:  22%|#033[34m██▏       #033[0m| 386/1792 [1:05:22<3:58:03, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 385 is completed and loss is 0.23604710400104523\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 386/1792 [1:05:22<3:58:03, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 386/1792 [1:05:22<3:58:04, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 387/1792 [1:05:32<3:57:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 387/1792 [1:05:32<3:57:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 386 is completed and loss is 0.24267259240150452\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 387/1792 [1:05:32<3:57:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 387/1792 [1:05:32<3:57:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 388/1792 [1:05:42<3:57:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 388/1792 [1:05:42<3:57:29, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 388/1792 [1:05:42<3:57:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 387 is completed and loss is 0.25081929564476013\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 388/1792 [1:05:42<3:57:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 389/1792 [1:05:52<3:57:15, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 389/1792 [1:05:53<3:57:15, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 388 is completed and loss is 0.23387862741947174\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 389/1792 [1:05:52<3:57:15, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 389/1792 [1:05:53<3:57:15, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 390/1792 [1:06:03<3:57:01, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 390/1792 [1:06:02<3:57:01, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 389 is completed and loss is 0.2250562161207199\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 390/1792 [1:06:02<3:57:01, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 390/1792 [1:06:03<3:57:01, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 391/1792 [1:06:13<3:56:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 391/1792 [1:06:12<3:56:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 390 is completed and loss is 0.22429963946342468\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 391/1792 [1:06:12<3:56:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 391/1792 [1:06:13<3:56:54, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 392/1792 [1:06:23<3:56:39, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 391 is completed and loss is 0.21420694887638092\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 392/1792 [1:06:22<3:56:39, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 392/1792 [1:06:22<3:56:39, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 392/1792 [1:06:23<3:56:39, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 393/1792 [1:06:33<3:56:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 393/1792 [1:06:33<3:56:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 393/1792 [1:06:33<3:56:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 392 is completed and loss is 0.26071563363075256\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 393/1792 [1:06:33<3:56:30, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 393 is completed and loss is 0.2300245314836502\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 394/1792 [1:06:43<3:56:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 394/1792 [1:06:43<3:56:27, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 394/1792 [1:06:43<3:56:27, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 394/1792 [1:06:43<3:56:27, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 395/1792 [1:06:53<3:56:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 394 is completed and loss is 0.2401745319366455\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 395/1792 [1:06:53<3:56:15, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 395/1792 [1:06:53<3:56:15, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 395/1792 [1:06:53<3:56:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 395 is completed and loss is 0.22064833343029022\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 396/1792 [1:07:03<3:56:00, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 396/1792 [1:07:04<3:56:01, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 396/1792 [1:07:04<3:56:01, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 396/1792 [1:07:03<3:56:01, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 396 is completed and loss is 0.25134527683258057\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 397/1792 [1:07:14<3:55:57, 10.15s/it]#015Training Epoch0:  22%|#033[34m██▏       #033[0m| 397/1792 [1:07:13<3:55:57, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 397/1792 [1:07:13<3:55:57, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 397/1792 [1:07:14<3:55:57, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 398/1792 [1:07:24<3:56:25, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 397 is completed and loss is 0.24278727173805237\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 398/1792 [1:07:23<3:56:25, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 398/1792 [1:07:23<3:56:25, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 398/1792 [1:07:24<3:56:26, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 398 is completed and loss is 0.23811866343021393\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 399/1792 [1:07:34<3:55:55, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 399/1792 [1:07:34<3:55:55, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 399/1792 [1:07:34<3:55:55, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 399/1792 [1:07:33<3:55:55, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 400/1792 [1:07:44<3:55:36, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 399 is completed and loss is 0.25655797123908997\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 400/1792 [1:07:44<3:55:36, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 400/1792 [1:07:44<3:55:36, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 400/1792 [1:07:44<3:55:36, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 401/1792 [1:07:55<3:56:08, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 401/1792 [1:07:55<3:56:08, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 400 is completed and loss is 0.26370537281036377\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 401/1792 [1:07:54<3:56:08, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 401/1792 [1:07:54<3:56:08, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 402/1792 [1:08:05<3:55:38, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 401 is completed and loss is 0.2095637023448944\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 402/1792 [1:08:04<3:55:39, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 402/1792 [1:08:04<3:55:38, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 402/1792 [1:08:05<3:55:39, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 403/1792 [1:08:14<3:55:15, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 402 is completed and loss is 0.27545052766799927\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 403/1792 [1:08:15<3:55:16, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 403/1792 [1:08:14<3:55:16, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  22%|#033[34m██▏       #033[0m| 403/1792 [1:08:15<3:55:16, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 404/1792 [1:08:25<3:55:50, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 403 is completed and loss is 0.2517039477825165\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 404/1792 [1:08:25<3:55:50, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 404/1792 [1:08:25<3:55:49, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 404/1792 [1:08:24<3:55:49, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 405/1792 [1:08:35<3:55:19, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 405/1792 [1:08:35<3:55:19, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 404 is completed and loss is 0.24176405370235443\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 405/1792 [1:08:35<3:55:19, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 405/1792 [1:08:35<3:55:19, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 406/1792 [1:08:45<3:54:50, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 405 is completed and loss is 0.2369655966758728\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 406/1792 [1:08:45<3:54:50, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 406/1792 [1:08:45<3:54:50, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 406/1792 [1:08:45<3:54:50, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 406 is completed and loss is 0.20288217067718506\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 407/1792 [1:08:55<3:54:24, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 407/1792 [1:08:55<3:54:25, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 407/1792 [1:08:55<3:54:25, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 407/1792 [1:08:56<3:54:25, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 408/1792 [1:09:06<3:54:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 407 is completed and loss is 0.22447580099105835\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 408/1792 [1:09:05<3:54:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 408/1792 [1:09:05<3:54:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 408/1792 [1:09:06<3:54:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 409/1792 [1:09:15<3:54:47, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 409/1792 [1:09:16<3:54:47, 10.19s/it]#015Training Epoch0:  23%|#033[34m██▎       #033[0m| 409/1792 [1:09:16<3:54:47, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 408 is completed and loss is 0.19724959135055542\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 409/1792 [1:09:15<3:54:47, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 410/1792 [1:09:25<3:54:16, 10.17s/it]#015Training Epoch0:  23%|#033[34m██▎       #033[0m| 410/1792 [1:09:26<3:54:16, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 410/1792 [1:09:26<3:54:16, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 409 is completed and loss is 0.22210447490215302\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 410/1792 [1:09:25<3:54:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 411/1792 [1:09:36<3:53:51, 10.16s/it]#015Training Epoch0:  23%|#033[34m██▎       #033[0m| 411/1792 [1:09:36<3:53:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 411/1792 [1:09:36<3:53:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 410 is completed and loss is 0.2825676500797272\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 411/1792 [1:09:36<3:53:52, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 412/1792 [1:09:46<3:54:35, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 411 is completed and loss is 0.26967814564704895\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 412/1792 [1:09:46<3:54:35, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 412/1792 [1:09:46<3:54:35, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 412/1792 [1:09:46<3:54:35, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 413/1792 [1:09:57<3:53:54, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 412 is completed and loss is 0.25519612431526184\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 413/1792 [1:09:56<3:53:54, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 413/1792 [1:09:56<3:53:54, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 413/1792 [1:09:57<3:53:55, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 413 is completed and loss is 0.2005363404750824\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 414/1792 [1:10:06<3:53:33, 10.17s/it]#015Training Epoch0:  23%|#033[34m██▎       #033[0m| 414/1792 [1:10:07<3:53:33, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 414/1792 [1:10:06<3:53:33, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 414/1792 [1:10:07<3:53:33, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 415/1792 [1:10:17<3:54:07, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 415/1792 [1:10:16<3:54:07, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 415/1792 [1:10:17<3:54:07, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 414 is completed and loss is 0.2375325858592987\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 415/1792 [1:10:16<3:54:07, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 415 is completed and loss is 0.2590605318546295\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 416/1792 [1:10:27<3:53:34, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 416/1792 [1:10:27<3:53:35, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 416/1792 [1:10:27<3:53:35, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 416/1792 [1:10:27<3:53:35, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 416 is completed and loss is 0.23172593116760254\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 417/1792 [1:10:37<3:53:07, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 417/1792 [1:10:37<3:53:08, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 417/1792 [1:10:37<3:53:08, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 417/1792 [1:10:37<3:53:08, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 417 is completed and loss is 0.19103920459747314\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 418/1792 [1:10:47<3:52:54, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 418/1792 [1:10:47<3:52:54, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 418/1792 [1:10:47<3:52:55, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 418/1792 [1:10:47<3:52:55, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 418 is completed and loss is 0.2625735402107239\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 419/1792 [1:10:57<3:52:29, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 419/1792 [1:10:57<3:52:29, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 419/1792 [1:10:58<3:52:30, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 419/1792 [1:10:58<3:52:30, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 419 is completed and loss is 0.22990316152572632\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 420/1792 [1:11:07<3:53:01, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 420/1792 [1:11:07<3:53:01, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 420/1792 [1:11:08<3:53:02, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 420/1792 [1:11:08<3:53:02, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 420 is completed and loss is 0.23203642666339874\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 421/1792 [1:11:17<3:52:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 421/1792 [1:11:18<3:52:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 421/1792 [1:11:18<3:52:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  23%|#033[34m██▎       #033[0m| 421/1792 [1:11:17<3:52:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 421 is completed and loss is 0.21528242528438568\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 422/1792 [1:11:28<3:52:12, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 422/1792 [1:11:28<3:52:12, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 422/1792 [1:11:28<3:52:13, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 422/1792 [1:11:28<3:52:13, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 422 is completed and loss is 0.2319529801607132\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 423/1792 [1:11:38<3:52:44, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 423/1792 [1:11:38<3:52:44, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 423/1792 [1:11:38<3:52:44, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 423/1792 [1:11:38<3:52:44, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 423 is completed and loss is 0.2577327787876129\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 424/1792 [1:11:48<3:52:07, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 424/1792 [1:11:49<3:52:06, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 424/1792 [1:11:49<3:52:06, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 424/1792 [1:11:48<3:52:07, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 424 is completed and loss is 0.21931131184101105\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 425/1792 [1:11:58<3:51:46, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 425/1792 [1:11:59<3:51:46, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 425/1792 [1:11:59<3:51:46, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▎       #033[0m| 425/1792 [1:11:58<3:51:46, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 426/1792 [1:12:09<3:52:10, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 425 is completed and loss is 0.24920149147510529\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 426/1792 [1:12:08<3:52:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 426/1792 [1:12:09<3:52:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 426/1792 [1:12:08<3:52:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 427/1792 [1:12:19<3:51:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 426 is completed and loss is 0.22773249447345734\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 427/1792 [1:12:19<3:51:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 427/1792 [1:12:19<3:51:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 427/1792 [1:12:18<3:51:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 427 is completed and loss is 0.24884580075740814\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 428/1792 [1:12:29<3:51:11, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 428/1792 [1:12:29<3:51:11, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 428/1792 [1:12:29<3:51:11, 10.17s/it]#015Training Epoch0:  24%|#033[34m██▍       #033[0m| 428/1792 [1:12:29<3:51:11, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 428 is completed and loss is 0.21294596791267395\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 429/1792 [1:12:39<3:50:46, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 429/1792 [1:12:39<3:50:46, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 429/1792 [1:12:39<3:50:46, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 429/1792 [1:12:39<3:50:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 429 is completed and loss is 0.231527179479599\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 430/1792 [1:12:49<3:50:32, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 430/1792 [1:12:50<3:50:32, 10.16s/it]#015Training Epoch0:  24%|#033[34m██▍       #033[0m| 430/1792 [1:12:50<3:50:32, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 430/1792 [1:12:49<3:50:32, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 430 is completed and loss is 0.23845162987709045\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 431/1792 [1:12:59<3:50:18, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 431/1792 [1:13:00<3:50:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 431/1792 [1:12:59<3:50:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 431/1792 [1:13:00<3:50:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 432/1792 [1:13:09<3:50:07, 10.15s/it]#015Training Epoch0:  24%|#033[34m██▍       #033[0m| 432/1792 [1:13:10<3:50:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 431 is completed and loss is 0.2672652304172516\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 432/1792 [1:13:09<3:50:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 432/1792 [1:13:10<3:50:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 433/1792 [1:13:20<3:49:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 433/1792 [1:13:20<3:49:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 433/1792 [1:13:19<3:49:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 432 is completed and loss is 0.2741132080554962\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 433/1792 [1:13:19<3:49:54, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 434/1792 [1:13:30<3:49:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 434/1792 [1:13:30<3:49:43, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 433 is completed and loss is 0.20002567768096924\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 434/1792 [1:13:30<3:49:43, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 434/1792 [1:13:29<3:49:43, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 435/1792 [1:13:40<3:49:27, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 434 is completed and loss is 0.21827159821987152\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 435/1792 [1:13:40<3:49:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 435/1792 [1:13:40<3:49:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 435/1792 [1:13:40<3:49:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 436/1792 [1:13:50<3:49:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 436/1792 [1:13:50<3:49:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 435 is completed and loss is 0.22785982489585876\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 436/1792 [1:13:50<3:49:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 436/1792 [1:13:50<3:49:12, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 437/1792 [1:14:01<3:49:01, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 436 is completed and loss is 0.28413188457489014\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 437/1792 [1:14:00<3:49:01, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 437/1792 [1:14:01<3:49:01, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 437/1792 [1:14:00<3:49:02, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 438/1792 [1:14:11<3:48:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 438/1792 [1:14:10<3:48:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 438/1792 [1:14:11<3:48:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 437 is completed and loss is 0.23227795958518982\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 438/1792 [1:14:10<3:48:54, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 439/1792 [1:14:21<3:48:38, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 438 is completed and loss is 0.26368486881256104\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 439/1792 [1:14:20<3:48:38, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 439/1792 [1:14:20<3:48:38, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  24%|#033[34m██▍       #033[0m| 439/1792 [1:14:21<3:48:38, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 440/1792 [1:14:31<3:48:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 440/1792 [1:14:31<3:48:28, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 440/1792 [1:14:30<3:48:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 439 is completed and loss is 0.24037818610668182\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 440/1792 [1:14:30<3:48:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 441/1792 [1:14:41<3:49:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 441/1792 [1:14:41<3:49:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 440 is completed and loss is 0.25537416338920593\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 441/1792 [1:14:41<3:49:17, 10.18s/it]#015Training Epoch0:  25%|#033[34m██▍       #033[0m| 441/1792 [1:14:41<3:49:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 442/1792 [1:14:51<3:48:55, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 441 is completed and loss is 0.2814180850982666\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 442/1792 [1:14:51<3:48:55, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 442/1792 [1:14:51<3:48:55, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 442/1792 [1:14:51<3:48:55, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 443/1792 [1:15:02<3:48:35, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 442 is completed and loss is 0.2704104483127594\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 443/1792 [1:15:02<3:48:35, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 443/1792 [1:15:01<3:48:35, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 443/1792 [1:15:01<3:48:35, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 444/1792 [1:15:12<3:49:07, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 443 is completed and loss is 0.23283688724040985\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 444/1792 [1:15:11<3:49:07, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 444/1792 [1:15:11<3:49:07, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 444/1792 [1:15:12<3:49:08, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 444 is completed and loss is 0.27358466386795044\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 445/1792 [1:15:21<3:48:33, 10.18s/it]#015Training Epoch0:  25%|#033[34m██▍       #033[0m| 445/1792 [1:15:22<3:48:33, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 445/1792 [1:15:21<3:48:33, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 445/1792 [1:15:22<3:48:34, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 445 is completed and loss is 0.23679691553115845\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 446/1792 [1:15:32<3:48:10, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 446/1792 [1:15:32<3:48:10, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 446/1792 [1:15:32<3:48:10, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 446/1792 [1:15:31<3:48:10, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 447/1792 [1:15:42<3:48:33, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 446 is completed and loss is 0.26585519313812256\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 447/1792 [1:15:42<3:48:33, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 447/1792 [1:15:42<3:48:33, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▍       #033[0m| 447/1792 [1:15:42<3:48:33, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 447 is completed and loss is 0.2725902199745178\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 448/1792 [1:15:52<3:47:56, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 448/1792 [1:15:52<3:47:56, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 448/1792 [1:15:52<3:47:56, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 448/1792 [1:15:53<3:47:56, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 449/1792 [1:16:03<3:47:34, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 448 is completed and loss is 0.24998882412910461\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 449/1792 [1:16:03<3:47:35, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 449/1792 [1:16:02<3:47:35, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 449/1792 [1:16:02<3:47:35, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 449 is completed and loss is 0.22415563464164734\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 450/1792 [1:16:12<3:47:29, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 450/1792 [1:16:13<3:47:29, 10.17s/it]#015Training Epoch0:  25%|#033[34m██▌       #033[0m| 450/1792 [1:16:13<3:47:29, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 450/1792 [1:16:12<3:47:29, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 450 is completed and loss is 0.24784791469573975\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 451/1792 [1:16:22<3:47:11, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 451/1792 [1:16:23<3:47:11, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 451/1792 [1:16:23<3:47:11, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 451/1792 [1:16:22<3:47:11, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 451 is completed and loss is 0.22915995121002197\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 452/1792 [1:16:33<3:46:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 452/1792 [1:16:33<3:46:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 452/1792 [1:16:33<3:46:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 452/1792 [1:16:32<3:46:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 453/1792 [1:16:43<3:46:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 452 is completed and loss is 0.2526354491710663\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 453/1792 [1:16:43<3:46:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 453/1792 [1:16:43<3:46:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 453/1792 [1:16:43<3:46:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 454/1792 [1:16:53<3:46:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 454/1792 [1:16:53<3:46:21, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 453 is completed and loss is 0.23622314631938934\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 454/1792 [1:16:53<3:46:21, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 454/1792 [1:16:53<3:46:21, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 455/1792 [1:17:04<3:46:58, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 454 is completed and loss is 0.19440889358520508\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 455/1792 [1:17:03<3:46:59, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 455/1792 [1:17:03<3:46:59, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 455/1792 [1:17:04<3:46:59, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 456/1792 [1:17:14<3:46:32, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 456/1792 [1:17:14<3:46:32, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 455 is completed and loss is 0.25435134768486023\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  25%|#033[34m██▌       #033[0m| 456/1792 [1:17:13<3:46:32, 10.17s/it]#015Training Epoch0:  25%|#033[34m██▌       #033[0m| 456/1792 [1:17:13<3:46:32, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 457/1792 [1:17:24<3:46:15, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 456 is completed and loss is 0.26797398924827576\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 457/1792 [1:17:23<3:46:15, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 457/1792 [1:17:23<3:46:16, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 457/1792 [1:17:24<3:46:16, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 458/1792 [1:17:34<3:46:47, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 457 is completed and loss is 0.23606747388839722\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 458/1792 [1:17:34<3:46:47, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 458/1792 [1:17:34<3:46:47, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 458/1792 [1:17:34<3:46:48, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 459/1792 [1:17:44<3:46:11, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 459/1792 [1:17:44<3:46:11, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 458 is completed and loss is 0.30142202973365784\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 459/1792 [1:17:44<3:46:11, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 459/1792 [1:17:44<3:46:11, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 460/1792 [1:17:55<3:45:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 459 is completed and loss is 0.19603660702705383\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 460/1792 [1:17:54<3:45:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 460/1792 [1:17:55<3:45:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 460/1792 [1:17:54<3:45:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 461/1792 [1:18:05<3:46:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 460 is completed and loss is 0.25166159868240356\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 461/1792 [1:18:04<3:46:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 461/1792 [1:18:05<3:46:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 461/1792 [1:18:04<3:46:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 462/1792 [1:18:15<3:45:47, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 461 is completed and loss is 0.20377294719219208\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 462/1792 [1:18:15<3:45:46, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 462/1792 [1:18:14<3:45:46, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 462/1792 [1:18:14<3:45:46, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 462 is completed and loss is 0.24667245149612427\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 463/1792 [1:18:25<3:45:26, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 463/1792 [1:18:25<3:45:26, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 463/1792 [1:18:24<3:45:26, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 463/1792 [1:18:25<3:45:26, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 464/1792 [1:18:35<3:45:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 464/1792 [1:18:35<3:45:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 463 is completed and loss is 0.23662465810775757\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 464/1792 [1:18:35<3:45:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 464/1792 [1:18:35<3:45:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 465/1792 [1:18:45<3:44:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 464 is completed and loss is 0.24595120549201965\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 465/1792 [1:18:45<3:44:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 465/1792 [1:18:45<3:44:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 465/1792 [1:18:45<3:44:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 466/1792 [1:18:56<3:45:19, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 465 is completed and loss is 0.24887794256210327\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 466/1792 [1:18:55<3:45:19, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 466/1792 [1:18:56<3:45:20, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 466/1792 [1:18:55<3:45:19, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 466 is completed and loss is 0.26060137152671814\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 467/1792 [1:19:05<3:45:33, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 467/1792 [1:19:05<3:45:34, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 467/1792 [1:19:06<3:45:34, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 467/1792 [1:19:06<3:45:34, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 468/1792 [1:19:16<3:44:57, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 468/1792 [1:19:16<3:44:57, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 468/1792 [1:19:15<3:44:57, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 467 is completed and loss is 0.22603587806224823\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 468/1792 [1:19:16<3:44:57, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 468 is completed and loss is 0.24260495603084564\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 469/1792 [1:19:26<3:45:23, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 469/1792 [1:19:26<3:45:23, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 469/1792 [1:19:26<3:45:23, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 469/1792 [1:19:26<3:45:23, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 469 is completed and loss is 0.24093998968601227\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 470/1792 [1:19:36<3:44:38, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 470/1792 [1:19:37<3:44:38, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 470/1792 [1:19:37<3:44:38, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▌       #033[0m| 470/1792 [1:19:36<3:44:39, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 470 is completed and loss is 0.2607909142971039\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 471/1792 [1:19:46<3:44:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 471/1792 [1:19:47<3:44:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 471/1792 [1:19:47<3:44:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 471/1792 [1:19:46<3:44:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 471 is completed and loss is 0.2634032666683197\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 472/1792 [1:19:56<3:44:25, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 472/1792 [1:19:57<3:44:26, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 472/1792 [1:19:56<3:44:26, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 472/1792 [1:19:57<3:44:26, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 473/1792 [1:20:07<3:43:54, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 472 is completed and loss is 0.24523450434207916\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 473/1792 [1:20:07<3:43:55, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 473/1792 [1:20:07<3:43:55, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 473/1792 [1:20:06<3:43:55, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 473 is completed and loss is 0.2511863112449646\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 474/1792 [1:20:17<3:43:28, 10.17s/it]#015Training Epoch0:  26%|#033[34m██▋       #033[0m| 474/1792 [1:20:17<3:43:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 474/1792 [1:20:17<3:43:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  26%|#033[34m██▋       #033[0m| 474/1792 [1:20:17<3:43:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 474 is completed and loss is 0.2271176278591156\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 475/1792 [1:20:27<3:43:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 475/1792 [1:20:27<3:43:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 475/1792 [1:20:27<3:43:05, 10.16s/it]#015Training Epoch0:  27%|#033[34m██▋       #033[0m| 475/1792 [1:20:27<3:43:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 476/1792 [1:20:37<3:42:43, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 475 is completed and loss is 0.26784977316856384\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 476/1792 [1:20:37<3:42:43, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 476/1792 [1:20:37<3:42:43, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 476/1792 [1:20:37<3:42:43, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 477/1792 [1:20:48<3:42:37, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 476 is completed and loss is 0.24727413058280945\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 477/1792 [1:20:47<3:42:37, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 477/1792 [1:20:47<3:42:37, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 477/1792 [1:20:48<3:42:38, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 478/1792 [1:20:58<3:42:20, 10.15s/it]#015Training Epoch0:  27%|#033[34m██▋       #033[0m| 478/1792 [1:20:58<3:42:21, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 477 is completed and loss is 0.2229258269071579\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 478/1792 [1:20:57<3:42:21, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 478/1792 [1:20:57<3:42:21, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 479/1792 [1:21:08<3:42:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 479/1792 [1:21:08<3:42:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 478 is completed and loss is 0.25134849548339844\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 479/1792 [1:21:07<3:42:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 479/1792 [1:21:07<3:42:08, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 480/1792 [1:21:18<3:41:57, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 479 is completed and loss is 0.22882945835590363\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 480/1792 [1:21:18<3:41:57, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 480/1792 [1:21:18<3:41:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 480/1792 [1:21:17<3:41:57, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 481/1792 [1:21:28<3:41:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 480 is completed and loss is 0.26076918840408325\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 481/1792 [1:21:28<3:41:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 481/1792 [1:21:28<3:41:42, 10.15s/it]#015Training Epoch0:  27%|#033[34m██▋       #033[0m| 481/1792 [1:21:28<3:41:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 482/1792 [1:21:38<3:41:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 481 is completed and loss is 0.2287299484014511\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 482/1792 [1:21:38<3:41:29, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 482/1792 [1:21:38<3:41:30, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 482/1792 [1:21:38<3:41:29, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 483/1792 [1:21:49<3:42:06, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 482 is completed and loss is 0.24295540153980255\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 483/1792 [1:21:48<3:42:06, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 483/1792 [1:21:48<3:42:06, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 483/1792 [1:21:49<3:42:06, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 484/1792 [1:21:59<3:41:33, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 483 is completed and loss is 0.2380310595035553\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 484/1792 [1:21:58<3:41:33, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 484/1792 [1:21:59<3:41:33, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 484/1792 [1:21:58<3:41:34, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 485/1792 [1:22:09<3:41:16, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 484 is completed and loss is 0.27947723865509033\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 485/1792 [1:22:08<3:41:15, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 485/1792 [1:22:09<3:41:16, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 485/1792 [1:22:08<3:41:16, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 486/1792 [1:22:19<3:41:01, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 485 is completed and loss is 0.23278483748435974\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 486/1792 [1:22:18<3:41:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 486/1792 [1:22:19<3:41:01, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 486/1792 [1:22:18<3:41:01, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 487/1792 [1:22:29<3:41:31, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 486 is completed and loss is 0.27243855595588684\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 487/1792 [1:22:29<3:41:31, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 487/1792 [1:22:29<3:41:31, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 487/1792 [1:22:29<3:41:31, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 488/1792 [1:22:39<3:41:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 487 is completed and loss is 0.22976714372634888\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 488/1792 [1:22:39<3:41:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 488/1792 [1:22:39<3:41:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 488/1792 [1:22:39<3:41:03, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 489/1792 [1:22:50<3:40:43, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 488 is completed and loss is 0.23951686918735504\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 489/1792 [1:22:49<3:40:43, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 489/1792 [1:22:49<3:40:42, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 489/1792 [1:22:50<3:40:43, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 490/1792 [1:23:00<3:41:12, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 489 is completed and loss is 0.25249528884887695\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 490/1792 [1:22:59<3:41:12, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 490/1792 [1:23:00<3:41:12, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 490/1792 [1:22:59<3:41:12, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 491/1792 [1:23:10<3:40:40, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 490 is completed and loss is 0.1946830153465271\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 491/1792 [1:23:09<3:40:40, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 491/1792 [1:23:10<3:40:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 491/1792 [1:23:09<3:40:40, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 492/1792 [1:23:20<3:40:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 491 is completed and loss is 0.2467375099658966\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 492/1792 [1:23:20<3:40:12, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 492/1792 [1:23:20<3:40:12, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  27%|#033[34m██▋       #033[0m| 492/1792 [1:23:19<3:40:12, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 492 is completed and loss is 0.21904292702674866\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 493/1792 [1:23:30<3:39:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 493/1792 [1:23:30<3:39:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 493/1792 [1:23:30<3:39:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 493/1792 [1:23:30<3:39:52, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 493 is completed and loss is 0.2550063133239746\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 494/1792 [1:23:40<3:39:38, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 494/1792 [1:23:40<3:39:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 494/1792 [1:23:40<3:39:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 494/1792 [1:23:40<3:39:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 495/1792 [1:23:51<3:39:25, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 495/1792 [1:23:50<3:39:25, 10.15s/it]#015Training Epoch0:  28%|#033[34m██▊       #033[0m| 495/1792 [1:23:51<3:39:25, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 494 is completed and loss is 0.2081034928560257\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 495/1792 [1:23:50<3:39:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 495 is completed and loss is 0.23807038366794586\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 496/1792 [1:24:00<3:39:14, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 496/1792 [1:24:01<3:39:14, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 496/1792 [1:24:01<3:39:14, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 496/1792 [1:24:00<3:39:14, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 496 is completed and loss is 0.25244084000587463\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 497/1792 [1:24:10<3:39:05, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 497/1792 [1:24:11<3:39:05, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 497/1792 [1:24:11<3:39:05, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 497/1792 [1:24:10<3:39:05, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 497 is completed and loss is 0.24929571151733398\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 498/1792 [1:24:21<3:39:46, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 498/1792 [1:24:20<3:39:46, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 498/1792 [1:24:21<3:39:46, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 498/1792 [1:24:21<3:39:46, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 499/1792 [1:24:31<3:39:14, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 498 is completed and loss is 0.2315274178981781\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 499/1792 [1:24:31<3:39:15, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 499/1792 [1:24:31<3:39:15, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 499/1792 [1:24:31<3:39:15, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 500/1792 [1:24:41<3:38:54, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 499 is completed and loss is 0.23965369164943695\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 500/1792 [1:24:41<3:38:55, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 500/1792 [1:24:41<3:38:55, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 500/1792 [1:24:41<3:38:55, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 501/1792 [1:24:51<3:39:25, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 501/1792 [1:24:52<3:39:26, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 500 is completed and loss is 0.21570494771003723\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 501/1792 [1:24:51<3:39:25, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 501/1792 [1:24:52<3:39:25, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 501 is completed and loss is 0.23296375572681427\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 502/1792 [1:25:01<3:38:51, 10.18s/it]#015Training Epoch0:  28%|#033[34m██▊       #033[0m| 502/1792 [1:25:02<3:38:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 502/1792 [1:25:01<3:38:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 502/1792 [1:25:02<3:38:52, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 503/1792 [1:25:12<3:38:36, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 502 is completed and loss is 0.23038490116596222\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 503/1792 [1:25:11<3:38:36, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 503/1792 [1:25:12<3:38:36, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 503/1792 [1:25:11<3:38:37, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 504/1792 [1:25:22<3:38:55, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 503 is completed and loss is 0.2718783915042877\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 504/1792 [1:25:22<3:38:55, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 504/1792 [1:25:22<3:38:55, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 504/1792 [1:25:22<3:38:55, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 505/1792 [1:25:32<3:38:22, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 505/1792 [1:25:32<3:38:22, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 504 is completed and loss is 0.277927964925766\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 505/1792 [1:25:32<3:38:22, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 505/1792 [1:25:32<3:38:22, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 506/1792 [1:25:42<3:37:54, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 505 is completed and loss is 0.276593953371048\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 506/1792 [1:25:43<3:37:54, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 506/1792 [1:25:42<3:37:54, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 506/1792 [1:25:42<3:37:54, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 507/1792 [1:25:53<3:38:23, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 507/1792 [1:25:52<3:38:22, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 507/1792 [1:25:53<3:38:23, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 506 is completed and loss is 0.2829236090183258\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 507/1792 [1:25:52<3:38:23, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 508/1792 [1:26:03<3:37:47, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 507 is completed and loss is 0.24172775447368622\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 508/1792 [1:26:03<3:37:47, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 508/1792 [1:26:02<3:37:47, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 508/1792 [1:26:02<3:37:48, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 509/1792 [1:26:13<3:38:10, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 508 is completed and loss is 0.2722628712654114\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 509/1792 [1:26:13<3:38:10, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 509/1792 [1:26:13<3:38:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 509/1792 [1:26:13<3:38:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 510/1792 [1:26:23<3:37:41, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 509 is completed and loss is 0.2294159084558487\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 510/1792 [1:26:23<3:37:41, 10.19s/it]#015Training Epoch0:  28%|#033[34m██▊       #033[0m| 510/1792 [1:26:23<3:37:41, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  28%|#033[34m██▊       #033[0m| 510/1792 [1:26:23<3:37:41, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 511/1792 [1:26:33<3:37:18, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 510 is completed and loss is 0.22784607112407684\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 511/1792 [1:26:33<3:37:18, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 511/1792 [1:26:33<3:37:18, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 511/1792 [1:26:33<3:37:18, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 512/1792 [1:26:44<3:37:37, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 512/1792 [1:26:44<3:37:37, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 512/1792 [1:26:43<3:37:37, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 511 is completed and loss is 0.25527632236480713\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 512/1792 [1:26:43<3:37:37, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 513/1792 [1:26:54<3:36:58, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 512 is completed and loss is 0.2569085955619812\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 513/1792 [1:26:53<3:36:57, 10.18s/it]#015Training Epoch0:  29%|#033[34m██▊       #033[0m| 513/1792 [1:26:54<3:36:58, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 513/1792 [1:26:53<3:36:57, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 514/1792 [1:27:04<3:36:34, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 514/1792 [1:27:04<3:36:34, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 513 is completed and loss is 0.24214498698711395\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 514/1792 [1:27:03<3:36:34, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 514/1792 [1:27:03<3:36:34, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 515/1792 [1:27:14<3:36:15, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 514 is completed and loss is 0.23737595975399017\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 515/1792 [1:27:14<3:36:15, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 515/1792 [1:27:14<3:36:15, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▊       #033[0m| 515/1792 [1:27:14<3:36:16, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 516/1792 [1:27:24<3:35:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 516/1792 [1:27:24<3:35:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 515 is completed and loss is 0.22119200229644775\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 516/1792 [1:27:24<3:35:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 516/1792 [1:27:24<3:35:55, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 517/1792 [1:27:34<3:35:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 516 is completed and loss is 0.20555458962917328\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 517/1792 [1:27:34<3:35:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 517/1792 [1:27:34<3:35:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 517/1792 [1:27:34<3:35:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 518/1792 [1:27:45<3:36:18, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 517 is completed and loss is 0.2748556435108185\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 518/1792 [1:27:45<3:36:18, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 518/1792 [1:27:44<3:36:18, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 518/1792 [1:27:44<3:36:18, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 519/1792 [1:27:55<3:35:51, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 519/1792 [1:27:55<3:35:51, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 518 is completed and loss is 0.22674308717250824\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 519/1792 [1:27:54<3:35:51, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 519/1792 [1:27:54<3:35:51, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 520/1792 [1:28:05<3:35:36, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 520/1792 [1:28:05<3:35:36, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 519 is completed and loss is 0.2193729281425476\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 520/1792 [1:28:04<3:35:36, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 520/1792 [1:28:04<3:35:36, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 521/1792 [1:28:15<3:35:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 521/1792 [1:28:14<3:35:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 520 is completed and loss is 0.25774022936820984\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 521/1792 [1:28:15<3:35:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 521/1792 [1:28:15<3:35:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 522/1792 [1:28:25<3:34:54, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 521 is completed and loss is 0.23270945250988007\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 522/1792 [1:28:25<3:34:54, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 522/1792 [1:28:25<3:34:54, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 522/1792 [1:28:25<3:34:54, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 523/1792 [1:28:35<3:34:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 522 is completed and loss is 0.2894665598869324\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 523/1792 [1:28:35<3:34:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 523/1792 [1:28:35<3:34:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 523/1792 [1:28:35<3:34:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 524/1792 [1:28:46<3:35:12, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 524/1792 [1:28:46<3:35:12, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 523 is completed and loss is 0.2471117526292801\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 524/1792 [1:28:45<3:35:12, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 524/1792 [1:28:45<3:35:12, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 525/1792 [1:28:56<3:34:48, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 524 is completed and loss is 0.28956839442253113\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 525/1792 [1:28:55<3:34:48, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 525/1792 [1:28:55<3:34:48, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 525/1792 [1:28:56<3:34:48, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 526/1792 [1:29:06<3:34:31, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 526/1792 [1:29:06<3:34:31, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 525 is completed and loss is 0.26505452394485474\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 526/1792 [1:29:05<3:34:31, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 526/1792 [1:29:05<3:34:31, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 527/1792 [1:29:16<3:34:09, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 527/1792 [1:29:16<3:34:09, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 527/1792 [1:29:15<3:34:09, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 526 is completed and loss is 0.2772621214389801\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 527/1792 [1:29:16<3:34:09, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 528/1792 [1:29:26<3:33:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 527 is completed and loss is 0.23431777954101562\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 528/1792 [1:29:26<3:33:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 528/1792 [1:29:26<3:33:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  29%|#033[34m██▉       #033[0m| 528/1792 [1:29:26<3:33:52, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 529/1792 [1:29:37<3:34:21, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 528 is completed and loss is 0.24616315960884094\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 529/1792 [1:29:36<3:34:21, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 529/1792 [1:29:36<3:34:22, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 529/1792 [1:29:37<3:34:22, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 530/1792 [1:29:47<3:34:40, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 530/1792 [1:29:47<3:34:40, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 529 is completed and loss is 0.22522948682308197\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 530/1792 [1:29:46<3:34:40, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 530/1792 [1:29:46<3:34:40, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 531/1792 [1:29:57<3:34:06, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 531/1792 [1:29:57<3:34:06, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 530 is completed and loss is 0.23373158276081085\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 531/1792 [1:29:56<3:34:06, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 531/1792 [1:29:56<3:34:07, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 532/1792 [1:30:07<3:33:38, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 531 is completed and loss is 0.22013047337532043\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 532/1792 [1:30:07<3:33:38, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 532/1792 [1:30:07<3:33:38, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 532/1792 [1:30:06<3:33:38, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 533/1792 [1:30:17<3:34:03, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 533/1792 [1:30:17<3:34:03, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 533/1792 [1:30:17<3:34:03, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 532 is completed and loss is 0.247968852519989\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 533/1792 [1:30:17<3:34:03, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 534/1792 [1:30:27<3:33:26, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 534/1792 [1:30:27<3:33:26, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 533 is completed and loss is 0.21840639412403107\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 534/1792 [1:30:27<3:33:26, 10.18s/it]#015Training Epoch0:  30%|#033[34m██▉       #033[0m| 534/1792 [1:30:27<3:33:26, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 535/1792 [1:30:38<3:33:53, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 535/1792 [1:30:37<3:33:53, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 535/1792 [1:30:38<3:33:53, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 534 is completed and loss is 0.21655593812465668\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 535/1792 [1:30:37<3:33:53, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 536/1792 [1:30:48<3:33:18, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 535 is completed and loss is 0.26688605546951294\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 536/1792 [1:30:47<3:33:18, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 536/1792 [1:30:48<3:33:18, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 536/1792 [1:30:47<3:33:18, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 537/1792 [1:30:58<3:32:48, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 536 is completed and loss is 0.25812777876853943\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 537/1792 [1:30:57<3:32:47, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 537/1792 [1:30:58<3:32:48, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m██▉       #033[0m| 537/1792 [1:30:57<3:32:48, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 538/1792 [1:31:08<3:32:31, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 538/1792 [1:31:08<3:32:31, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 537 is completed and loss is 0.24023114144802094\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 538/1792 [1:31:08<3:32:31, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 538/1792 [1:31:08<3:32:31, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 539/1792 [1:31:18<3:32:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 538 is completed and loss is 0.2341775745153427\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 539/1792 [1:31:18<3:32:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 539/1792 [1:31:18<3:32:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 539/1792 [1:31:18<3:32:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 540/1792 [1:31:28<3:32:03, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 539 is completed and loss is 0.22014187276363373\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 540/1792 [1:31:28<3:32:03, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 540/1792 [1:31:29<3:32:03, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 540/1792 [1:31:28<3:32:04, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 541/1792 [1:31:39<3:32:35, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 540 is completed and loss is 0.20530739426612854\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 541/1792 [1:31:38<3:32:35, 10.20s/it]#015Training Epoch0:  30%|#033[34m███       #033[0m| 541/1792 [1:31:39<3:32:35, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 541/1792 [1:31:38<3:32:35, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 542/1792 [1:31:49<3:32:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 541 is completed and loss is 0.22120337188243866\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 542/1792 [1:31:48<3:32:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 542/1792 [1:31:49<3:32:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 542/1792 [1:31:48<3:32:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 543/1792 [1:31:59<3:31:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 542 is completed and loss is 0.2497582882642746\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 543/1792 [1:31:59<3:31:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 543/1792 [1:31:59<3:31:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 543/1792 [1:31:58<3:31:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 543 is completed and loss is 0.2577642500400543\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 544/1792 [1:32:09<3:32:05, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 544/1792 [1:32:09<3:32:05, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 544/1792 [1:32:09<3:32:05, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 544/1792 [1:32:09<3:32:06, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 545/1792 [1:32:19<3:31:34, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 544 is completed and loss is 0.21824802458286285\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 545/1792 [1:32:19<3:31:34, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 545/1792 [1:32:19<3:31:34, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 545/1792 [1:32:19<3:31:34, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 546/1792 [1:32:30<3:31:57, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 546/1792 [1:32:30<3:31:57, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 545 is completed and loss is 0.24954469501972198\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 546/1792 [1:32:29<3:31:57, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  30%|#033[34m███       #033[0m| 546/1792 [1:32:29<3:31:57, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 547/1792 [1:32:40<3:31:25, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 546 is completed and loss is 0.22956503927707672\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 547/1792 [1:32:39<3:31:25, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 547/1792 [1:32:40<3:31:25, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 547/1792 [1:32:39<3:31:25, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 548/1792 [1:32:50<3:31:00, 10.18s/it]#015Training Epoch0:  31%|#033[34m███       #033[0m| 548/1792 [1:32:50<3:31:00, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 548/1792 [1:32:49<3:31:00, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 547 is completed and loss is 0.2528151571750641\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 548/1792 [1:32:49<3:31:00, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 549/1792 [1:33:00<3:30:34, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 549/1792 [1:33:00<3:30:34, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 548 is completed and loss is 0.23917946219444275\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 549/1792 [1:33:00<3:30:34, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 549/1792 [1:33:00<3:30:34, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 549 is completed and loss is 0.2799725830554962\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 550/1792 [1:33:10<3:31:03, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 550/1792 [1:33:10<3:31:03, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 550/1792 [1:33:10<3:31:03, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 550/1792 [1:33:10<3:31:03, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 550 is completed and loss is 0.21969470381736755\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 551/1792 [1:33:20<3:30:33, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 551/1792 [1:33:20<3:30:33, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 551/1792 [1:33:21<3:30:34, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 551/1792 [1:33:21<3:30:34, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 551 is completed and loss is 0.2162269949913025\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 552/1792 [1:33:30<3:30:56, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 552/1792 [1:33:30<3:30:56, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 552/1792 [1:33:31<3:30:56, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 552/1792 [1:33:31<3:30:57, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 552 is completed and loss is 0.24203187227249146\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 553/1792 [1:33:40<3:30:21, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 553/1792 [1:33:41<3:30:22, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 553/1792 [1:33:40<3:30:22, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 553/1792 [1:33:41<3:30:22, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 553 is completed and loss is 0.22370539605617523\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 554/1792 [1:33:51<3:30:01, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 554/1792 [1:33:51<3:30:01, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 554/1792 [1:33:51<3:30:01, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 554/1792 [1:33:50<3:30:01, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 554 is completed and loss is 0.2453790158033371\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 555/1792 [1:34:01<3:30:17, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 555/1792 [1:34:01<3:30:17, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 555/1792 [1:34:01<3:30:17, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 555/1792 [1:34:01<3:30:17, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 555 is completed and loss is 0.24478627741336823\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 556/1792 [1:34:11<3:29:43, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 556/1792 [1:34:12<3:29:43, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 556/1792 [1:34:11<3:29:43, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 556/1792 [1:34:12<3:29:43, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 556 is completed and loss is 0.2690141499042511\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 557/1792 [1:34:21<3:29:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 557/1792 [1:34:22<3:29:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 557/1792 [1:34:21<3:29:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 557/1792 [1:34:22<3:29:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 557 is completed and loss is 0.2428514063358307\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 558/1792 [1:34:31<3:29:01, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 558/1792 [1:34:32<3:29:01, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 558/1792 [1:34:32<3:29:01, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 558/1792 [1:34:31<3:29:01, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 558 is completed and loss is 0.21574360132217407\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 559/1792 [1:34:41<3:28:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 559/1792 [1:34:42<3:28:48, 10.16s/it]#015Training Epoch0:  31%|#033[34m███       #033[0m| 559/1792 [1:34:42<3:28:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███       #033[0m| 559/1792 [1:34:41<3:28:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 559 is completed and loss is 0.22015371918678284\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 560/1792 [1:34:52<3:28:37, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 560/1792 [1:34:52<3:28:37, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 560/1792 [1:34:52<3:28:38, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 560/1792 [1:34:51<3:28:38, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 560 is completed and loss is 0.2099924236536026\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 561/1792 [1:35:02<3:29:07, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 561/1792 [1:35:02<3:29:07, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 561/1792 [1:35:02<3:29:07, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 561/1792 [1:35:02<3:29:07, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 561 is completed and loss is 0.2562698721885681\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 562/1792 [1:35:12<3:28:33, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 562/1792 [1:35:13<3:28:33, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 562/1792 [1:35:12<3:28:33, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 562/1792 [1:35:13<3:28:34, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 562 is completed and loss is 0.22502031922340393\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 563/1792 [1:35:22<3:28:13, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 563/1792 [1:35:23<3:28:13, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 563/1792 [1:35:23<3:28:13, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 563/1792 [1:35:22<3:28:13, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 563 is completed and loss is 0.21654139459133148\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 564/1792 [1:35:32<3:27:53, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 564/1792 [1:35:33<3:27:53, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 564/1792 [1:35:33<3:27:53, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  31%|#033[34m███▏      #033[0m| 564/1792 [1:35:32<3:27:53, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 564 is completed and loss is 0.23805932700634003\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 565/1792 [1:35:42<3:27:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 565/1792 [1:35:42<3:27:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 565/1792 [1:35:43<3:27:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 565/1792 [1:35:43<3:27:40, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 565 is completed and loss is 0.25288355350494385\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 566/1792 [1:35:53<3:27:32, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 566/1792 [1:35:53<3:27:32, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 566/1792 [1:35:53<3:27:32, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 566/1792 [1:35:52<3:27:32, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 566 is completed and loss is 0.2358480840921402\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 567/1792 [1:36:03<3:27:57, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 567/1792 [1:36:03<3:27:57, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 567/1792 [1:36:03<3:27:57, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 567/1792 [1:36:03<3:27:57, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 567 is completed and loss is 0.2537187933921814\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 568/1792 [1:36:13<3:27:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 568/1792 [1:36:14<3:27:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 568/1792 [1:36:14<3:27:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 568/1792 [1:36:13<3:27:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 568 is completed and loss is 0.2681715190410614\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 569/1792 [1:36:23<3:27:11, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 569/1792 [1:36:24<3:27:11, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 569/1792 [1:36:24<3:27:11, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 569/1792 [1:36:23<3:27:11, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 569 is completed and loss is 0.22988280653953552\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 570/1792 [1:36:33<3:26:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 570/1792 [1:36:34<3:26:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 570/1792 [1:36:34<3:26:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 570/1792 [1:36:33<3:26:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 570 is completed and loss is 0.24098141491413116\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 571/1792 [1:36:43<3:26:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 571/1792 [1:36:44<3:26:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 571/1792 [1:36:43<3:26:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 571/1792 [1:36:44<3:26:35, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 571 is completed and loss is 0.23430298268795013\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 572/1792 [1:36:54<3:27:07, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 572/1792 [1:36:54<3:27:07, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 572/1792 [1:36:54<3:27:07, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 572/1792 [1:36:54<3:27:07, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 572 is completed and loss is 0.27018457651138306\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 573/1792 [1:37:04<3:27:27, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 573/1792 [1:37:04<3:27:27, 10.21s/it]#015Training Epoch0:  32%|#033[34m███▏      #033[0m| 573/1792 [1:37:05<3:27:27, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 573/1792 [1:37:04<3:27:27, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 573 is completed and loss is 0.2669629156589508\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 574/1792 [1:37:14<3:26:50, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 574/1792 [1:37:15<3:26:50, 10.19s/it]#015Training Epoch0:  32%|#033[34m███▏      #033[0m| 574/1792 [1:37:14<3:26:50, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 574/1792 [1:37:15<3:26:50, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 574 is completed and loss is 0.21005737781524658\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 575/1792 [1:37:24<3:26:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 575/1792 [1:37:25<3:26:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 575/1792 [1:37:24<3:26:22, 10.17s/it]#015Training Epoch0:  32%|#033[34m███▏      #033[0m| 575/1792 [1:37:25<3:26:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 575 is completed and loss is 0.23767594993114471\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 576/1792 [1:37:34<3:26:47, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 576/1792 [1:37:35<3:26:47, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 576/1792 [1:37:35<3:26:47, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 576/1792 [1:37:34<3:26:47, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 576 is completed and loss is 0.2274327427148819\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 577/1792 [1:37:45<3:26:20, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 577/1792 [1:37:45<3:26:20, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 577/1792 [1:37:45<3:26:20, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 577/1792 [1:37:45<3:26:20, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 577 is completed and loss is 0.2209283709526062\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 578/1792 [1:37:55<3:25:50, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 578/1792 [1:37:55<3:25:50, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 578/1792 [1:37:55<3:25:51, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 578/1792 [1:37:55<3:25:50, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 578 is completed and loss is 0.24191127717494965\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 579/1792 [1:38:05<3:25:32, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 579/1792 [1:38:05<3:25:32, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 579/1792 [1:38:05<3:25:32, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 579/1792 [1:38:05<3:25:32, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 580/1792 [1:38:16<3:25:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 579 is completed and loss is 0.273188978433609\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 580/1792 [1:38:15<3:25:14, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 580/1792 [1:38:16<3:25:14, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 580/1792 [1:38:15<3:25:14, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 581/1792 [1:38:26<3:24:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 580 is completed and loss is 0.23848550021648407\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 581/1792 [1:38:25<3:24:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 581/1792 [1:38:26<3:24:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 581/1792 [1:38:25<3:24:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 581 is completed and loss is 0.2206408530473709\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 582/1792 [1:38:35<3:24:41, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 582/1792 [1:38:36<3:24:41, 10.15s/it]#015Training Epoch0:  32%|#033[34m███▏      #033[0m| 582/1792 [1:38:36<3:24:41, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  32%|#033[34m███▏      #033[0m| 582/1792 [1:38:35<3:24:41, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 582 is completed and loss is 0.22543060779571533\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 583/1792 [1:38:46<3:24:37, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 583/1792 [1:38:46<3:24:37, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 583/1792 [1:38:46<3:24:37, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 583/1792 [1:38:45<3:24:37, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 583 is completed and loss is 0.18631422519683838\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 584/1792 [1:38:56<3:24:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 584/1792 [1:38:56<3:24:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 584/1792 [1:38:56<3:24:26, 10.15s/it]#015Training Epoch0:  33%|#033[34m███▎      #033[0m| 584/1792 [1:38:56<3:24:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 585/1792 [1:39:06<3:24:18, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 584 is completed and loss is 0.24485410749912262\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 585/1792 [1:39:06<3:24:18, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 585/1792 [1:39:06<3:24:18, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 585/1792 [1:39:06<3:24:18, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 586/1792 [1:39:17<3:24:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 586/1792 [1:39:17<3:24:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 586/1792 [1:39:16<3:24:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 585 is completed and loss is 0.22645936906337738\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 586/1792 [1:39:16<3:24:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 587/1792 [1:39:27<3:25:16, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 586 is completed and loss is 0.22365115582942963\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 587/1792 [1:39:27<3:25:16, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 587/1792 [1:39:26<3:25:16, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 587/1792 [1:39:26<3:25:16, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 587 is completed and loss is 0.24068816006183624\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 588/1792 [1:39:36<3:24:38, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 588/1792 [1:39:37<3:24:38, 10.20s/it]#015Training Epoch0:  33%|#033[34m███▎      #033[0m| 588/1792 [1:39:37<3:24:38, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 588/1792 [1:39:37<3:24:38, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 589/1792 [1:39:47<3:24:07, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 589/1792 [1:39:47<3:24:07, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 589/1792 [1:39:47<3:24:08, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 588 is completed and loss is 0.2094217985868454\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 589/1792 [1:39:47<3:24:08, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 589 is completed and loss is 0.22145366668701172\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 590/1792 [1:39:57<3:23:44, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 590/1792 [1:39:57<3:23:44, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 590/1792 [1:39:57<3:23:44, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 590/1792 [1:39:57<3:23:44, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 590 is completed and loss is 0.2629128694534302\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 591/1792 [1:40:07<3:23:25, 10.16s/it]#015Training Epoch0:  33%|#033[34m███▎      #033[0m| 591/1792 [1:40:07<3:23:25, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 591/1792 [1:40:08<3:23:25, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 591/1792 [1:40:07<3:23:25, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 592/1792 [1:40:18<3:23:07, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 591 is completed and loss is 0.22232407331466675\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 592/1792 [1:40:17<3:23:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 592/1792 [1:40:17<3:23:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 592/1792 [1:40:18<3:23:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 593/1792 [1:40:28<3:23:40, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 593/1792 [1:40:28<3:23:40, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 592 is completed and loss is 0.22013738751411438\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 593/1792 [1:40:27<3:23:40, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 593/1792 [1:40:27<3:23:40, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 594/1792 [1:40:38<3:23:15, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 593 is completed and loss is 0.23815582692623138\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 594/1792 [1:40:38<3:23:15, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 594/1792 [1:40:38<3:23:15, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 594/1792 [1:40:37<3:23:15, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 595/1792 [1:40:48<3:22:59, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 594 is completed and loss is 0.22257636487483978\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 595/1792 [1:40:48<3:23:00, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 595/1792 [1:40:48<3:23:00, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 595/1792 [1:40:48<3:23:00, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 595 is completed and loss is 0.26209092140197754\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 596/1792 [1:40:58<3:22:38, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 596/1792 [1:40:58<3:22:38, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 596/1792 [1:40:58<3:22:38, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 596/1792 [1:40:58<3:22:38, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 597/1792 [1:41:09<3:22:21, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 597/1792 [1:41:08<3:22:21, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 596 is completed and loss is 0.22002239525318146\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 597/1792 [1:41:08<3:22:21, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 597/1792 [1:41:09<3:22:21, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 598/1792 [1:41:19<3:23:39, 10.23s/it]\u001b[0m\n",
      "\u001b[34mstep 597 is completed and loss is 0.2347559630870819\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 598/1792 [1:41:18<3:23:39, 10.23s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 598/1792 [1:41:19<3:23:40, 10.23s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 598/1792 [1:41:18<3:23:40, 10.23s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 599/1792 [1:41:29<3:22:51, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 599/1792 [1:41:28<3:22:51, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 598 is completed and loss is 0.23636214435100555\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 599/1792 [1:41:29<3:22:51, 10.20s/it]#015Training Epoch0:  33%|#033[34m███▎      #033[0m| 599/1792 [1:41:29<3:22:51, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 599 is completed and loss is 0.24446478486061096\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 600/1792 [1:41:39<3:22:19, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 600/1792 [1:41:39<3:22:19, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 600/1792 [1:41:39<3:22:19, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  33%|#033[34m███▎      #033[0m| 600/1792 [1:41:39<3:22:19, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 601/1792 [1:41:49<3:22:03, 10.18s/it]#015Training Epoch0:  34%|#033[34m███▎      #033[0m| 601/1792 [1:41:49<3:22:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 600 is completed and loss is 0.2710651755332947\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 601/1792 [1:41:49<3:22:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 601/1792 [1:41:49<3:22:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 602/1792 [1:41:59<3:21:36, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 601 is completed and loss is 0.25079289078712463\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 602/1792 [1:42:00<3:21:36, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 602/1792 [1:41:59<3:21:36, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 602/1792 [1:41:59<3:21:36, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 603/1792 [1:42:10<3:21:21, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 603/1792 [1:42:10<3:21:22, 10.16s/it]#015Training Epoch0:  34%|#033[34m███▎      #033[0m| 603/1792 [1:42:09<3:21:21, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 602 is completed and loss is 0.2705293297767639\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 603/1792 [1:42:09<3:21:22, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 603 is completed and loss is 0.23775598406791687\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 604/1792 [1:42:20<3:21:54, 10.20s/it]#015Training Epoch0:  34%|#033[34m███▎      #033[0m| 604/1792 [1:42:19<3:21:54, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 604/1792 [1:42:19<3:21:54, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▎      #033[0m| 604/1792 [1:42:20<3:21:54, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 605/1792 [1:42:30<3:21:28, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 604 is completed and loss is 0.23551061749458313\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 605/1792 [1:42:30<3:21:28, 10.18s/it]#015Training Epoch0:  34%|#033[34m███▍      #033[0m| 605/1792 [1:42:30<3:21:28, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 605/1792 [1:42:29<3:21:28, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 606/1792 [1:42:40<3:21:04, 10.17s/it]#015Training Epoch0:  34%|#033[34m███▍      #033[0m| 606/1792 [1:42:40<3:21:04, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 605 is completed and loss is 0.23706725239753723\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 606/1792 [1:42:40<3:21:04, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 606/1792 [1:42:40<3:21:04, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 606 is completed and loss is 0.216293603181839\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 607/1792 [1:42:50<3:20:33, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 607/1792 [1:42:50<3:20:33, 10.15s/it]#015Training Epoch0:  34%|#033[34m███▍      #033[0m| 607/1792 [1:42:50<3:20:33, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 607/1792 [1:42:50<3:20:33, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 607 is completed and loss is 0.27500227093696594\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 608/1792 [1:43:00<3:20:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 608/1792 [1:43:00<3:20:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 608/1792 [1:43:00<3:20:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 608/1792 [1:43:00<3:20:20, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 609/1792 [1:43:11<3:20:52, 10.19s/it]#015Training Epoch0:  34%|#033[34m███▍      #033[0m| 609/1792 [1:43:11<3:20:52, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 609/1792 [1:43:10<3:20:52, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 608 is completed and loss is 0.22254249453544617\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 609/1792 [1:43:10<3:20:52, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 610/1792 [1:43:20<3:20:23, 10.17s/it]#015Training Epoch0:  34%|#033[34m███▍      #033[0m| 610/1792 [1:43:21<3:20:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 610/1792 [1:43:21<3:20:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 609 is completed and loss is 0.23647838830947876\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 610/1792 [1:43:20<3:20:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 610 is completed and loss is 0.21432511508464813\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 611/1792 [1:43:30<3:19:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 611/1792 [1:43:30<3:19:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 611/1792 [1:43:31<3:19:59, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 611/1792 [1:43:31<3:19:59, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 612/1792 [1:43:41<3:19:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 611 is completed and loss is 0.24666094779968262\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 612/1792 [1:43:41<3:19:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 612/1792 [1:43:41<3:19:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 612/1792 [1:43:41<3:19:47, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 612 is completed and loss is 0.24301403760910034\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 613/1792 [1:43:51<3:20:11, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 613/1792 [1:43:51<3:20:11, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 613/1792 [1:43:51<3:20:11, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 613/1792 [1:43:51<3:20:11, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 614/1792 [1:44:02<3:19:46, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 613 is completed and loss is 0.19640487432479858\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 614/1792 [1:44:01<3:19:46, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 614/1792 [1:44:02<3:19:46, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 614/1792 [1:44:01<3:19:46, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 615/1792 [1:44:12<3:20:16, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 614 is completed and loss is 0.23570720851421356\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 615/1792 [1:44:11<3:20:17, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 615/1792 [1:44:12<3:20:17, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 615/1792 [1:44:11<3:20:17, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 616/1792 [1:44:21<3:19:44, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 616/1792 [1:44:22<3:19:45, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 615 is completed and loss is 0.24284188449382782\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 616/1792 [1:44:21<3:19:45, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 616/1792 [1:44:22<3:19:45, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 617/1792 [1:44:32<3:19:16, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 617/1792 [1:44:32<3:19:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 617/1792 [1:44:32<3:19:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 616 is completed and loss is 0.23202849924564362\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 617/1792 [1:44:32<3:19:17, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 618/1792 [1:44:42<3:18:56, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 617 is completed and loss is 0.19947579503059387\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 618/1792 [1:44:42<3:18:57, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 618/1792 [1:44:42<3:18:57, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  34%|#033[34m███▍      #033[0m| 618/1792 [1:44:42<3:18:57, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 619/1792 [1:44:52<3:19:18, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 619/1792 [1:44:53<3:19:18, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 618 is completed and loss is 0.2298736274242401\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 619/1792 [1:44:52<3:19:18, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 619/1792 [1:44:53<3:19:18, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 620/1792 [1:45:02<3:18:49, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 620/1792 [1:45:03<3:18:49, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 620/1792 [1:45:03<3:18:49, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 619 is completed and loss is 0.21479634940624237\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 620/1792 [1:45:02<3:18:49, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 621/1792 [1:45:12<3:18:30, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 620 is completed and loss is 0.2084827423095703\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 621/1792 [1:45:12<3:18:29, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 621/1792 [1:45:13<3:18:30, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 621/1792 [1:45:13<3:18:30, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 622/1792 [1:45:22<3:18:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 621 is completed and loss is 0.22513200342655182\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 622/1792 [1:45:23<3:18:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 622/1792 [1:45:22<3:18:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 622/1792 [1:45:23<3:18:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 623/1792 [1:45:32<3:17:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 623/1792 [1:45:33<3:17:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 622 is completed and loss is 0.21697856485843658\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 623/1792 [1:45:33<3:17:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 623/1792 [1:45:33<3:17:51, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 624/1792 [1:45:43<3:18:19, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 624/1792 [1:45:43<3:18:19, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 623 is completed and loss is 0.2400602102279663\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 624/1792 [1:45:43<3:18:19, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 624/1792 [1:45:43<3:18:19, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 625/1792 [1:45:53<3:17:59, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 625/1792 [1:45:54<3:17:59, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 624 is completed and loss is 0.24309152364730835\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 625/1792 [1:45:53<3:17:59, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 625/1792 [1:45:54<3:18:00, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 626/1792 [1:46:03<3:17:44, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 626/1792 [1:46:04<3:17:44, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 625 is completed and loss is 0.23421190679073334\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 626/1792 [1:46:04<3:17:45, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 626/1792 [1:46:03<3:17:44, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 627/1792 [1:46:13<3:17:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 626 is completed and loss is 0.259425550699234\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 627/1792 [1:46:13<3:17:23, 10.17s/it]#015Training Epoch0:  35%|#033[34m███▍      #033[0m| 627/1792 [1:46:14<3:17:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▍      #033[0m| 627/1792 [1:46:14<3:17:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 628/1792 [1:46:23<3:17:07, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 628/1792 [1:46:24<3:17:07, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 627 is completed and loss is 0.20600564777851105\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 628/1792 [1:46:24<3:17:07, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 628/1792 [1:46:23<3:17:07, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 629/1792 [1:46:34<3:16:56, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 629/1792 [1:46:34<3:16:56, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 628 is completed and loss is 0.2792004942893982\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 629/1792 [1:46:34<3:16:56, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 629/1792 [1:46:34<3:16:56, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 630/1792 [1:46:44<3:17:20, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 630/1792 [1:46:44<3:17:21, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 629 is completed and loss is 0.2123648226261139\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 630/1792 [1:46:44<3:17:20, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 630/1792 [1:46:44<3:17:20, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 630 is completed and loss is 0.21845221519470215\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 631/1792 [1:46:54<3:16:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 631/1792 [1:46:55<3:16:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 631/1792 [1:46:55<3:16:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 631/1792 [1:46:54<3:16:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 632/1792 [1:47:05<3:16:28, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 632/1792 [1:47:05<3:16:28, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 631 is completed and loss is 0.20546460151672363\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 632/1792 [1:47:04<3:16:28, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 632/1792 [1:47:04<3:16:28, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 633/1792 [1:47:15<3:16:06, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 632 is completed and loss is 0.23570331931114197\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 633/1792 [1:47:14<3:16:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 633/1792 [1:47:15<3:16:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 633/1792 [1:47:14<3:16:07, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 634/1792 [1:47:25<3:15:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 633 is completed and loss is 0.22666887938976288\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 634/1792 [1:47:25<3:15:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 634/1792 [1:47:24<3:15:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 634/1792 [1:47:24<3:15:58, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 635/1792 [1:47:35<3:16:27, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 634 is completed and loss is 0.22845406830310822\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 635/1792 [1:47:35<3:16:27, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 635/1792 [1:47:35<3:16:27, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 635/1792 [1:47:35<3:16:27, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 636/1792 [1:47:46<3:16:47, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 636/1792 [1:47:46<3:16:47, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 635 is completed and loss is 0.23782344162464142\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 636/1792 [1:47:45<3:16:47, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  35%|#033[34m███▌      #033[0m| 636/1792 [1:47:45<3:16:47, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 637/1792 [1:47:56<3:16:09, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 636 is completed and loss is 0.2694542109966278\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 637/1792 [1:47:55<3:16:10, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 637/1792 [1:47:56<3:16:10, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 637/1792 [1:47:55<3:16:10, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 638/1792 [1:48:06<3:15:45, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 638/1792 [1:48:06<3:15:45, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 637 is completed and loss is 0.2040666788816452\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 638/1792 [1:48:05<3:15:46, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 638/1792 [1:48:05<3:15:45, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 639/1792 [1:48:16<3:16:09, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 638 is completed and loss is 0.22718526422977448\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 639/1792 [1:48:16<3:16:09, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 639/1792 [1:48:16<3:16:09, 10.21s/it]#015Training Epoch0:  36%|#033[34m███▌      #033[0m| 639/1792 [1:48:15<3:16:09, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 640/1792 [1:48:26<3:15:36, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 640/1792 [1:48:26<3:15:35, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 639 is completed and loss is 0.18764278292655945\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 640/1792 [1:48:26<3:15:36, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 640/1792 [1:48:26<3:15:36, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 641/1792 [1:48:36<3:15:07, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 640 is completed and loss is 0.24823631346225739\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 641/1792 [1:48:36<3:15:07, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 641/1792 [1:48:36<3:15:07, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 641/1792 [1:48:36<3:15:07, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 642/1792 [1:48:47<3:14:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 641 is completed and loss is 0.266834020614624\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 642/1792 [1:48:46<3:14:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 642/1792 [1:48:46<3:14:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 642/1792 [1:48:46<3:14:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 643/1792 [1:48:57<3:14:27, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 642 is completed and loss is 0.2229289710521698\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 643/1792 [1:48:56<3:14:27, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 643/1792 [1:48:57<3:14:27, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 643/1792 [1:48:56<3:14:27, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 644/1792 [1:49:07<3:14:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 644/1792 [1:49:07<3:14:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 643 is completed and loss is 0.25128623843193054\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 644/1792 [1:49:06<3:14:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 644/1792 [1:49:06<3:14:09, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 645/1792 [1:49:17<3:14:00, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 644 is completed and loss is 0.2470187246799469\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 645/1792 [1:49:17<3:14:00, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 645/1792 [1:49:16<3:14:00, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 645/1792 [1:49:16<3:14:00, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 646/1792 [1:49:27<3:13:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 645 is completed and loss is 0.2475111484527588\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 646/1792 [1:49:27<3:13:56, 10.15s/it]#015Training Epoch0:  36%|#033[34m███▌      #033[0m| 646/1792 [1:49:27<3:13:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 646/1792 [1:49:26<3:13:56, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 647/1792 [1:49:37<3:13:44, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 646 is completed and loss is 0.243485689163208\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 647/1792 [1:49:37<3:13:44, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 647/1792 [1:49:37<3:13:44, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 647/1792 [1:49:37<3:13:44, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 648/1792 [1:49:47<3:13:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 647 is completed and loss is 0.236980602145195\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 648/1792 [1:49:47<3:13:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 648/1792 [1:49:47<3:13:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 648/1792 [1:49:47<3:13:28, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 649/1792 [1:49:58<3:13:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 648 is completed and loss is 0.24736671149730682\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 649/1792 [1:49:58<3:13:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 649/1792 [1:49:57<3:13:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▌      #033[0m| 649/1792 [1:49:57<3:13:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 650/1792 [1:50:08<3:14:34, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 650/1792 [1:50:08<3:14:33, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 649 is completed and loss is 0.22055383026599884\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 650/1792 [1:50:07<3:14:34, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 650/1792 [1:50:07<3:14:34, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 651/1792 [1:50:18<3:13:57, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 650 is completed and loss is 0.24104595184326172\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 651/1792 [1:50:18<3:13:57, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 651/1792 [1:50:18<3:13:57, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 651/1792 [1:50:17<3:13:57, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 652/1792 [1:50:28<3:13:28, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 652/1792 [1:50:28<3:13:28, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 651 is completed and loss is 0.24073998630046844\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 652/1792 [1:50:28<3:13:28, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 652/1792 [1:50:28<3:13:28, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 653/1792 [1:50:38<3:13:05, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 653/1792 [1:50:38<3:13:04, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 653/1792 [1:50:38<3:13:05, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 652 is completed and loss is 0.23396369814872742\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 653/1792 [1:50:38<3:13:05, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 654/1792 [1:50:49<3:12:44, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 654/1792 [1:50:48<3:12:44, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 653 is completed and loss is 0.26426616311073303\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 654/1792 [1:50:48<3:12:44, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  36%|#033[34m███▋      #033[0m| 654/1792 [1:50:48<3:12:44, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 655/1792 [1:50:59<3:12:28, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 655/1792 [1:50:59<3:12:28, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 654 is completed and loss is 0.253449022769928\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 655/1792 [1:50:58<3:12:28, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 655/1792 [1:50:58<3:12:28, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 655 is completed and loss is 0.20993773639202118\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 656/1792 [1:51:08<3:12:55, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 656/1792 [1:51:09<3:12:56, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 656/1792 [1:51:08<3:12:55, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 656/1792 [1:51:09<3:12:56, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 656 is completed and loss is 0.2531008720397949\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 657/1792 [1:51:19<3:12:34, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 657/1792 [1:51:19<3:12:34, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 657/1792 [1:51:18<3:12:34, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 657/1792 [1:51:19<3:12:34, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 657 is completed and loss is 0.22790735960006714\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 658/1792 [1:51:29<3:12:15, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 658/1792 [1:51:29<3:12:15, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 658/1792 [1:51:29<3:12:15, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 658/1792 [1:51:29<3:12:15, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 659/1792 [1:51:39<3:11:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 658 is completed and loss is 0.23167060315608978\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 659/1792 [1:51:39<3:11:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 659/1792 [1:51:39<3:11:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 659/1792 [1:51:39<3:11:49, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 659 is completed and loss is 0.16733428835868835\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 660/1792 [1:51:49<3:11:38, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 660/1792 [1:51:50<3:11:38, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 660/1792 [1:51:49<3:11:38, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 660/1792 [1:51:49<3:11:38, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 660 is completed and loss is 0.23835507035255432\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 661/1792 [1:51:59<3:12:06, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 661/1792 [1:52:00<3:12:06, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 661/1792 [1:52:00<3:12:06, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 661/1792 [1:51:59<3:12:06, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 662/1792 [1:52:10<3:11:40, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 661 is completed and loss is 0.20035049319267273\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 662/1792 [1:52:09<3:11:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 662/1792 [1:52:09<3:11:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 662/1792 [1:52:10<3:11:41, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 662 is completed and loss is 0.2660882771015167\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 663/1792 [1:52:20<3:11:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 663/1792 [1:52:20<3:11:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 663/1792 [1:52:19<3:11:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 663/1792 [1:52:20<3:11:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 664/1792 [1:52:30<3:10:56, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 663 is completed and loss is 0.2776154577732086\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 664/1792 [1:52:30<3:10:56, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 664/1792 [1:52:30<3:10:56, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 664/1792 [1:52:30<3:10:56, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 665/1792 [1:52:40<3:10:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 664 is completed and loss is 0.2770272493362427\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 665/1792 [1:52:40<3:10:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 665/1792 [1:52:40<3:10:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 665/1792 [1:52:40<3:10:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 666/1792 [1:52:50<3:10:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 665 is completed and loss is 0.1968030035495758\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 666/1792 [1:52:50<3:10:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 666/1792 [1:52:50<3:10:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 666/1792 [1:52:50<3:10:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 666 is completed and loss is 0.2564968764781952\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 667/1792 [1:53:01<3:10:55, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 667/1792 [1:53:00<3:10:55, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 667/1792 [1:53:01<3:10:55, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 667/1792 [1:53:00<3:10:55, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 668/1792 [1:53:11<3:10:35, 10.17s/it]#015Training Epoch0:  37%|#033[34m███▋      #033[0m| 668/1792 [1:53:11<3:10:36, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 667 is completed and loss is 0.1902720034122467\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 668/1792 [1:53:10<3:10:36, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 668/1792 [1:53:10<3:10:35, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 669/1792 [1:53:21<3:10:11, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 668 is completed and loss is 0.24659322202205658\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 669/1792 [1:53:20<3:10:11, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 669/1792 [1:53:20<3:10:12, 10.16s/it]#015Training Epoch0:  37%|#033[34m███▋      #033[0m| 669/1792 [1:53:21<3:10:11, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 670/1792 [1:53:31<3:10:30, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 669 is completed and loss is 0.2248317152261734\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 670/1792 [1:53:31<3:10:31, 10.19s/it]#015Training Epoch0:  37%|#033[34m███▋      #033[0m| 670/1792 [1:53:31<3:10:31, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 670/1792 [1:53:31<3:10:31, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 671/1792 [1:53:41<3:10:09, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 670 is completed and loss is 0.2628861367702484\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 671/1792 [1:53:41<3:10:09, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 671/1792 [1:53:41<3:10:09, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  37%|#033[34m███▋      #033[0m| 671/1792 [1:53:41<3:10:09, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 671 is completed and loss is 0.20753347873687744\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 672/1792 [1:53:51<3:09:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 672/1792 [1:53:51<3:09:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 672/1792 [1:53:52<3:09:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 672/1792 [1:53:52<3:09:52, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 673/1792 [1:54:02<3:09:33, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 672 is completed and loss is 0.26434651017189026\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 673/1792 [1:54:01<3:09:33, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 673/1792 [1:54:01<3:09:33, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 673/1792 [1:54:02<3:09:33, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 674/1792 [1:54:12<3:09:19, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 673 is completed and loss is 0.24640387296676636\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 674/1792 [1:54:11<3:09:20, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 674/1792 [1:54:11<3:09:20, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 674/1792 [1:54:12<3:09:20, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 675/1792 [1:54:22<3:09:02, 10.15s/it]#015Training Epoch0:  38%|#033[34m███▊      #033[0m| 675/1792 [1:54:22<3:09:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 674 is completed and loss is 0.2518361508846283\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 675/1792 [1:54:21<3:09:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 675/1792 [1:54:21<3:09:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 676/1792 [1:54:32<3:09:28, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 675 is completed and loss is 0.26347222924232483\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 676/1792 [1:54:32<3:09:28, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 676/1792 [1:54:32<3:09:28, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 676/1792 [1:54:32<3:09:28, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 676 is completed and loss is 0.2153157740831375\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 677/1792 [1:54:42<3:09:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 677/1792 [1:54:42<3:09:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 677/1792 [1:54:42<3:09:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 677/1792 [1:54:42<3:09:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 678/1792 [1:54:53<3:09:23, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 677 is completed and loss is 0.23485466837882996\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 678/1792 [1:54:52<3:09:23, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 678/1792 [1:54:52<3:09:23, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 678/1792 [1:54:53<3:09:24, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 678 is completed and loss is 0.25809144973754883\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 679/1792 [1:55:02<3:08:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 679/1792 [1:55:02<3:08:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 679/1792 [1:55:03<3:08:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 679/1792 [1:55:03<3:08:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 680/1792 [1:55:13<3:08:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 680/1792 [1:55:12<3:08:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 679 is completed and loss is 0.24519065022468567\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 680/1792 [1:55:12<3:08:29, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 680/1792 [1:55:13<3:08:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 681/1792 [1:55:22<3:08:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 681/1792 [1:55:23<3:08:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 680 is completed and loss is 0.25928470492362976\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 681/1792 [1:55:23<3:08:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 681/1792 [1:55:23<3:08:06, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 682/1792 [1:55:33<3:08:25, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 682/1792 [1:55:33<3:08:25, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 681 is completed and loss is 0.20851978659629822\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 682/1792 [1:55:33<3:08:25, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 682/1792 [1:55:33<3:08:25, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 683/1792 [1:55:43<3:07:59, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 682 is completed and loss is 0.24022945761680603\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 683/1792 [1:55:43<3:07:59, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 683/1792 [1:55:44<3:07:59, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 683/1792 [1:55:43<3:07:59, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 684/1792 [1:55:54<3:07:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 684/1792 [1:55:53<3:07:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 683 is completed and loss is 0.20318667590618134\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 684/1792 [1:55:54<3:07:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 684/1792 [1:55:53<3:07:41, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 685/1792 [1:56:04<3:07:24, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 684 is completed and loss is 0.2584172785282135\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 685/1792 [1:56:03<3:07:24, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 685/1792 [1:56:03<3:07:24, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 685/1792 [1:56:04<3:07:24, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 686/1792 [1:56:14<3:07:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 685 is completed and loss is 0.2093394547700882\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 686/1792 [1:56:13<3:07:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 686/1792 [1:56:13<3:07:13, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 686/1792 [1:56:14<3:07:14, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 687/1792 [1:56:24<3:07:01, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 686 is completed and loss is 0.232506662607193\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 687/1792 [1:56:24<3:07:01, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 687/1792 [1:56:24<3:07:01, 10.16s/it]#015Training Epoch0:  38%|#033[34m███▊      #033[0m| 687/1792 [1:56:23<3:07:01, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 688/1792 [1:56:34<3:06:49, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 687 is completed and loss is 0.20217324793338776\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 688/1792 [1:56:34<3:06:49, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 688/1792 [1:56:34<3:06:49, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 688/1792 [1:56:34<3:06:49, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 689/1792 [1:56:44<3:06:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 688 is completed and loss is 0.22600242495536804\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 689/1792 [1:56:44<3:06:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 689/1792 [1:56:44<3:06:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  38%|#033[34m███▊      #033[0m| 689/1792 [1:56:44<3:06:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 689 is completed and loss is 0.2037186175584793\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 690/1792 [1:56:54<3:07:02, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 690/1792 [1:56:55<3:07:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 690/1792 [1:56:55<3:07:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 690/1792 [1:56:54<3:07:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 690 is completed and loss is 0.22024910151958466\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 691/1792 [1:57:05<3:06:40, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 691/1792 [1:57:04<3:06:40, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 691/1792 [1:57:04<3:06:40, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 691/1792 [1:57:05<3:06:41, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 691 is completed and loss is 0.19889889657497406\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 692/1792 [1:57:15<3:06:25, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 692/1792 [1:57:14<3:06:25, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 692/1792 [1:57:14<3:06:25, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 692/1792 [1:57:15<3:06:25, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 692 is completed and loss is 0.23724547028541565\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 693/1792 [1:57:25<3:06:02, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 693/1792 [1:57:25<3:06:02, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 693/1792 [1:57:24<3:06:02, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 693/1792 [1:57:25<3:06:02, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 693 is completed and loss is 0.2671799957752228\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 694/1792 [1:57:35<3:05:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 694/1792 [1:57:35<3:05:46, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 694/1792 [1:57:35<3:05:47, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▊      #033[0m| 694/1792 [1:57:35<3:05:47, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 695/1792 [1:57:45<3:05:34, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 694 is completed and loss is 0.23711258172988892\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 695/1792 [1:57:45<3:05:34, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 695/1792 [1:57:45<3:05:34, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 695/1792 [1:57:45<3:05:34, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 696/1792 [1:57:56<3:06:05, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 696/1792 [1:57:56<3:06:05, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 695 is completed and loss is 0.23320485651493073\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 696/1792 [1:57:55<3:06:06, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 696/1792 [1:57:55<3:06:06, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 697/1792 [1:58:06<3:05:42, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 696 is completed and loss is 0.236318901181221\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 697/1792 [1:58:05<3:05:43, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 697/1792 [1:58:05<3:05:43, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 697/1792 [1:58:06<3:05:43, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 698/1792 [1:58:16<3:05:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 697 is completed and loss is 0.22835443913936615\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 698/1792 [1:58:15<3:05:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 698/1792 [1:58:16<3:05:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 698/1792 [1:58:15<3:05:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 699/1792 [1:58:26<3:05:43, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 698 is completed and loss is 0.21348829567432404\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 699/1792 [1:58:26<3:05:43, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 699/1792 [1:58:26<3:05:43, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 699/1792 [1:58:26<3:05:43, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 700/1792 [1:58:36<3:05:15, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 699 is completed and loss is 0.27450957894325256\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 700/1792 [1:58:36<3:05:15, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 700/1792 [1:58:36<3:05:15, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 700/1792 [1:58:36<3:05:15, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 701/1792 [1:58:47<3:05:36, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 700 is completed and loss is 0.21275191009044647\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 701/1792 [1:58:46<3:05:35, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 701/1792 [1:58:47<3:05:35, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 701/1792 [1:58:46<3:05:36, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 702/1792 [1:58:57<3:05:45, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 702/1792 [1:58:56<3:05:44, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 701 is completed and loss is 0.2662012279033661\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 702/1792 [1:58:56<3:05:44, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 702/1792 [1:58:57<3:05:44, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 703/1792 [1:59:06<3:05:06, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 702 is completed and loss is 0.23619040846824646\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 703/1792 [1:59:06<3:05:06, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 703/1792 [1:59:07<3:05:06, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 703/1792 [1:59:07<3:05:07, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 704/1792 [1:59:17<3:04:40, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 703 is completed and loss is 0.254229873418808\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 704/1792 [1:59:17<3:04:40, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 704/1792 [1:59:17<3:04:39, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 704/1792 [1:59:17<3:04:40, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 704 is completed and loss is 0.2631329894065857\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 705/1792 [1:59:27<3:04:18, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 705/1792 [1:59:27<3:04:18, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 705/1792 [1:59:27<3:04:18, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 705/1792 [1:59:27<3:04:18, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 706/1792 [1:59:37<3:04:06, 10.17s/it]#015Training Epoch0:  39%|#033[34m███▉      #033[0m| 706/1792 [1:59:37<3:04:06, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 705 is completed and loss is 0.23741774260997772\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 706/1792 [1:59:37<3:04:06, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 706/1792 [1:59:37<3:04:06, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 706 is completed and loss is 0.22202858328819275\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 707/1792 [1:59:47<3:03:45, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 707/1792 [1:59:48<3:03:45, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 707/1792 [1:59:48<3:03:45, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  39%|#033[34m███▉      #033[0m| 707/1792 [1:59:47<3:03:45, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 708/1792 [1:59:58<3:03:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 707 is completed and loss is 0.2226131707429886\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 708/1792 [1:59:57<3:03:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 708/1792 [1:59:58<3:03:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 708/1792 [1:59:57<3:03:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 709/1792 [2:00:08<3:03:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 708 is completed and loss is 0.2439580112695694\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 709/1792 [2:00:07<3:03:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 709/1792 [2:00:08<3:03:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 709/1792 [2:00:07<3:03:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 710/1792 [2:00:18<3:03:39, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 709 is completed and loss is 0.20655250549316406\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 710/1792 [2:00:18<3:03:39, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 710/1792 [2:00:18<3:03:39, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 710/1792 [2:00:18<3:03:39, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 711/1792 [2:00:28<3:03:14, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 710 is completed and loss is 0.2415425181388855\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 711/1792 [2:00:28<3:03:14, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 711/1792 [2:00:28<3:03:14, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 711/1792 [2:00:28<3:03:14, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 711 is completed and loss is 0.26195287704467773\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 712/1792 [2:00:39<3:03:38, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 712/1792 [2:00:38<3:03:38, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 712/1792 [2:00:39<3:03:39, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 712/1792 [2:00:38<3:03:38, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 713/1792 [2:00:49<3:03:52, 10.23s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 713/1792 [2:00:49<3:03:52, 10.23s/it]\u001b[0m\n",
      "\u001b[34mstep 712 is completed and loss is 0.20755323767662048\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 713/1792 [2:00:48<3:03:53, 10.23s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 713/1792 [2:00:48<3:03:53, 10.23s/it]\u001b[0m\n",
      "\u001b[34mstep 713 is completed and loss is 0.25151094794273376\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 714/1792 [2:00:59<3:03:18, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 714/1792 [2:00:58<3:03:18, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 714/1792 [2:00:59<3:03:18, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 714/1792 [2:00:58<3:03:18, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 715/1792 [2:01:09<3:02:51, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 715/1792 [2:01:08<3:02:51, 10.19s/it]#015Training Epoch0:  40%|#033[34m███▉      #033[0m| 715/1792 [2:01:09<3:02:51, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 714 is completed and loss is 0.23318979144096375\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 715/1792 [2:01:09<3:02:51, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 716/1792 [2:01:19<3:03:07, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 715 is completed and loss is 0.22990818321704865\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 716/1792 [2:01:19<3:03:07, 10.21s/it]#015Training Epoch0:  40%|#033[34m███▉      #033[0m| 716/1792 [2:01:19<3:03:07, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m███▉      #033[0m| 716/1792 [2:01:19<3:03:07, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 717/1792 [2:01:29<3:02:34, 10.19s/it]#015Training Epoch0:  40%|#033[34m████      #033[0m| 717/1792 [2:01:30<3:02:34, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 716 is completed and loss is 0.2647436559200287\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 717/1792 [2:01:29<3:02:34, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 717/1792 [2:01:30<3:02:34, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 717 is completed and loss is 0.19827836751937866\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 718/1792 [2:01:39<3:02:01, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 718/1792 [2:01:39<3:02:01, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 718/1792 [2:01:40<3:02:01, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 718/1792 [2:01:40<3:02:01, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 719/1792 [2:01:49<3:01:42, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 719/1792 [2:01:50<3:01:42, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 718 is completed and loss is 0.2306971251964569\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 719/1792 [2:01:49<3:01:42, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 719/1792 [2:01:50<3:01:42, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 719 is completed and loss is 0.250833123922348\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 720/1792 [2:01:59<3:01:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 720/1792 [2:01:59<3:01:24, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 720/1792 [2:02:00<3:01:24, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 720/1792 [2:02:00<3:01:24, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 720 is completed and loss is 0.2313614934682846\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 721/1792 [2:02:10<3:01:54, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 721/1792 [2:02:10<3:01:54, 10.19s/it]#015Training Epoch0:  40%|#033[34m████      #033[0m| 721/1792 [2:02:10<3:01:54, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 721/1792 [2:02:10<3:01:54, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 722/1792 [2:02:20<3:01:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 721 is completed and loss is 0.21627689898014069\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 722/1792 [2:02:20<3:01:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 722/1792 [2:02:20<3:01:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 722/1792 [2:02:20<3:01:24, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 723/1792 [2:02:30<3:01:10, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 723/1792 [2:02:31<3:01:11, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 722 is completed and loss is 0.25815528631210327\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 723/1792 [2:02:31<3:01:11, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 723/1792 [2:02:30<3:01:11, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 724/1792 [2:02:40<3:01:30, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 724/1792 [2:02:41<3:01:30, 10.20s/it]#015Training Epoch0:  40%|#033[34m████      #033[0m| 724/1792 [2:02:41<3:01:30, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 723 is completed and loss is 0.23894254863262177\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 724/1792 [2:02:40<3:01:30, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 725/1792 [2:02:50<3:01:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 725/1792 [2:02:51<3:01:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 724 is completed and loss is 0.24203795194625854\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 725/1792 [2:02:50<3:01:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  40%|#033[34m████      #033[0m| 725/1792 [2:02:51<3:01:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 726/1792 [2:03:00<3:00:41, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 726/1792 [2:03:01<3:00:41, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 726/1792 [2:03:01<3:00:41, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 725 is completed and loss is 0.2123653143644333\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 726/1792 [2:03:01<3:00:41, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 727/1792 [2:03:11<3:00:24, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 727/1792 [2:03:11<3:00:23, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 726 is completed and loss is 0.23668141663074493\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 727/1792 [2:03:11<3:00:24, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 727/1792 [2:03:11<3:00:24, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 728/1792 [2:03:21<3:00:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 727 is completed and loss is 0.2630164623260498\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 728/1792 [2:03:21<3:00:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 728/1792 [2:03:21<3:00:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 728/1792 [2:03:21<3:00:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 729/1792 [2:03:31<2:59:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 728 is completed and loss is 0.2675498425960541\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 729/1792 [2:03:31<2:59:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 729/1792 [2:03:31<2:59:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 729/1792 [2:03:32<2:59:53, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 730/1792 [2:03:41<2:59:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 729 is completed and loss is 0.25014421343803406\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 730/1792 [2:03:41<2:59:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 730/1792 [2:03:42<2:59:36, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 730/1792 [2:03:42<2:59:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 730 is completed and loss is 0.22401829063892365\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 731/1792 [2:03:51<2:59:23, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 731/1792 [2:03:51<2:59:22, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 731/1792 [2:03:52<2:59:23, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 731/1792 [2:03:52<2:59:23, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 731 is completed and loss is 0.21784618496894836\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 732/1792 [2:04:01<2:59:15, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 732/1792 [2:04:01<2:59:16, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 732/1792 [2:04:02<2:59:15, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 732/1792 [2:04:02<2:59:15, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 733/1792 [2:04:12<2:59:42, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 733/1792 [2:04:12<2:59:42, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 732 is completed and loss is 0.25623494386672974\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 733/1792 [2:04:12<2:59:42, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 733/1792 [2:04:12<2:59:42, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 734/1792 [2:04:22<2:59:20, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 733 is completed and loss is 0.22205795347690582\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 734/1792 [2:04:22<2:59:20, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 734/1792 [2:04:22<2:59:20, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 734/1792 [2:04:22<2:59:20, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 734 is completed and loss is 0.243743434548378\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 735/1792 [2:04:32<2:59:06, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 735/1792 [2:04:32<2:59:06, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 735/1792 [2:04:32<2:59:06, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 735/1792 [2:04:33<2:59:06, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 736/1792 [2:04:42<2:59:24, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 735 is completed and loss is 0.21259090304374695\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 736/1792 [2:04:42<2:59:24, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 736/1792 [2:04:43<2:59:24, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 736/1792 [2:04:43<2:59:24, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 737/1792 [2:04:52<2:58:54, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 736 is completed and loss is 0.2589469850063324\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 737/1792 [2:04:52<2:58:54, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 737/1792 [2:04:53<2:58:54, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 737/1792 [2:04:53<2:58:54, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 737 is completed and loss is 0.25935977697372437\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 738/1792 [2:05:02<2:58:33, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 738/1792 [2:05:02<2:58:33, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 738/1792 [2:05:03<2:58:33, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 738/1792 [2:05:03<2:58:33, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 738 is completed and loss is 0.2850915789604187\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 739/1792 [2:05:13<2:58:17, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 739/1792 [2:05:13<2:58:17, 10.16s/it]#015Training Epoch0:  41%|#033[34m████      #033[0m| 739/1792 [2:05:13<2:58:17, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████      #033[0m| 739/1792 [2:05:13<2:58:17, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 739 is completed and loss is 0.2541760206222534\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 740/1792 [2:05:23<2:58:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 740/1792 [2:05:23<2:58:01, 10.15s/it]#015Training Epoch0:  41%|#033[34m████▏     #033[0m| 740/1792 [2:05:23<2:58:01, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 740/1792 [2:05:23<2:58:02, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 740 is completed and loss is 0.26630356907844543\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 741/1792 [2:05:33<2:57:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 741/1792 [2:05:33<2:57:43, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 741/1792 [2:05:33<2:57:42, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 741/1792 [2:05:33<2:57:43, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 742/1792 [2:05:44<2:58:16, 10.19s/it]#015Training Epoch0:  41%|#033[34m████▏     #033[0m| 742/1792 [2:05:44<2:58:16, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 742/1792 [2:05:43<2:58:16, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 741 is completed and loss is 0.23778364062309265\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 742/1792 [2:05:43<2:58:16, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 743/1792 [2:05:54<2:57:51, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 742 is completed and loss is 0.2368864268064499\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 743/1792 [2:05:53<2:57:51, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 743/1792 [2:05:53<2:57:51, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  41%|#033[34m████▏     #033[0m| 743/1792 [2:05:54<2:57:51, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 744/1792 [2:06:04<2:58:16, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 743 is completed and loss is 0.21360480785369873\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 744/1792 [2:06:04<2:58:16, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 744/1792 [2:06:04<2:58:16, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 744/1792 [2:06:04<2:58:16, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 744 is completed and loss is 0.23991332948207855\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 745/1792 [2:06:14<2:58:23, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 745/1792 [2:06:14<2:58:23, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 745/1792 [2:06:14<2:58:23, 10.22s/it]#015Training Epoch0:  42%|#033[34m████▏     #033[0m| 745/1792 [2:06:14<2:58:23, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 746/1792 [2:06:25<2:57:46, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 745 is completed and loss is 0.20339816808700562\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 746/1792 [2:06:24<2:57:46, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 746/1792 [2:06:25<2:57:46, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 746/1792 [2:06:24<2:57:46, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 747/1792 [2:06:35<2:57:59, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 746 is completed and loss is 0.22805675864219666\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 747/1792 [2:06:34<2:57:59, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 747/1792 [2:06:35<2:57:59, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 747/1792 [2:06:34<2:57:59, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 748/1792 [2:06:45<2:57:22, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 747 is completed and loss is 0.25000157952308655\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 748/1792 [2:06:45<2:57:22, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 748/1792 [2:06:44<2:57:22, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 748/1792 [2:06:44<2:57:22, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 748 is completed and loss is 0.22606775164604187\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 749/1792 [2:06:55<2:57:05, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 749/1792 [2:06:55<2:57:05, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 749/1792 [2:06:55<2:57:05, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 749/1792 [2:06:54<2:57:06, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 750/1792 [2:07:05<2:56:49, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 750/1792 [2:07:05<2:56:49, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 749 is completed and loss is 0.2518027722835541\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 750/1792 [2:07:05<2:56:50, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 750/1792 [2:07:05<2:56:50, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 750 is completed and loss is 0.21776705980300903\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 751/1792 [2:07:15<2:56:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 751/1792 [2:07:15<2:56:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 751/1792 [2:07:15<2:56:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 751/1792 [2:07:15<2:56:28, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 752/1792 [2:07:25<2:56:04, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 752/1792 [2:07:26<2:56:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 751 is completed and loss is 0.19385361671447754\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 752/1792 [2:07:25<2:56:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 752/1792 [2:07:26<2:56:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 753/1792 [2:07:36<2:56:37, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 753/1792 [2:07:35<2:56:37, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 752 is completed and loss is 0.26107534766197205\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 753/1792 [2:07:35<2:56:37, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 753/1792 [2:07:36<2:56:37, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 754/1792 [2:07:46<2:56:10, 10.18s/it]#015Training Epoch0:  42%|#033[34m████▏     #033[0m| 754/1792 [2:07:46<2:56:10, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 753 is completed and loss is 0.23521685600280762\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 754/1792 [2:07:45<2:56:10, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 754/1792 [2:07:45<2:56:10, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 754 is completed and loss is 0.22312845289707184\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 755/1792 [2:07:56<2:56:30, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 755/1792 [2:07:56<2:56:30, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 755/1792 [2:07:56<2:56:30, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 755/1792 [2:07:56<2:56:30, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 755 is completed and loss is 0.23426249623298645\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 756/1792 [2:08:06<2:56:35, 10.23s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 756/1792 [2:08:06<2:56:35, 10.23s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 756/1792 [2:08:07<2:56:35, 10.23s/it]#015Training Epoch0:  42%|#033[34m████▏     #033[0m| 756/1792 [2:08:07<2:56:35, 10.23s/it]\u001b[0m\n",
      "\u001b[34mstep 756 is completed and loss is 0.2578037679195404\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 757/1792 [2:08:16<2:55:59, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 757/1792 [2:08:17<2:55:59, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 757/1792 [2:08:17<2:55:59, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 757/1792 [2:08:16<2:55:59, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 758/1792 [2:08:26<2:56:10, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 758/1792 [2:08:27<2:56:11, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 757 is completed and loss is 0.21576771140098572\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 758/1792 [2:08:26<2:56:11, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 758/1792 [2:08:27<2:56:11, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 758 is completed and loss is 0.22854986786842346\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 759/1792 [2:08:37<2:55:31, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 759/1792 [2:08:37<2:55:31, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 759/1792 [2:08:37<2:55:31, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 759/1792 [2:08:36<2:55:31, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 760/1792 [2:08:47<2:55:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 760/1792 [2:08:47<2:55:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 760/1792 [2:08:47<2:55:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 759 is completed and loss is 0.23582051694393158\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 760/1792 [2:08:47<2:55:03, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 761/1792 [2:08:57<2:54:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 760 is completed and loss is 0.21451088786125183\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 761/1792 [2:08:57<2:54:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 761/1792 [2:08:57<2:54:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  42%|#033[34m████▏     #033[0m| 761/1792 [2:08:57<2:54:42, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 761 is completed and loss is 0.2394481897354126\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 762/1792 [2:09:07<2:54:25, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 762/1792 [2:09:08<2:54:25, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 762/1792 [2:09:08<2:54:26, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 762/1792 [2:09:07<2:54:26, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 763/1792 [2:09:18<2:54:04, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 762 is completed and loss is 0.28072404861450195\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 763/1792 [2:09:17<2:54:04, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 763/1792 [2:09:17<2:54:04, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 763/1792 [2:09:18<2:54:04, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 764/1792 [2:09:28<2:54:35, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 763 is completed and loss is 0.2439400851726532\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 764/1792 [2:09:28<2:54:35, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 764/1792 [2:09:27<2:54:35, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 764/1792 [2:09:27<2:54:36, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 765/1792 [2:09:38<2:54:11, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 764 is completed and loss is 0.24935834109783173\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 765/1792 [2:09:38<2:54:11, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 765/1792 [2:09:38<2:54:12, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 765/1792 [2:09:37<2:54:11, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 766/1792 [2:09:48<2:53:58, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 766/1792 [2:09:48<2:53:58, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 765 is completed and loss is 0.22778749465942383\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 766/1792 [2:09:48<2:53:58, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 766/1792 [2:09:48<2:53:58, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 767/1792 [2:09:59<2:54:18, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 767/1792 [2:09:59<2:54:18, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 766 is completed and loss is 0.26934292912483215\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 767/1792 [2:09:58<2:54:18, 10.20s/it]#015Training Epoch0:  43%|#033[34m████▎     #033[0m| 767/1792 [2:09:58<2:54:18, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 767 is completed and loss is 0.22014881670475006\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 768/1792 [2:10:09<2:53:43, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 768/1792 [2:10:08<2:53:43, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 768/1792 [2:10:09<2:53:44, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 768/1792 [2:10:08<2:53:44, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 769/1792 [2:10:19<2:53:27, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 768 is completed and loss is 0.23611243069171906\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 769/1792 [2:10:19<2:53:27, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 769/1792 [2:10:18<2:53:27, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 769/1792 [2:10:18<2:53:27, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 769 is completed and loss is 0.19216126203536987\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 770/1792 [2:10:28<2:53:05, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 770/1792 [2:10:29<2:53:06, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 770/1792 [2:10:29<2:53:06, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 770/1792 [2:10:28<2:53:06, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 770 is completed and loss is 0.2702130377292633\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 771/1792 [2:10:39<2:52:49, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 771/1792 [2:10:39<2:52:49, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 771/1792 [2:10:39<2:52:49, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 771/1792 [2:10:38<2:52:49, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 771 is completed and loss is 0.2299976646900177\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 772/1792 [2:10:49<2:52:33, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 772/1792 [2:10:49<2:52:33, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 772/1792 [2:10:49<2:52:33, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 772/1792 [2:10:49<2:52:33, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 772 is completed and loss is 0.26225745677948\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 773/1792 [2:10:59<2:52:21, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 773/1792 [2:10:59<2:52:21, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 773/1792 [2:10:59<2:52:21, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 773/1792 [2:10:59<2:52:21, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 773 is completed and loss is 0.26962581276893616\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 774/1792 [2:11:10<2:52:10, 10.15s/it]#015Training Epoch0:  43%|#033[34m████▎     #033[0m| 774/1792 [2:11:09<2:52:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 774/1792 [2:11:10<2:52:10, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 774/1792 [2:11:09<2:52:11, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 775/1792 [2:11:20<2:52:01, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 775/1792 [2:11:20<2:52:01, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 774 is completed and loss is 0.23776619136333466\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 775/1792 [2:11:19<2:52:01, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 775/1792 [2:11:19<2:52:01, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 775 is completed and loss is 0.23771031200885773\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 776/1792 [2:11:29<2:52:25, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 776/1792 [2:11:30<2:52:25, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 776/1792 [2:11:30<2:52:25, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 776/1792 [2:11:29<2:52:25, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 776 is completed and loss is 0.22814132273197174\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 777/1792 [2:11:40<2:52:02, 10.17s/it]#015Training Epoch0:  43%|#033[34m████▎     #033[0m| 777/1792 [2:11:40<2:52:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 777/1792 [2:11:40<2:52:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 777/1792 [2:11:39<2:52:02, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 777 is completed and loss is 0.19969595968723297\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 778/1792 [2:11:50<2:51:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 778/1792 [2:11:50<2:51:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 778/1792 [2:11:50<2:51:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 778/1792 [2:11:50<2:51:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 779/1792 [2:12:01<2:52:14, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 779/1792 [2:12:01<2:52:14, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 778 is completed and loss is 0.24829638004302979\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 779/1792 [2:12:00<2:52:14, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  43%|#033[34m████▎     #033[0m| 779/1792 [2:12:00<2:52:14, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 779 is completed and loss is 0.2650901973247528\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 780/1792 [2:12:10<2:51:42, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 780/1792 [2:12:10<2:51:43, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 780/1792 [2:12:11<2:51:43, 10.18s/it]#015Training Epoch0:  44%|#033[34m████▎     #033[0m| 780/1792 [2:12:11<2:51:43, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 780 is completed and loss is 0.2408292144536972\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 781/1792 [2:12:20<2:51:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 781/1792 [2:12:20<2:51:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 781/1792 [2:12:21<2:51:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 781/1792 [2:12:21<2:51:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 782/1792 [2:12:31<2:50:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 781 is completed and loss is 0.27270540595054626\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 782/1792 [2:12:31<2:50:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 782/1792 [2:12:30<2:50:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 782/1792 [2:12:30<2:50:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 783/1792 [2:12:41<2:50:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 782 is completed and loss is 0.23923473060131073\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 783/1792 [2:12:41<2:50:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 783/1792 [2:12:40<2:50:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▎     #033[0m| 783/1792 [2:12:41<2:50:45, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 784/1792 [2:12:51<2:50:25, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 783 is completed and loss is 0.25480085611343384\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 784/1792 [2:12:51<2:50:25, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 784/1792 [2:12:51<2:50:25, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 784/1792 [2:12:51<2:50:25, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 785/1792 [2:13:01<2:50:49, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 784 is completed and loss is 0.24326159060001373\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 785/1792 [2:13:01<2:50:49, 10.18s/it]#015Training Epoch0:  44%|#033[34m████▍     #033[0m| 785/1792 [2:13:01<2:50:49, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 785/1792 [2:13:01<2:50:49, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 786/1792 [2:13:12<2:50:30, 10.17s/it]#015Training Epoch0:  44%|#033[34m████▍     #033[0m| 786/1792 [2:13:12<2:50:30, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 785 is completed and loss is 0.24627552926540375\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 786/1792 [2:13:11<2:50:30, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 786/1792 [2:13:11<2:50:30, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 786 is completed and loss is 0.2407321184873581\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 787/1792 [2:13:22<2:50:49, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 787/1792 [2:13:21<2:50:49, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 787/1792 [2:13:21<2:50:49, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 787/1792 [2:13:22<2:50:49, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 787 is completed and loss is 0.21781150996685028\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 788/1792 [2:13:32<2:50:58, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 788/1792 [2:13:32<2:50:59, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 788/1792 [2:13:32<2:50:59, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 788/1792 [2:13:31<2:50:59, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 788 is completed and loss is 0.22359812259674072\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 789/1792 [2:13:42<2:50:21, 10.19s/it]#015Training Epoch0:  44%|#033[34m████▍     #033[0m| 789/1792 [2:13:42<2:50:21, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 789/1792 [2:13:42<2:50:21, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 789/1792 [2:13:42<2:50:21, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 789 is completed and loss is 0.20866402983665466\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 790/1792 [2:13:52<2:50:38, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 790/1792 [2:13:53<2:50:38, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 790/1792 [2:13:53<2:50:38, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 790/1792 [2:13:52<2:50:38, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 790 is completed and loss is 0.21881037950515747\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 791/1792 [2:14:02<2:50:00, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 791/1792 [2:14:03<2:50:00, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 791/1792 [2:14:03<2:50:00, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 791/1792 [2:14:02<2:50:00, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 791 is completed and loss is 0.23683185875415802\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 792/1792 [2:14:12<2:49:38, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 792/1792 [2:14:13<2:49:38, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 792/1792 [2:14:13<2:49:38, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 792/1792 [2:14:12<2:49:38, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 793/1792 [2:14:23<2:49:16, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 792 is completed and loss is 0.2440139353275299\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 793/1792 [2:14:22<2:49:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 793/1792 [2:14:23<2:49:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 793/1792 [2:14:22<2:49:17, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 793 is completed and loss is 0.249676913022995\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 794/1792 [2:14:33<2:49:01, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 794/1792 [2:14:33<2:49:01, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 794/1792 [2:14:33<2:49:01, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 794/1792 [2:14:32<2:49:01, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 794 is completed and loss is 0.21388724446296692\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 795/1792 [2:14:43<2:48:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 795/1792 [2:14:43<2:48:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 795/1792 [2:14:43<2:48:48, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 795/1792 [2:14:43<2:48:49, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 795 is completed and loss is 0.21429754793643951\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 796/1792 [2:14:53<2:49:17, 10.20s/it]#015Training Epoch0:  44%|#033[34m████▍     #033[0m| 796/1792 [2:14:54<2:49:17, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 796/1792 [2:14:54<2:49:17, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 796/1792 [2:14:53<2:49:17, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 796 is completed and loss is 0.21037988364696503\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 797/1792 [2:15:03<2:48:51, 10.18s/it]#015Training Epoch0:  44%|#033[34m████▍     #033[0m| 797/1792 [2:15:04<2:48:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 797/1792 [2:15:04<2:48:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  44%|#033[34m████▍     #033[0m| 797/1792 [2:15:03<2:48:51, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 797 is completed and loss is 0.23584608733654022\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 798/1792 [2:15:14<2:49:04, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 798/1792 [2:15:13<2:49:04, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 798/1792 [2:15:14<2:49:04, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 798/1792 [2:15:13<2:49:05, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 798 is completed and loss is 0.2428787648677826\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 799/1792 [2:15:24<2:49:11, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 799/1792 [2:15:24<2:49:11, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 799/1792 [2:15:24<2:49:11, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 799/1792 [2:15:24<2:49:11, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 799 is completed and loss is 0.2308092564344406\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 800/1792 [2:15:34<2:48:37, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 800/1792 [2:15:34<2:48:36, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 800/1792 [2:15:34<2:48:37, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 800/1792 [2:15:34<2:48:37, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 800 is completed and loss is 0.24288047850131989\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 801/1792 [2:15:44<2:48:46, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 801/1792 [2:15:45<2:48:46, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 801/1792 [2:15:45<2:48:46, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 801/1792 [2:15:44<2:48:46, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 801 is completed and loss is 0.22576862573623657\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 802/1792 [2:15:54<2:48:14, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 802/1792 [2:15:55<2:48:14, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 802/1792 [2:15:55<2:48:14, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 802/1792 [2:15:54<2:48:14, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 802 is completed and loss is 0.2699541449546814\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 803/1792 [2:16:04<2:47:46, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 803/1792 [2:16:05<2:47:46, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 803/1792 [2:16:05<2:47:46, 10.18s/it]#015Training Epoch0:  45%|#033[34m████▍     #033[0m| 803/1792 [2:16:04<2:47:46, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 803 is completed and loss is 0.2671463191509247\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 804/1792 [2:16:14<2:47:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 804/1792 [2:16:15<2:47:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 804/1792 [2:16:15<2:47:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 804/1792 [2:16:14<2:47:23, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 804 is completed and loss is 0.28755617141723633\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 805/1792 [2:16:25<2:47:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 805/1792 [2:16:25<2:47:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 805/1792 [2:16:25<2:47:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 805/1792 [2:16:25<2:47:08, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 805 is completed and loss is 0.2391081005334854\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 806/1792 [2:16:35<2:46:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 806/1792 [2:16:35<2:46:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 806/1792 [2:16:35<2:46:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▍     #033[0m| 806/1792 [2:16:35<2:46:58, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 806 is completed and loss is 0.22830742597579956\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 807/1792 [2:16:45<2:47:21, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 807/1792 [2:16:46<2:47:21, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 807/1792 [2:16:46<2:47:21, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 807/1792 [2:16:45<2:47:21, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 807 is completed and loss is 0.26253679394721985\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 808/1792 [2:16:55<2:46:57, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 808/1792 [2:16:55<2:46:57, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 808/1792 [2:16:56<2:46:57, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 808/1792 [2:16:56<2:46:57, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 808 is completed and loss is 0.19700706005096436\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 809/1792 [2:17:05<2:46:44, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 809/1792 [2:17:06<2:46:44, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 809/1792 [2:17:06<2:46:44, 10.18s/it]#015Training Epoch0:  45%|#033[34m████▌     #033[0m| 809/1792 [2:17:05<2:46:44, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 809 is completed and loss is 0.24574853479862213\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 810/1792 [2:17:16<2:47:00, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 810/1792 [2:17:16<2:47:00, 10.20s/it]#015Training Epoch0:  45%|#033[34m████▌     #033[0m| 810/1792 [2:17:16<2:47:00, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 810/1792 [2:17:16<2:47:00, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 810 is completed and loss is 0.18777555227279663\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 811/1792 [2:17:26<2:46:30, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 811/1792 [2:17:26<2:46:30, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 811/1792 [2:17:26<2:46:30, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 811/1792 [2:17:26<2:46:30, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 811 is completed and loss is 0.25417351722717285\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 812/1792 [2:17:36<2:46:15, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 812/1792 [2:17:37<2:46:14, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 812/1792 [2:17:36<2:46:14, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 812/1792 [2:17:36<2:46:14, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 812 is completed and loss is 0.23574991524219513\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 813/1792 [2:17:47<2:45:52, 10.17s/it]#015Training Epoch0:  45%|#033[34m████▌     #033[0m| 813/1792 [2:17:46<2:45:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 813/1792 [2:17:47<2:45:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 813/1792 [2:17:46<2:45:53, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 813 is completed and loss is 0.26755350828170776\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 814/1792 [2:17:56<2:45:34, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 814/1792 [2:17:57<2:45:34, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 814/1792 [2:17:57<2:45:34, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 814/1792 [2:17:56<2:45:34, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 814 is completed and loss is 0.20858432352542877\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 815/1792 [2:18:06<2:45:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 815/1792 [2:18:07<2:45:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 815/1792 [2:18:07<2:45:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  45%|#033[34m████▌     #033[0m| 815/1792 [2:18:06<2:45:17, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 815 is completed and loss is 0.21490734815597534\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 816/1792 [2:18:17<2:44:59, 10.14s/it]#015Training Epoch0:  46%|#033[34m████▌     #033[0m| 816/1792 [2:18:17<2:44:59, 10.14s/it]#015Training Epoch0:  46%|#033[34m████▌     #033[0m| 816/1792 [2:18:16<2:44:59, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 816/1792 [2:18:17<2:44:59, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 817/1792 [2:18:27<2:44:44, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 816 is completed and loss is 0.25770649313926697\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 817/1792 [2:18:27<2:44:44, 10.14s/it]#015Training Epoch0:  46%|#033[34m████▌     #033[0m| 817/1792 [2:18:27<2:44:45, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 817/1792 [2:18:27<2:44:44, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 817 is completed and loss is 0.2097528725862503\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 818/1792 [2:18:37<2:44:32, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 818/1792 [2:18:37<2:44:32, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 818/1792 [2:18:37<2:44:32, 10.14s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 818/1792 [2:18:37<2:44:32, 10.14s/it]\u001b[0m\n",
      "\u001b[34mstep 818 is completed and loss is 0.20359939336776733\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 819/1792 [2:18:48<2:44:57, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 819/1792 [2:18:47<2:44:57, 10.17s/it]#015Training Epoch0:  46%|#033[34m████▌     #033[0m| 819/1792 [2:18:47<2:44:58, 10.17s/it]#015Training Epoch0:  46%|#033[34m████▌     #033[0m| 819/1792 [2:18:48<2:44:57, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 819 is completed and loss is 0.24040038883686066\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 820/1792 [2:18:57<2:44:38, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 820/1792 [2:18:58<2:44:38, 10.16s/it]#015Training Epoch0:  46%|#033[34m████▌     #033[0m| 820/1792 [2:18:58<2:44:38, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 820/1792 [2:18:57<2:44:38, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 820 is completed and loss is 0.2571205794811249\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 821/1792 [2:19:07<2:44:25, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 821/1792 [2:19:08<2:44:25, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 821/1792 [2:19:08<2:44:25, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 821/1792 [2:19:07<2:44:25, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 821 is completed and loss is 0.21019142866134644\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 822/1792 [2:19:18<2:44:44, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 822/1792 [2:19:18<2:44:44, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 822/1792 [2:19:18<2:44:44, 10.19s/it]#015Training Epoch0:  46%|#033[34m████▌     #033[0m| 822/1792 [2:19:17<2:44:44, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 823/1792 [2:19:28<2:44:22, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 822 is completed and loss is 0.2652086317539215\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 823/1792 [2:19:28<2:44:22, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 823/1792 [2:19:28<2:44:23, 10.18s/it]#015Training Epoch0:  46%|#033[34m████▌     #033[0m| 823/1792 [2:19:28<2:44:22, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 824/1792 [2:19:38<2:44:01, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 823 is completed and loss is 0.2390391081571579\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 824/1792 [2:19:38<2:44:01, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 824/1792 [2:19:38<2:44:01, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 824/1792 [2:19:38<2:44:01, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 824 is completed and loss is 0.25585782527923584\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 825/1792 [2:19:48<2:43:42, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 825/1792 [2:19:48<2:43:42, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 825/1792 [2:19:49<2:43:42, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 825/1792 [2:19:49<2:43:42, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 825 is completed and loss is 0.2442483901977539\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 826/1792 [2:19:59<2:43:25, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 826/1792 [2:19:58<2:43:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 826/1792 [2:19:59<2:43:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 826/1792 [2:19:58<2:43:26, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 827/1792 [2:20:09<2:43:11, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 826 is completed and loss is 0.20276442170143127\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 827/1792 [2:20:08<2:43:11, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 827/1792 [2:20:09<2:43:11, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 827/1792 [2:20:08<2:43:11, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 828/1792 [2:20:19<2:43:33, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 828/1792 [2:20:19<2:43:33, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 827 is completed and loss is 0.2318917214870453\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 828/1792 [2:20:19<2:43:33, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▌     #033[0m| 828/1792 [2:20:18<2:43:33, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 829/1792 [2:20:29<2:43:06, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 828 is completed and loss is 0.20648540556430817\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 829/1792 [2:20:29<2:43:06, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 829/1792 [2:20:29<2:43:06, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 829/1792 [2:20:29<2:43:07, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 830/1792 [2:20:39<2:43:29, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 830/1792 [2:20:39<2:43:29, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 829 is completed and loss is 0.25736790895462036\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 830/1792 [2:20:39<2:43:29, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 830/1792 [2:20:39<2:43:29, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 831/1792 [2:20:50<2:43:36, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 831/1792 [2:20:50<2:43:36, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 830 is completed and loss is 0.23309004306793213\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 831/1792 [2:20:49<2:43:36, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 831/1792 [2:20:49<2:43:36, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 832/1792 [2:21:00<2:43:01, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 832/1792 [2:21:00<2:43:01, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 831 is completed and loss is 0.20807933807373047\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 832/1792 [2:20:59<2:43:01, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 832/1792 [2:20:59<2:43:01, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 833/1792 [2:21:10<2:43:17, 10.22s/it]\u001b[0m\n",
      "\u001b[34mstep 832 is completed and loss is 0.21188771724700928\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 833/1792 [2:21:10<2:43:17, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 833/1792 [2:21:09<2:43:17, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  46%|#033[34m████▋     #033[0m| 833/1792 [2:21:10<2:43:17, 10.22s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 834/1792 [2:21:20<2:42:43, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 833 is completed and loss is 0.19435246288776398\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 834/1792 [2:21:20<2:42:43, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 834/1792 [2:21:20<2:42:43, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 834/1792 [2:21:20<2:42:43, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 835/1792 [2:21:30<2:42:18, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 835/1792 [2:21:30<2:42:18, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 834 is completed and loss is 0.23707568645477295\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 835/1792 [2:21:30<2:42:18, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 835/1792 [2:21:30<2:42:18, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 836/1792 [2:21:41<2:41:55, 10.16s/it]#015Training Epoch0:  47%|#033[34m████▋     #033[0m| 836/1792 [2:21:41<2:41:55, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 835 is completed and loss is 0.2726067900657654\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 836/1792 [2:21:40<2:41:55, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 836/1792 [2:21:40<2:41:55, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 837/1792 [2:21:51<2:41:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 836 is completed and loss is 0.23653703927993774\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 837/1792 [2:21:50<2:41:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 837/1792 [2:21:51<2:41:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 837/1792 [2:21:50<2:41:37, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 838/1792 [2:22:01<2:41:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 837 is completed and loss is 0.24614039063453674\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 838/1792 [2:22:00<2:41:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 838/1792 [2:22:01<2:41:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 838/1792 [2:22:00<2:41:23, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 838 is completed and loss is 0.22508545219898224\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 839/1792 [2:22:11<2:41:45, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 839/1792 [2:22:10<2:41:45, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 839/1792 [2:22:11<2:41:46, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 839/1792 [2:22:11<2:41:46, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 840/1792 [2:22:21<2:41:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 839 is completed and loss is 0.24094396829605103\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 840/1792 [2:22:21<2:41:22, 10.17s/it]#015Training Epoch0:  47%|#033[34m████▋     #033[0m| 840/1792 [2:22:21<2:41:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 840/1792 [2:22:21<2:41:22, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 841/1792 [2:22:32<2:41:48, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 841/1792 [2:22:32<2:41:47, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 841/1792 [2:22:31<2:41:47, 10.21s/it]\u001b[0m\n",
      "\u001b[34mstep 840 is completed and loss is 0.21983692049980164\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 841/1792 [2:22:31<2:41:48, 10.21s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 842/1792 [2:22:41<2:41:16, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 842/1792 [2:22:42<2:41:16, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 841 is completed and loss is 0.24526219069957733\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 842/1792 [2:22:41<2:41:16, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 842/1792 [2:22:42<2:41:16, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 843/1792 [2:22:51<2:40:54, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 843/1792 [2:22:52<2:40:54, 10.17s/it]\u001b[0m\n",
      "\u001b[34mstep 842 is completed and loss is 0.23291023075580597\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 843/1792 [2:22:51<2:40:55, 10.17s/it]#015Training Epoch0:  47%|#033[34m████▋     #033[0m| 843/1792 [2:22:52<2:40:55, 10.17s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 844/1792 [2:23:01<2:41:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mstep 843 is completed and loss is 0.27670741081237793\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 844/1792 [2:23:02<2:41:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 844/1792 [2:23:02<2:41:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 844/1792 [2:23:02<2:41:11, 10.20s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 845/1792 [2:23:12<2:40:53, 10.19s/it]\u001b[0m\n",
      "\u001b[34mstep 844 is completed and loss is 0.21617060899734497\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 845/1792 [2:23:12<2:40:53, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 845/1792 [2:23:12<2:40:53, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 845/1792 [2:23:12<2:40:53, 10.19s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 846/1792 [2:23:22<2:40:27, 10.18s/it]\u001b[0m\n",
      "\u001b[34mstep 845 is completed and loss is 0.2307630032300949\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 846/1792 [2:23:22<2:40:27, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 846/1792 [2:23:22<2:40:27, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 846/1792 [2:23:22<2:40:27, 10.18s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 847/1792 [2:23:32<2:40:00, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 847/1792 [2:23:32<2:40:00, 10.16s/it]\u001b[0m\n",
      "\u001b[34mstep 846 is completed and loss is 0.2677447497844696\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 847/1792 [2:23:32<2:40:00, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 847/1792 [2:23:33<2:40:00, 10.16s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 848/1792 [2:23:42<2:39:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 848/1792 [2:23:43<2:39:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mstep 847 is completed and loss is 0.20279023051261902\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 848/1792 [2:23:42<2:39:39, 10.15s/it]\u001b[0m\n",
      "\u001b[34mTraining Epoch0:  47%|#033[34m████▋     #033[0m| 848/1792 [2:23:43<2:39:40, 10.15s/it]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},  # Please change this to {\"accept_eula\": \"true\"}\n",
    "    disable_output_compression=True,\n",
    "    instance_type=\"ml.g5.24xlarge\"\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=True, epoch=\"3\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_test_data_location})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "ce09e6a2-6353-4f12-8a07-ba9d49b2e2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S3DataSource': {'S3Uri': 's3://sagemaker-us-east-1-769977401909/meta-textgeneration-llama-3-1-8b-2024-10-23-23-03-38-240/output/model/',\n",
       "  'S3DataType': 'S3Prefix',\n",
       "  'CompressionType': 'None'}}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e709d760-2285-4991-8b6b-5f8264d53912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'rivchess_model_src' (dict)\n"
     ]
    }
   ],
   "source": [
    "rivchess_model_src = {\"s3DataSource\": {\"s3Uri\": f'{ estimator.model_data[\"S3DataSource\"][\"S3Uri\"] }'}}\n",
    "%store rivchess_model_src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee68a9e7-9d0b-4cd3-a5d4-6c8774b1f186",
   "metadata": {},
   "source": [
    "### Import the fine tuned model into Bedrock as Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "09fd2cc9-3e6d-4e2f-a30a-480e648be36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.35.46\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "742d221f-9999-47f3-8c1f-0d76bdac0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "br_client = boto3.client('bedrock', region_name='us-east-1')\n",
    "br_run_client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "model_split = model_id.split(\"/\")\n",
    "if len(model_split) > 1:\n",
    "    rivchess_model_nm = f\"RIVCHESS-{model_split[1]}\"\n",
    "else:\n",
    "    rivchess_model_nm = f\"RIVCHESS-{model_split[0]}\"\n",
    "\n",
    "rivchess_imp_jb_nm = f\"{rivchess_model_nm}-job-{datetime.datetime.now().strftime('%Y%m%d%M%H%S')}\"\n",
    "role_arn = role\n",
    "\n",
    "create_model_import_job_resp = br_client.create_model_import_job(jobName=rivchess_imp_jb_nm,\n",
    "                                  importedModelName=rivchess_model_nm,\n",
    "                                  roleArn=role_arn,\n",
    "                                  modelDataSource=rivchess_model_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f4544ef6-da52-4f89-8e3e-99bb884d8827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BR CMI Import Job - arn:aws:bedrock:us-east-1:769977401909:model-import-job/7iab154rn07o is - InProgress\n",
      "..............................\r"
     ]
    }
   ],
   "source": [
    "list_model_import_jobs_response = br_client.list_model_import_jobs(\n",
    "    nameContains=rivchess_imp_jb_nm)\n",
    "\n",
    "print(f\"BR CMI Import Job - {create_model_import_job_resp['jobArn']} is - {list_model_import_jobs_response['modelImportJobSummaries'][0]['status']}\")\n",
    "while list_model_import_jobs_response['modelImportJobSummaries'][0]['status'] != 'Completed':\n",
    "    interactive_sleep(30)\n",
    "    list_model_import_jobs_response = br_client.list_model_import_jobs(nameContains=rivchess_imp_jb_nm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20476bbc-9b2b-4df4-8016-d0d2527287c0",
   "metadata": {},
   "source": [
    "### Invoke the imported model using Bedrock API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ea5778e6-d1e0-4d2d-9684-06bc410bfb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    get_imported_model_response = br_client.get_imported_model(\n",
    "        modelIdentifier=rivchess_model_nm\n",
    "    )\n",
    "\n",
    "    br_model_id = get_imported_model_response['modelArn']\n",
    "    br_model_id\n",
    "except br_client.exceptions.ResourceNotFoundException:\n",
    "    print(\"Model not yet imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "35f1812b-7ccd-405b-8e00-676ae4b80db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_invoke_model_and_print(native_request):\n",
    "    request = json.dumps(native_request)\n",
    "\n",
    "    try:\n",
    "        # Invoke the model with the request.\n",
    "        response = br_run_client.invoke_model(modelId=br_model_id, body=request)\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "        # print(f\"model_response: {model_response}\")\n",
    "        response_text = model_response['generation'].replace(\"\\n\", \"\").replace(\"### Response:\", \"\")\n",
    "        return response_text\n",
    "    except (ClientError, Exception) as e:\n",
    "        print(f\"ERROR: Can't invoke '{br_model_id}'. Reason: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "9f9790b5-1d6a-49d8-ae59-03fd9d90382f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is your next move in UCI notation. Provide only the UCI notation for your next move. Your next move should be a valid legal move only for the color you are playing.\\n\\n### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template[\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b0a103cd-4a69-4307-9fb4-46e1529ebeba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' e6f4'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instruction = '''In the FEN Lowercase letters describe the black pieces. \"p\" stands for pawn, \"r\" for rook, \"n\" for knight, \"b\" for bishop, \"q\" for queen, and \"k\" for king.\n",
    "The same letters are used for the white pieces, but they appear in uppercase.\n",
    "Empty squares are denoted by numbers from one to eight, depending on how many empty squares are between two pieces.\n",
    "Use the FEN to understand the position of the pieces on the chessboard and recommend legal moves accordingly and follow the rules of playing chess to recommend legal moves.'''\n",
    "\n",
    "move_color = \"BLACK\"\n",
    "board_fen = \"6k1/p1q1prbp/b3n1p1/2pPPp2/5P1Q/4BN2/Pr2N1PP/R1R4K b - - 0 21\"\n",
    "\n",
    "context = f\"You are a chess grandmaster. You are playing {move_color} color and the current chessboard FEN is {board_fen}.\"\n",
    "\n",
    "formatted_prompt = template[\"prompt\"].format(instruction=instruction, \n",
    "                          context=context,\n",
    "                          answer=\"\")\n",
    "\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_tokens\": 50,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.1\n",
    "}\n",
    "\n",
    "call_invoke_model_and_print(native_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e24cc1fd-4022-4621-a192-785aa2167613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' a2a4'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "move_color = \"WHITE\"\n",
    "board_fen = \"1r1qk2r/p2bppbp/n2P1np1/1pp5/3P4/2P1BB2/PP2QPPP/RN2K1NR w KQk - 3 11\"\n",
    "\n",
    "formatted_prompt = template[\"prompt\"].format(instruction=instruction, \n",
    "                          context=context,\n",
    "                          answer=\"\")\n",
    "\n",
    "native_request = {\n",
    "    \"prompt\": formatted_prompt,\n",
    "    \"max_tokens\": 100,\n",
    "    \"top_p\": 0.9,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_k\": 50,\n",
    "}\n",
    "\n",
    "call_invoke_model_and_print(native_request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb9e06-7481-4054-9a3f-c2b2442e6d4c",
   "metadata": {},
   "source": [
    "### Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "3eb94591-000e-4b58-a47b-a5d75d3856e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_imported_model_response = br_client.delete_imported_model(\n",
    "    modelIdentifier=br_model_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48ffe14-f62b-4ec1-99cf-d867a65fd8cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
