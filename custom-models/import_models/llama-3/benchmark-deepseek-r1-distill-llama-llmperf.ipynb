{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df07d6e0-a652-4c47-8f1b-48f08ff01e9d",
   "metadata": {},
   "source": [
    "# Benchmarking Bedrock Custom Imported Models with LLMPerf and LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaff294-1095-469e-a8b5-f572e63e3e7e",
   "metadata": {},
   "source": [
    "Amazon Bedrock [custom model import](https://aws.amazon.com/bedrock/custom-model-import/) allows users to deploy their own model weights, with Bedrock automatically optimizing deployment for performance, security, and scalability. While this feature accelerates and simplified the process, it raises a critical question: *how fast is this deployment?*\n",
    "\n",
    "This notebook provides a quick way to benchmark the performance of a Bedrock-managed deployment using [LLMPerf](https://github.com/ray-project/llmperf) (a lightweight benchmarking tool for large language models) and [LiteLLM](https://github.com/BerriAI/litellm) (a universal model gateway that abstracts API calls across different providers). This process is illustrated using [deepseek-ai/DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B) as the imported model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcfdaf0-dcbf-4af1-8812-d7126474e6ea",
   "metadata": {},
   "source": [
    "**Notes**:\n",
    "\n",
    "- Use *SageMaker Distribution 1.12.10* as the image in an Amazon SageMaker Studio JupyterLab app.\n",
    "- This notebook uses [deepseek-ai/DeepSeek-R1-Distill-Llama-8B](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-8B). You will need more than 16 GB in instance memory and storage to download the model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b25eb8-6624-4c7b-ae9a-971e549cc333",
   "metadata": {},
   "source": [
    "### Installing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9bf0eeb-539e-425f-934b-5adf86d80eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.13.3 requires botocore<1.34.163,>=1.34.70, but you have botocore 1.36.25 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.3 requires sqlparse==0.5.0, but you have sqlparse 0.5.3 which is incompatible.\n",
      "autogluon-common 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-core 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-features 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-features 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires jsonschema<4.18,>=4.14, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torch<1.14,>=1.9, but you have torch 2.0.0.post104 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchmetrics<0.12.0,>=0.11.0, but you have torchmetrics 1.0.3 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires torchvision<0.15.0, but you have torchvision 0.15.2a0+ab7b3e6 which is incompatible.\n",
      "autogluon-multimodal 0.8.3 requires transformers[sentencepiece]<4.41.0,>=4.36.0, but you have transformers 4.48.1 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "autogluon-tabular 0.8.3 requires scikit-learn<1.4.1,>=1.1, but you have scikit-learn 1.4.2 which is incompatible.\n",
      "autogluon-timeseries 0.8.3 requires pandas<1.6,>=1.4.1, but you have pandas 2.1.4 which is incompatible.\n",
      "langchain-aws 0.1.18 requires boto3<1.35.0,>=1.34.131, but you have boto3 1.36.25 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers sagemaker --quiet\n",
    "!pip install boto3 huggingface huggingface_hub hf_transfer --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2df2f31-b6ca-461b-b262-e7b177b2f0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "boto_session = boto3.session.Session()\n",
    "region = boto_session.region_name\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "sagemaker_session_bucket = None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f673a6-b45f-498d-937c-beb4b347883f",
   "metadata": {},
   "source": [
    "### Downloading model artifacts from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c77b956-39ac-4d7d-88e8-f40fd6d6178c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'deepseek-r1-distill-llama-8b' already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "s3_prefix = \"deepseek-r1-distill-llama-8b\"\n",
    "if not os.path.exists(s3_prefix):\n",
    "    os.makedirs(s3_prefix)\n",
    "    print(f\"Directory '{s3_prefix}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{s3_prefix}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "469abbe3-d73a-4a90-9437-a7e02ad269f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76fbdb9647fb40bc8ba29d592fd7e6b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 11 files:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/deepseek-r1-distill-llama-8b'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "hf_model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "snapshot_download(\n",
    "    repo_id=hf_model_id,\n",
    "    local_dir=f\"./{s3_prefix}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3f54c9-5c6e-4702-bca7-2f694aad7575",
   "metadata": {},
   "source": [
    "### Uploading model artifacts to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b76e043-e7d6-45e6-a399-3716137cf61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files: 100%|██████████| 34/34 [02:04<00:00,  3.65s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def upload_directory_to_s3(local_directory: str, bucket_name: str, s3_prefix: str):\n",
    "    \"\"\"\n",
    "    Upload files from a local directory to a user-defined prefix in an Amazon S3 bucket\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "    local_directory = Path(local_directory)\n",
    "\n",
    "    all_files = []\n",
    "    # Walk the local directory\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            local_path = Path(root) / filename\n",
    "            relative_path = local_path.relative_to(local_directory)\n",
    "            s3_key = f\"{s3_prefix}/{relative_path}\"\n",
    "            all_files.append((local_path, s3_key))\n",
    "\n",
    "    # Upload to S3\n",
    "    for local_path, s3_key in tqdm(all_files, desc=\"Uploading files\"):\n",
    "        try:\n",
    "            s3_client.upload_file(str(local_path),bucket_name,s3_key)\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {local_path}: {str(e)}\")\n",
    "\n",
    "upload_directory_to_s3(s3_prefix, sess.default_bucket(), s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589ad49-cd4e-426f-a3b6-f39d0b3926cc",
   "metadata": {},
   "source": [
    "## Importing model into Amazon Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0306fda4-6a2e-483a-8a05-b037244aaddd",
   "metadata": {},
   "source": [
    "### Create IAM role for model import job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c5e708-da8a-4cfd-89fd-b42d2b2b1437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Role ARN: arn:aws:iam::447500535019:role/BedrockExecutionRole-2025-02-21-15-13-44-840\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "from botocore.exceptions import ClientError\n",
    "import logging\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def create_bedrock_execution_role(role_name, account_id, region, s3_bucket):\n",
    "    \"\"\"\n",
    "    Creates an IAM role that allows Amazon Bedrock to assume the role and that grants S3 read permissions.\n",
    "    \"\"\"\n",
    "    iam = boto3.client('iam')\n",
    "    trust_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"1\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\n",
    "                    \"Service\": \"bedrock.amazonaws.com\"\n",
    "                },\n",
    "                \"Action\": \"sts:AssumeRole\",\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"aws:SourceAccount\": account_id\n",
    "                    },\n",
    "                    \"ArnEquals\": {\n",
    "                        \"aws:SourceArn\": f\"arn:aws:bedrock:{region}:{account_id}:model-import-job/*\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    s3_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Sid\": \"S3ReadAccess\",\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"s3:GetObject\",\n",
    "                    \"s3:ListBucket\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    f\"arn:aws:s3:::{s3_bucket}\",\n",
    "                    f\"arn:aws:s3:::{s3_bucket}/*\"\n",
    "                ],\n",
    "                \"Condition\": {\n",
    "                    \"StringEquals\": {\n",
    "                        \"s3:ResourceAccount\": account_id\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = iam.create_role(\n",
    "            RoleName=role_name,\n",
    "            AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "            Description=\"Execution role for Amazon Bedrock model import jobs\"\n",
    "        )\n",
    "        policy_name = f\"{role_name}-S3ReadAccess\"\n",
    "        policy_response = iam.create_policy(\n",
    "            PolicyName=policy_name,\n",
    "            PolicyDocument=json.dumps(s3_policy),\n",
    "            Description=\"Allows S3 read access for Bedrock execution role\"\n",
    "        )\n",
    "        iam.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=policy_response['Policy']['Arn']\n",
    "        )\n",
    "        logger.info(f\"Successfully created role: {role_name}\")\n",
    "        return response['Role']\n",
    "\n",
    "    except ClientError as error:\n",
    "        logger.error(f\"Failed to create role: {error}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "bedrock_role_name = name_from_base(\"BedrockExecutionRole\")\n",
    "aws_account = boto3.client('sts').get_caller_identity()['Account']\n",
    "\n",
    "bedrock_job_import_role = create_bedrock_execution_role(bedrock_role_name, aws_account, region, sagemaker_session_bucket)\n",
    "if bedrock_job_import_role:\n",
    "    print(f\"Role ARN: {bedrock_job_import_role['Arn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c866f93-415d-4b7c-98bd-0ec039e35a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model import job created with ARN: arn:aws:bedrock:us-west-2:447500535019:model-import-job/4vo2whytwsux\n"
     ]
    }
   ],
   "source": [
    "bedrock = boto3.client('bedrock',region_name=region)\n",
    "s3_uri = f's3://{sess.default_bucket()}/{s3_prefix}/' # S3 URI that contains th model artifacts\n",
    "\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "job_name = name_from_base(s3_prefix+\"-job\")\n",
    "imported_model_name = name_from_base(s3_prefix+\"-model\")\n",
    "\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=imported_model_name,\n",
    "    roleArn=bedrock_job_import_role['Arn'],\n",
    "    modelDataSource={\n",
    "        's3DataSource': {\n",
    "            's3Uri': s3_uri\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "job_Arn = response['jobArn']\n",
    "print(f\"Model import job created with ARN: {job_Arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395fe253-4799-4ff0-afdd-b1d3a05e3e5a",
   "metadata": {},
   "source": [
    "The model import process can take ~10 min. The following cell will wait for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f940dbfd-9153-4650-bfbc-4770e7777587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: INPROGRESS\n",
      "Status: COMPLETED\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_Arn)\n",
    "    status = response['status'].upper()\n",
    "    print(f\"Status: {status}\")\n",
    "    if status in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "    time.sleep(60)\n",
    "\n",
    "model_id = response['importedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f574e7e-c9b2-4664-855c-8073eb47f804",
   "metadata": {},
   "source": [
    "Once the model has been imported, we may still have to give Bedrock a few minutes to initialize a serving container to handle our requests. If the model is not available yet, we will get an error message saying that the model is **not ready**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91dc4473-05d6-40f6-9fb8-8db4e36c54a8",
   "metadata": {},
   "source": [
    "## Testing model inference with the InvokeModel API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb67bc1-a20f-4cd6-983c-2878d183973c",
   "metadata": {},
   "source": [
    "### Configuration to work with DeepSeek models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c10444-d857-4de7-aebe-da8dcafdfef3",
   "metadata": {},
   "source": [
    "DeepSeek models expect inputs to follow a specific format defined in the `tokenizer_config.json` file. This format ensures that the model receives prompts in the same structure that it was trained on. We are going to initialize a `tokenizer` and then use it to shape our model requests to ensure that the model sees prompts with the expected structure.\n",
    "\n",
    "We are also going to edit the configuration of the Bedrock runtime client to increase the timeout and the number of retries so we can work with long wait times and the potential unavailability of the model when the serving container is still being prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e367a5f6-0967-42eb-80aa-b640aed1eae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_model_id)\n",
    "\n",
    "session = boto3.Session()\n",
    "client = session.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name=region,\n",
    "    config=Config(\n",
    "        connect_timeout=300,\n",
    "        read_timeout=300,\n",
    "        retries={'max_attempts': 3}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766c7d0-ea9c-40b2-9073-c36304cae9a1",
   "metadata": {},
   "source": [
    "The `generate` function is going to take our messages and send them to the DeepSeek model using proper tokenization and a robust retry mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac11dadc-4259-4b85-a330-fb9121d16288",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(messages, temperature=0.3, max_tokens=4096, top_p=0.9, continuation=False, max_retries=10):\n",
    "    \"\"\"\n",
    "    Generate response using the model with proper tokenization and retry mechanism\n",
    "\n",
    "    Parameters:\n",
    "        messages (list): List of message dictionaries with 'role' and 'content'\n",
    "        temperature (float): Controls randomness in generation (0.0-1.0)\n",
    "        max_tokens (int): Maximum number of tokens to generate\n",
    "        top_p (float): Nucleus sampling parameter (0.0-1.0)\n",
    "        continuation (bool): Whether this is a continuation of previous generation\n",
    "        max_retries (int): Maximum number of retry attempts\n",
    "\n",
    "    Returns:\n",
    "        dict: Model response containing generated text and metadata\n",
    "    \"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(messages,\n",
    "                                           tokenize=False,\n",
    "                                           add_generation_prompt=not continuation)\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            response = client.invoke_model(\n",
    "                modelId=model_id,\n",
    "                body=json.dumps({\n",
    "                    'prompt': prompt,\n",
    "                    'temperature': temperature,\n",
    "                    'max_gen_len': max_tokens,\n",
    "                    'top_p': top_p\n",
    "                }),\n",
    "                accept='application/json',\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            \n",
    "            result = json.loads(response['body'].read().decode('utf-8'))\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            attempt += 1\n",
    "            if attempt < max_retries:\n",
    "                time.sleep(30)\n",
    "    \n",
    "    raise Exception(\"Failed to get response after maximum retries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4328ec-4ef8-4a01-b310-e4c6841c1297",
   "metadata": {},
   "source": [
    "### Running simple test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79c1dcf5-89c2-4bc2-908e-11e14c7aaab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed: An error occurred (ModelNotReadyException) when calling the InvokeModel operation (reached max retries: 3): Model is not ready for inference. Wait and try your request again. Refer to https://docs.aws.amazon.com/bedrock/latest/userguide/invoke-imported-model.html#handle-model-not-ready-exception.\n",
      "Attempt 2 failed: An error occurred (ModelNotReadyException) when calling the InvokeModel operation (reached max retries: 3): Model is not ready for inference. Wait and try your request again. Refer to https://docs.aws.amazon.com/bedrock/latest/userguide/invoke-imported-model.html#handle-model-not-ready-exception.\n",
      "Model Response:\n",
      "First, I need to understand what an operating margin is. It's calculated by dividing the operating income by the total revenue.\n",
      "\n",
      "Next, I'll determine the operating income. Since the revenue increased from $10 million to $15 million, the operating income is the revenue minus the initial operating costs. So, $15 million minus $7 million equals an operating income of $8 million.\n",
      "\n",
      "Finally, to find the operating margin, I'll divide the operating income by the total revenue for 2023, which is $15 million. This gives me an operating margin of 53%.\n",
      "</think>\n",
      "\n",
      "To calculate the **operating margin** for Company A in 2023, follow these steps:\n",
      "\n",
      "1. **Understand the Formula:**\n",
      "   \n",
      "   The operating margin is calculated as:\n",
      "   \\[\n",
      "   \\text{Operating Margin} = \\frac{\\text{Operating Income}}{\\text{Total Revenue}}\n",
      "   \\]\n",
      "   \n",
      "2. **Identify the Given Values:**\n",
      "   \n",
      "   - **Total Revenue in 2023:** \\$15 million\n",
      "   - **Initial Operating Costs:** \\$7 million\n",
      "   - **Operating Costs Increase:** 20%\n",
      "\n",
      "3. **Calculate the New Operating Costs:**\n",
      "   \n",
      "   Since the operating costs increased by 20%, the new operating costs are:\n",
      "   \\[\n",
      "   \\text{New Operating Costs} = \\$7 \\text{ million} \\times (1 + 0.20) = \\$8.4 \\text{ million}\n",
      "   \\]\n",
      "\n",
      "4. **Calculate Operating Income:**\n",
      "   \n",
      "   Operating income is total revenue minus operating costs:\n",
      "   \\[\n",
      "   \\text{Operating Income} = \\text{Total Revenue} - \\text{New Operating Costs} = \\$15 \\text{ million} - \\$8.4 \\text{ million} = \\$6.6 \\text{ million}\n",
      "   \\]\n",
      "\n",
      "5. **Calculate the Operating Margin:**\n",
      "   \n",
      "   \\[\n",
      "   \\text{Operating Margin} = \\frac{\\$6.6 \\text{ million}}{\\$15 \\text{ million}} = 0.44 = 44\\%\n",
      "   \\]\n",
      "\n",
      "**Final Answer:**\n",
      "\\[\n",
      "\\boxed{44\\%}\n",
      "\\]\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"Given the following financial data:\n",
    "- Company A's revenue grew from $10M to $15M in 2023\n",
    "- Operating costs increased by 20%\n",
    "- Initial operating costs were $7M\n",
    "\n",
    "Calculate the company's operating margin for 2023. Please reason step by step.\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": test_prompt}]\n",
    "response = generate(messages)\n",
    "print(\"Model Response:\")\n",
    "print(response[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6367771d-b213-4ac3-bf80-140b07554e06",
   "metadata": {},
   "source": [
    "## Running benchmark with LLMPerf\n",
    "\n",
    "[LLMPerf](https://github.com/ray-project/llmperf) is a benchmarking library for large language models. A load test is going to spawn a number of concurrent requests to the LLM API and measure the intertoken latnecy and generation throughput per request and across concurrent requests.\n",
    "\n",
    "LLMPerf already supports LiteLLM, which we can use to send API requests to Amazon Bedrock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab08f66-b659-4679-b379-6b7305c49deb",
   "metadata": {},
   "source": [
    "### Install LLMPerf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3025424-4dfc-4c22-9b46-45335d276d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'llmperf'...\n",
      "remote: Enumerating objects: 162, done.\u001b[K\n",
      "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
      "remote: Compressing objects: 100% (30/30), done.\u001b[K\n",
      "remote: Total 162 (delta 25), reused 16 (delta 16), pack-reused 116 (from 2)\u001b[K\n",
      "Receiving objects: 100% (162/162), 250.49 KiB | 7.59 MiB/s, done.\n",
      "Resolving deltas: 100% (77/77), done.\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gluonts 0.13.7 requires pydantic~=1.7, but you have pydantic 2.4.2 which is incompatible.\n",
      "sagemaker 2.228.0 requires protobuf<5.0,>=3.12, but you have protobuf 5.29.3 which is incompatible.\n",
      "tensorflow 2.15.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ray-project/llmperf.git\n",
    "!cd llmperf; pip install -e . --quiet; cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3009a040-01d1-4f4c-affe-2e795bcd11ae",
   "metadata": {},
   "source": [
    "### Setting up AWS credentials"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a0db2-df1a-42fb-922b-94463d3fffa2",
   "metadata": {},
   "source": [
    "[LiteLLM](https://github.com/BerriAI/litellm) is a Python library that provides consitent input and output formats for invoking generative models from various providers. LiteLLLM can be used to invoke models on Amazon Bedrock, but requires that we configure the authorization parameters. More information available [here](https://docs.litellm.ai/docs/providers/bedrock)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "548ddbc0-f416-4c93-a9e1-858c30669747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your AWS access key ID:  ········\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "\n",
    "AWS_ACCESS_KEY_ID = getpass.getpass(\"Enter your AWS access key ID: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "898ae399-8434-409e-ac4b-40f54ae425cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your AWS secret access key:  ········\n"
     ]
    }
   ],
   "source": [
    "AWS_SECRET_ACCESS_KEY = getpass.getpass(\"Enter your AWS secret access key: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8709582-e0ab-40b3-919d-7858a0f1f765",
   "metadata": {},
   "source": [
    "### Single invocation using LiteLLM\n",
    "\n",
    "The Llama distill version of Deepseek R1 uses the Llama request/response spec. We define this part in the `model` param."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "975804ee-1263-473c-82f6-87e03309ac5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from litellm import completion\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = AWS_ACCESS_KEY_ID\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = AWS_SECRET_ACCESS_KEY\n",
    "os.environ[\"AWS_REGION_NAME\"] = region\n",
    "\n",
    "response = completion(\n",
    "    model=f\"bedrock/llama/{model_id}\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Tell me a knock-knock joke.\"},\n",
    "              {\"role\": \"assistant\", \"content\": \"\"}],\n",
    "    max_tokens=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c597f9b-0520-4c72-9f4e-099e69f7defa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Why did the scarecrow win an award? Because he was outstanding in his field.\n",
      "Okay, so I need to come up with a knock-knock joke. Hmm, I remember that knock-knock jokes usually start with a person knocking on the door, and then the punchline is a play on words or a pun. Let me think about some common themes or subjects for these jokes. There are a lot, like food, animals, professions, school subjects, etc.\n",
      "\n",
      "The user gave an example with the scarecrow, so maybe I can think of another profession or something related to that. Let's see, professions that have something to do with their work tools or actions. Maybe a baker, a fisherman, a musician, or something like that.\n",
      "\n",
      "Wait, the scarecrow joke used \"outstanding in his field,\" which is a pun on \"field\" as in both the actual field and the area of expertise. Maybe I can find another pun like that.\n",
      "\n",
      "How about a baker? If someone knocks on the door and says, \"Knock-knock, who's there?\" The answer could be \"The cookie,\" because cookies are baked, so it's like a play on \"baker.\" But that might be a bit forced. Alternatively, maybe \"The pie is on the door,\" but that doesn't make much sense.\n",
      "\n",
      "What about a fisherman? Maybe something with fishing terms. \"Knock-knock, who's there?\" \"The bait,\" because you use bait to fish. So, \"The bait is here.\" Hmm, that could work, but I'm not sure if it's as clever as the scarecrow joke.\n",
      "\n",
      "How about a musician? Maybe \"The drummer\" because drummers are often associated with knocking. So, \"The drummer wants to come in.\" That's a bit of a stretch, but it's a play on words with \"knock\" and \"drummer.\"\n",
      "\n",
      "Wait, another idea. Maybe a profession that's related to the act of knocking. A carpenter? \"Knock-knock, who's there?\" \"The woodpecker,\" because woodpeckers knock on wood. That's a good one because it's a real bird that does that. So, \"The woodpecker is here.\"\n",
      "\n",
      "Alternatively, maybe a profession that's related to something else. How about a writer? \"Knock-knock, who's there?\" \"The novel,\" because novels are written. So, \"The novel is on the door.\" That's a bit abstract, but it's a pun.\n",
      "\n",
      "Wait, another angle. Maybe something involving a tool or an object that's used for knocking. Like a hammer. But then the punchline would be \"The hammer's here,\" which is straightforward but not a pun.\n",
      "\n",
      "I think the carpenter and the woodpecker idea is better because it's a natural connection to knocking. So, the joke would be:\n",
      "\n",
      "\"Knock-knock, who's there?\"\n",
      "\n",
      "\"The woodpecker, can I come in?\"\n",
      "\n",
      "That's a solid joke because it's a real bird known for knocking on wood, which ties into the knock-knock structure.\n",
      "\n",
      "Alternatively, if I want to keep it simple and more straightforward, like the baker or the fisherman, but the woodpecker seems more fitting because it's directly related to the action of knocking.\n",
      "\n",
      "I think I'll go with the woodpecker. It makes sense and the pun is clear. So, the joke is:\n",
      "\n",
      "\"Knock-knock, who's there?\"\n",
      "\n",
      "\"The woodpecker, can I come in?\"\n",
      "\n",
      "Yeah, that works well.\n",
      "</think>\n",
      "\n",
      "**Joke:**\n",
      "\n",
      "\"Knock-knock, who's there?\"\n",
      "\n",
      "\"The woodpecker, can I come in?\"\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "This joke cleverly uses the woodpecker's natural habit of knocking on wood, which ties into the knock-knock structure. It's a playful pun that connects the action of knocking with the bird's behavior, making it both amusing and relevant.\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bddd0-ba15-4279-b5eb-8541d299ccbc",
   "metadata": {},
   "source": [
    "### Write testing script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ccbda-f60e-46d3-b43e-d1734f1bbd73",
   "metadata": {},
   "source": [
    "We are going to define a Shell script that will run the `token_benchmark_ray.py` script that is part of the `LLMPerf` library. There will be some minor modifications to use our `LiteLLM` compatible model and to pass our AWS credentials as environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac7a3dfa-94ab-44c4-a85e-0263813c3aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script written to scripts/run_benchmark.sh and made executable.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import stat\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "def write_benchmarking_script(mean_input_tokens: int,\n",
    "                              stddev_input_tokens: int,\n",
    "                              mean_output_tokens: int,\n",
    "                              stddev_output_tokens: int,\n",
    "                              num_concurrent_requests: int,\n",
    "                              num_requests_per_client: int):\n",
    "    \"\"\"\n",
    "    Write a benchmarking script that uses the token_benchmark_ray script\n",
    "    \"\"\"\n",
    "    results_dir = os.path.join(\"outputs\",\n",
    "                               name_from_base(hf_model_id)\n",
    "                              )\n",
    "    \n",
    "    script_content = f\"\"\"#!/bin/bash\n",
    "export LLM_PERF_CONCURRENT={num_concurrent_requests}\n",
    "export LLM_PERF_MAX_REQUESTS=$(expr $LLM_PERF_CONCURRENT \\* {num_requests_per_client})\n",
    "export LLM_PERF_SCRIPT_DIR=$HOME/llmperf\n",
    "\n",
    "export AWS_ACCESS_KEY_ID=\"{AWS_ACCESS_KEY_ID}\"\n",
    "export AWS_SECRET_ACCESS_KEY=\"{AWS_SECRET_ACCESS_KEY}\"\n",
    "export AWS_REGION_NAME=\"{region}\"\n",
    "\n",
    "export LLM_PERF_OUTPUT={results_dir}\n",
    "\n",
    "mkdir -p $LLM_PERF_OUTPUT\n",
    "cp \"$0\" \"${{LLM_PERF_OUTPUT}}\"/\n",
    "\n",
    "python3 ${{LLM_PERF_SCRIPT_DIR}}/token_benchmark_ray.py \\\\\n",
    "    --model \"bedrock/llama/{model_id}\" \\\\\n",
    "    --mean-input-tokens {mean_input_tokens} \\\\\n",
    "    --stddev-input-tokens {stddev_input_tokens} \\\\\n",
    "    --mean-output-tokens {mean_output_tokens} \\\\\n",
    "    --stddev-output-tokens {stddev_output_tokens} \\\\\n",
    "    --max-num-completed-requests ${{LLM_PERF_MAX_REQUESTS}} \\\\\n",
    "    --timeout 1800 \\\\\n",
    "    --num-concurrent-requests ${{LLM_PERF_CONCURRENT}} \\\\\n",
    "    --results-dir \"${{LLM_PERF_OUTPUT}}\" \\\\\n",
    "    --llm-api litellm \\\\\n",
    "    --additional-sampling-params '{{}}'\n",
    "\"\"\"\n",
    "\n",
    "    os.makedirs(\"scripts\", exist_ok=True)\n",
    "\n",
    "    script_path = \"scripts/run_benchmark.sh\"\n",
    "    with open(script_path, \"w\") as file:\n",
    "        file.write(script_content)\n",
    "\n",
    "    current_permissions = os.stat(script_path).st_mode\n",
    "    os.chmod(script_path, current_permissions | stat.S_IXUSR)\n",
    "\n",
    "    print(f\"Script written to {script_path} and made executable.\")\n",
    "\n",
    "    return results_dir\n",
    "\n",
    "results_dir = write_benchmarking_script(\n",
    "    mean_input_tokens=200,\n",
    "    stddev_input_tokens=25,\n",
    "    mean_output_tokens=200,\n",
    "    stddev_output_tokens=50,\n",
    "    num_concurrent_requests=2,\n",
    "    num_requests_per_client=50\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a338a6a9-b63c-495f-a7e5-98c0645d4410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "2025-02-21 15:29:01,150\tWARNING services.py:2063 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 4049555456 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=9.70gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n",
      "2025-02-21 15:29:02,299\tINFO worker.py:1841 -- Started a local Ray instance.\n",
      "100%|█████████████████████████████████████████| 100/100 [02:06<00:00,  1.27s/it]\n",
      "\\Results for token benchmark for bedrock/llama/arn:aws:bedrock:us-west-2:447500535019:imported-model/gzjez2fl6fqv queried with the litellm api.\n",
      "\n",
      "inter_token_latency_s\n",
      "    p25 = 0.011539844607906875\n",
      "    p50 = 0.011799892394804237\n",
      "    p75 = 0.012215049464239491\n",
      "    p90 = 0.012857840641913884\n",
      "    p95 = 0.013223844971041855\n",
      "    p99 = 0.014052271172335597\n",
      "    mean = 0.011944668978055371\n",
      "    min = 0.010633086730606976\n",
      "    max = 0.014947145426838122\n",
      "    stddev = 0.0006828104456837079\n",
      "ttft_s\n",
      "    p25 = 0.3268987215000152\n",
      "    p50 = 0.3682970859999841\n",
      "    p75 = 0.4180455057500012\n",
      "    p90 = 0.5336911679998594\n",
      "    p95 = 0.6172413196999741\n",
      "    p99 = 0.7114198719799638\n",
      "    mean = 0.3870447555800001\n",
      "    min = 0.1441436679999697\n",
      "    max = 0.895927754000013\n",
      "    stddev = 0.11664909965783593\n",
      "end_to_end_latency_s\n",
      "    p25 = 2.129780807999964\n",
      "    p50 = 2.354759672499995\n",
      "    p75 = 2.727337426750182\n",
      "    p90 = 2.957789504699986\n",
      "    p95 = 3.0286894300997456\n",
      "    p99 = 3.3317388518000106\n",
      "    mean = 2.3973647034899885\n",
      "    min = 1.3210850899999969\n",
      "    max = 3.4830668659999446\n",
      "    stddev = 0.43259106402204067\n",
      "request_output_throughput_token_per_s\n",
      "    p25 = 92.04560708202308\n",
      "    p50 = 95.47220463609025\n",
      "    p75 = 98.96451065888115\n",
      "    p90 = 101.81892716533996\n",
      "    p95 = 103.1246786822685\n",
      "    p99 = 105.73408214968124\n",
      "    mean = 94.7382745123819\n",
      "    min = 74.60494871297615\n",
      "    max = 105.93932046054711\n",
      "    stddev = 6.290004634941457\n",
      "number_input_tokens\n",
      "    p25 = 184.0\n",
      "    p50 = 200.0\n",
      "    p75 = 214.0\n",
      "    p90 = 231.20000000000002\n",
      "    p95 = 243.1\n",
      "    p99 = 269.12000000000006\n",
      "    mean = 199.06\n",
      "    min = 133\n",
      "    max = 281\n",
      "    stddev = 26.549294727074212\n",
      "number_output_tokens\n",
      "    p25 = 196.5\n",
      "    p50 = 226.0\n",
      "    p75 = 264.75\n",
      "    p90 = 283.1\n",
      "    p95 = 304.05\n",
      "    p99 = 333.07000000000005\n",
      "    mean = 228.0\n",
      "    min = 108\n",
      "    max = 340\n",
      "    stddev = 47.3004580668447\n",
      "Number Of Errored Requests: 0\n",
      "Overall Output Throughput: 179.7000987331598\n",
      "Number Of Completed Requests: 100\n",
      "Completed Requests Per Minute: 47.28949966662099\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!bash ./scripts/run_benchmark.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3987421-dd75-41af-a384-3b9ddb632e89",
   "metadata": {},
   "source": [
    "###  Analyze results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962bed7-cbc5-4ad0-915e-2565941f5dd6",
   "metadata": {},
   "source": [
    "LLMPerf will write two files in the results directory: a summary file, whose results we can already see above, and the individual responses, which we can extract for plotting and detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "027ba243-de53-47e3-af2a-c281ba139db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def load_responses_json(local_dir):\n",
    "    \"\"\"\n",
    "    Load JSON file with detailed responses from the LLMPerf test\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(local_dir):\n",
    "        raise FileNotFoundError(f\"Directory '{local_dir}' does not exist.\")\n",
    "\n",
    "    for file_name in os.listdir(local_dir):\n",
    "        if file_name.endswith(\"individual_responses.json\"):\n",
    "            file_path = os.path.join(local_dir, file_name)\n",
    "            with open(file_path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "    raise FileNotFoundError(\"No file ending with 'individual_responses.json' found in the directory.\")\n",
    "\n",
    "try:\n",
    "    individual_responses = load_responses_json(results_dir)\n",
    "except FileNotFoundError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca5e3ce3-4022-47e6-847f-d193a886317f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ttft_values = [d[\"ttft_s\"] for d in individual_responses if \"ttft_s\" in d]\n",
    "throughput_values = [d[\"request_output_throughput_token_per_s\"] for d in individual_responses if \"request_output_throughput_token_per_s\" in d]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].hist(ttft_values, bins=50, edgecolor=\"black\")\n",
    "axes[0].set_title(\"Time to first token (s)\")\n",
    "axes[1].hist(throughput_values, bins=50, edgecolor=\"black\")\n",
    "axes[1].set_title(\"Request throughput (tokens/s)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc0dad0-e704-496b-934f-e7795d5be9c7",
   "metadata": {},
   "source": [
    "Feel free to edit the arguments of the `write_benchmarking_script` function to re-run the analysis for your specific scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76dc1d9-5f34-4e13-8864-5939b4c9fe9a",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c64052fb-cfa8-4b7e-a2f3-55df3a2b124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = boto3.client('bedrock', region_name=region)\n",
    "response = bedrock.delete_imported_model(\n",
    "    modelIdentifier=imported_model_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eeb654-818a-46c7-8961-0a716e7d48a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
