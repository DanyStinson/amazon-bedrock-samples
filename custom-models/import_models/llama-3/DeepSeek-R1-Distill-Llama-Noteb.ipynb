{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170cd66c-9f7f-4db9-9673-31b4d7e9e1fe",
   "metadata": {},
   "source": [
    "# Import DeepSeek-R1-Distill-Llama Models to Amazon Bedrock\n",
    "\n",
    "This notebook demonstrates how to import DeepSeek's distilled Llama models to Amazon Bedrock using Custom Model Import (CMI) feature. We'll use the 8B parameter model as an example, <u>but the same process applies to the 70B variant</u>.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "DeepSeek has released several distilled versions of their models based on Llama architecture. These models maintain strong performance while being more efficient than their larger counterparts. The 8B model we'll use here is derived from Llama 3.1 and has been **optimized for reasoning tasks**.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- An AWS account with access to Amazon Bedrock\n",
    "- Appropriate IAM roles and permissions for Bedrock and S3, following [the instruction here](https://docs.aws.amazon.com/bedrock/latest/userguide/model-import-iam-role.html)\n",
    "- A S3 bucket prepared to store the custom model\n",
    "- Sufficient local storage space (At least 17GB for 8B and 135GB for 70B models)\n",
    "\n",
    "## Important Note About Inference API Options\n",
    "This notebook supports two ways to interact with your imported model:\n",
    "1. Direct [InvokeModel API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_InvokeModel.html) \n",
    "2. [Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-call.html)\n",
    "\n",
    "If you plan to use the Converse API, you'll need to update the model's <i>chat_template</i> configuration before uploading to S3. We provide instructions for both paths in this notebook. For details on using Converse API with CMI, check it [here](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-model-import-code-samples-converse.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15844386-cbc9-49c9-8dbe-fcbf2dfab1eb",
   "metadata": {},
   "source": [
    "### Step 1: Install Required Packages\n",
    "\n",
    "First, let's install the necessary Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8e0984-0d83-4e83-8710-8d4870444af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install boto3 --upgrade\n",
    "!pip install -U huggingface_hub\n",
    "!pip install hf_transfer huggingface huggingface_hub \"huggingface_hub[hf_transfer]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec46496-b12a-407e-8ad5-bf4e60a7bf97",
   "metadata": {},
   "source": [
    "### Step 2: Configure Parameters\n",
    "\n",
    "Update these parameters according to your AWS environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd8cb8-d3c0-4e6c-ba58-071cdee2894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your parameters (please update this part based on your setup)\n",
    "bucket_name = \"<YOUR-PREDEFINED-S3-BUCKET-TO-HOST-IMPORT-MODEL>\"\n",
    "s3_prefix = \"<S3-PREFIX>\" # E.x. DeepSeek-R1-Distill-Llama-8B\n",
    "local_directory = \"<LOCAL-FOLDER-TO-STORE-DOWNLOADED-MODEL>\" # E.x. DeepSeek-R1-Distill-Llama-8B\n",
    "\n",
    "job_name = '<CMI-JOB-NAME>' # E.x. Deepseek-8B-job\n",
    "imported_model_name = '<CMI-MODEL-NAME>' # E.x. Deepseek-8B-model\n",
    "role_arn = '<IAM-ROLE-ARN>' # Please make sure it has sufficient permission as listed in the pre-requisite\n",
    "\n",
    "# Region (currently only 'us-west-2' and 'us-east-1' support CMI with Deepseek-Distilled-Llama models)\n",
    "region_info = 'us-west-2' # You can modify to 'us-east-1' based on your need"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cffabe2-0c4b-431a-aadb-167f5cb26fa4",
   "metadata": {},
   "source": [
    "### Step 3: Download Model from Hugging Face\n",
    "\n",
    "Download the model files from Hugging Face. \n",
    "\n",
    "- Note that you can also use the 70B model by changing the model_id to \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\":"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d7789f-958b-4459-a345-67891c5c86ec",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<b>Note:</b> Downloading the 8B model files may take 2-10 minutes depending on your internet connection speed.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc12e1c1-2aec-4fda-9199-a32d95d2e615",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "# Enable hf_transfer for faster downloads\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "\n",
    "# Download using snapshot_download with hf_transfer enabled\n",
    "snapshot_download(repo_id=model_id, local_dir=f\"./{local_directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8a088-9f4b-4022-a2e4-abb1fbe7e64c",
   "metadata": {},
   "source": [
    "### Step 3b: (Optional) Configure for Converse API\n",
    "If you plan to use the Converse API, you'll need to update the model's chat_template configuration.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6201f29c-9675-4483-a814-173242a8ccb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def update_tokenizer_config(local_directory):\n",
    "    \"\"\"\n",
    "    Updates the tokenizer_config.json file with the required chat template for Converse API support\n",
    "    \"\"\"\n",
    "        \n",
    "    config_path = f\"{local_directory}/tokenizer_config.json\"\n",
    "    \n",
    "    try:\n",
    "        # Read existing config\n",
    "        with open(config_path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        # Update chat template\n",
    "        config[\"chat_template\"] = \"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \\\"26 Jul 2024\\\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}\\n    {%- set system_message = messages[0]['content']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \\\"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\\\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \\\"Environment: ipython\\\\n\\\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \\\"Tools: \\\" + builtin_tools | reject('equalto', 'code_interpreter') | join(\\\", \\\") + \\\"\\\\n\\\\n\\\"}}\\n{%- endif %}\\n{{- \\\"Cutting Knowledge Date: December 2023\\\\n\\\" }}\\n{{- \\\"Today Date: \\\" + date_string + \\\"\\\\n\\\\n\\\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \\\"<|eot_id|>\\\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|start_header_id|>user<|end_header_id|>\\\\n\\\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\\\n\\\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\\\n\\\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\\\n\\\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot_id|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n'+ message['content'] | trim + '<|eot_id|>' }}\\n    {%- elif 'tool_calls' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\\\"This model only supports single tool-calls at once!\\\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- \\\"<|python_tag|>\\\" + tool_call.name + \\\".call(\\\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + '=\\\"' + arg_val + '\\\"' }}\\n                {%- if not loop.last %}\\n                    {{- \\\", \\\" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \\\")\\\" }}\\n        {%- else  %}\\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' -}}\\n            {{- '{\\\"name\\\": \\\"' + tool_call.name + '\\\", ' }}\\n            {{- '\\\"parameters\\\": ' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\\"}\\\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we're in ipython mode #}\\n            {{- \\\"<|eom_id|>\\\" }}\\n        {%- else %}\\n            {{- \\\"<|eot_id|>\\\" }}\\n        {%- endif %}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot_id|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\\n{%- endif %}\\n\"\n",
    "\n",
    "        # Write updated config back\n",
    "        with open(config_path, 'w') as f:\n",
    "            json.dump(config, f)\n",
    "        \n",
    "        print(\"Successfully updated tokenizer_config.json with chat_template for Converse API support\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating tokenizer config: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Update the configuration if Converse API is enabled\n",
    "update_tokenizer_config(local_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1557e7ac-0626-4b9a-ba62-d2207e25cd23",
   "metadata": {},
   "source": [
    "### Step 4: Upload Model to S3\n",
    "\n",
    "Upload the downloaded model files to your S3 bucket\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Note:</b> Uploading the 8B model files normally takes 10-20 minutes.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ed56d4-ee79-4378-8a77-105889f840b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def upload_directory_to_s3(local_directory, bucket_name, s3_prefix):\n",
    "    s3_client = boto3.client('s3')\n",
    "    local_directory = Path(local_directory)\n",
    "    \n",
    "    # Get list of all files first\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(local_directory):\n",
    "        for filename in files:\n",
    "            local_path = Path(root) / filename\n",
    "            relative_path = local_path.relative_to(local_directory)\n",
    "            s3_key = f\"{s3_prefix}/{relative_path}\"\n",
    "            all_files.append((local_path, s3_key))\n",
    "    \n",
    "    # Upload with progress bar\n",
    "    for local_path, s3_key in tqdm(all_files, desc=\"Uploading files\"):\n",
    "        try:\n",
    "            s3_client.upload_file(\n",
    "                str(local_path),\n",
    "                bucket_name,\n",
    "                s3_key\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {local_path}: {str(e)}\")\n",
    "\n",
    "\n",
    "# Upload all files\n",
    "upload_directory_to_s3(local_directory, bucket_name, s3_prefix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3042657-a7e1-4063-abb5-73ea1f8cda80",
   "metadata": {},
   "source": [
    "### Step 5: Create Custom Model Import Job\n",
    "\n",
    "Initialize the import job in Amazon Bedrock\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "<b>Note:</b> Creating CMI job for 8B model could take 5-20 minutes to complete.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f898ec5-6123-4647-8852-456045efcada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Bedrock client\n",
    "bedrock = boto3.client('bedrock', region_name=region_info)\n",
    "\n",
    "s3_uri = f's3://{bucket_name}/{s3_prefix}/'\n",
    "\n",
    "# Create the model import job\n",
    "response = bedrock.create_model_import_job(\n",
    "    jobName=job_name,\n",
    "    importedModelName=imported_model_name,\n",
    "    roleArn=role_arn,\n",
    "    modelDataSource={\n",
    "        's3DataSource': {\n",
    "            's3Uri': s3_uri\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "job_Arn = response['jobArn']\n",
    "\n",
    "# Output the job ARN\n",
    "print(f\"Model import job created with ARN: {response['jobArn']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a18c49-e5a9-4706-abe5-f11e1dfa4c3a",
   "metadata": {},
   "source": [
    "### Step 6: Monitor Import Job Status\n",
    "\n",
    "Check the status of your import job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4235f3-4136-43a8-b461-f7c4e96e174a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check CMI job status\n",
    "while True:\n",
    "    response = bedrock.get_model_import_job(jobIdentifier=job_Arn)\n",
    "    status = response['status'].upper()\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    if status in ['COMPLETED', 'FAILED']:\n",
    "        break\n",
    "        \n",
    "    time.sleep(60)  # Check every 60 seconds\n",
    "\n",
    "# Get the model ID\n",
    "model_id = response['importedModelArn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5612d199-ffcf-4ffd-803e-b05e1c148f88",
   "metadata": {},
   "source": [
    "### Step 7: Wait for Model Initialization\n",
    "\n",
    "Allow time for the model to initialize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465e3ed-4cb8-4e9e-9740-3e972fec8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for 5mins for cold start \n",
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5319747f-4b8f-41b7-abf2-a046e0a23d51",
   "metadata": {},
   "source": [
    "### Step 8: Model Inference Options\n",
    "\n",
    "After successful model import and initialization, you can interact with your model through two different APIs:\n",
    "1. Direct InvokeModel API \n",
    "2. Converse API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51149554-a419-4cf7-93fb-6a5c14f3f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = boto3.client(\"bedrock-runtime\", region_name=region_info)\n",
    "\n",
    "def invoke_r1_direct(user_prompt, max_retries=10, return_prompt=False):\n",
    "    \"\"\"\n",
    "    Direct invocation using invoke_model API\n",
    "    \n",
    "    Parameters:\n",
    "        user_prompt (str): The prompt to send to the model\n",
    "        max_retries (int): Number of retries if the model doesn't respond\n",
    "        return_prompt (bool): If True, prints the formatted prompt\n",
    "    \n",
    "    Returns:\n",
    "        str: The model's response\n",
    "    \"\"\"\n",
    "    formatted_prompt = (\n",
    "        f\"<s>[INST]\\n\"\n",
    "        f\"\\nHuman: {user_prompt}[/INST]\"\n",
    "        \"\\n\\nAssistant: \"\n",
    "    )\n",
    "\n",
    "    if return_prompt:\n",
    "        print(\"==== Prompt ====\")\n",
    "        print(formatted_prompt)\n",
    "        print(\"================\")\n",
    "\n",
    "    native_request = {\n",
    "        \"prompt\": formatted_prompt,\n",
    "        \"max_gen_len\": 4096,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6\n",
    "    }\n",
    "\n",
    "    attempt = 0\n",
    "    response_text = \"\"\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            response = client.invoke_model(modelId=model_id, body=json.dumps(native_request))\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            if 'generation' in response_body:\n",
    "                response_text = response_body['generation'].strip()\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "        attempt += 1\n",
    "        time.sleep(30)\n",
    "\n",
    "    return response_text\n",
    "\n",
    "def invoke_r1_converse(user_prompt, system_prompt=\"You are a helpful assistant.\", max_retries=10):\n",
    "    \"\"\"\n",
    "    Invocation using the Converse API\n",
    "    \n",
    "    Parameters:\n",
    "        user_prompt (str): The prompt to send to the model\n",
    "        system_prompt (str): System prompt to set the model's behavior\n",
    "        max_retries (int): Number of retries if the model doesn't respond\n",
    "    \n",
    "    Returns:\n",
    "        str: The model's response\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\n",
    "                \"text\": user_prompt\n",
    "            }]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    system_prompts = [{\"text\": system_prompt}]\n",
    "\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            response = client.converse(\n",
    "                modelId=model_id,\n",
    "                messages=messages,\n",
    "                system=system_prompts,\n",
    "                inferenceConfig={\n",
    "                    \"temperature\": 0.6,\n",
    "                    \"topP\": 0.9,\n",
    "                    \"maxTokens\": 2048\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            output_message = response['output']['message']\n",
    "            return output_message['content'][0]['text']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "            attempt += 1\n",
    "            time.sleep(30)\n",
    "    \n",
    "    return \"Failed to get response after maximum retries\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3c0a9c-6893-4cb1-9462-eabecc2d1f80",
   "metadata": {},
   "source": [
    "#### Example Usage\n",
    "Let's test the model with a simple reasoning task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b9022-652e-4e4f-a13d-9edd5b62069d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompt = \"\"\"Given the following financial data:\n",
    "- Company A's revenue grew from $10M to $15M in 2023\n",
    "- Operating costs increased by 20%\n",
    "- Initial operating costs were $7M\n",
    "\n",
    "Calculate the company's operating margin for 2023. Please reason step by step, and put your final answer within \\\\boxed{}.\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Using direct invoke_model API ===\")\n",
    "response_direct = invoke_r1_direct(test_prompt)\n",
    "print(response_direct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6082dcb-2ea1-425c-8391-00b7eb9030f7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "Run the following code block only if you have run <b>step 3b</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28045fb6-c48e-41c1-b397-007fa55adc4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Using Converse API ===\")\n",
    "response_converse = invoke_r1_converse(\n",
    "    test_prompt,\n",
    "    system_prompt=\"You are a helpful financial analyst.\"\n",
    ")\n",
    "print(response_converse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24455a7f-89eb-4538-aadd-784ab1a817d7",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates the end-to-end process of importing DeepSeek's distilled Llama models to Amazon Bedrock using Custom Model Import (CMI). Starting from downloading the model from HuggingFace, through preparing and uploading files to S3, to creating a CMI job and performing inference, we've covered the essential steps to get your DeepSeek distilled Llama models running on Amazon Bedrock.\n",
    "\n",
    "\n",
    "While we've used the DeepSeek-R1-Distill-Llama-8B model in this example, the same process applies to other variants including the 70B model. For more information about Custom Model Import and its features, refer to the [Amazon Bedrock documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-customization-import-model.html)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
