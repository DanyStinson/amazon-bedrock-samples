{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contextual retrieval using custom chunking provided by Amazon Bedrock Knowledge Bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will create 2 knowledge bases to provide sample code for the following chunking options supported by Amazon Bedrock Knowledge Bases: \n",
    "- Fixed Chunking\n",
    "- Contextual retrieval using custom chunking with Lambda function \n",
    "\n",
    "We will use a synthetic 10K report as data for a fiticious company called `Octank Financial` to demo the solution.\n",
    "After creating knowledge bases we will evaluate the results on the same dataset. The focus will be on improving the quality of search results which in turn will improve the accuracy of responses generated by the foundation model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import the needed libraries\n",
    "First step is to install the pre-requisites packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip --quiet\n",
    "%pip install -r ../requirements.txt --no-deps --quiet\n",
    "%pip install -r ../requirements.txt --upgrade --quiet\n",
    "%pip install --upgrade ragas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restart kernel\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "botocore.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import boto3\n",
    "import logging\n",
    "import pprint\n",
    "import json\n",
    "\n",
    "# Set the path to import module\n",
    "from pathlib import Path\n",
    "current_path = Path().resolve()\n",
    "current_path = current_path.parent\n",
    "if str(current_path) not in sys.path:\n",
    "    sys.path.append(str(current_path))\n",
    "# Print sys.path to verify\n",
    "# print(sys.path)\n",
    "\n",
    "from utils.knowledge_base import BedrockKnowledgeBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clients\n",
    "s3_client = boto3.client('s3')\n",
    "sts_client = boto3.client('sts')\n",
    "session = boto3.session.Session()\n",
    "region =  session.region_name\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "bedrock_agent_client = boto3.client('bedrock-agent')\n",
    "bedrock_agent_runtime_client = boto3.client('bedrock-agent-runtime') \n",
    "logging.basicConfig(format='[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s', level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "region, account_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Get the current timestamp\n",
    "current_time = time.time()\n",
    "\n",
    "# Format the timestamp as a string\n",
    "timestamp_str = time.strftime(\"%Y%m%d%H%M%S\", time.localtime(current_time))[-7:]\n",
    "# Create the suffix using the timestamp\n",
    "suffix = f\"{timestamp_str}\"\n",
    "knowledge_base_name_standard = 'standard-kb'\n",
    "knowledge_base_name_custom = 'custom-chunking-kb'\n",
    "knowledge_base_description = \"Knowledge Base containing complex PDF.\"\n",
    "bucket_name = f'{knowledge_base_name_standard}-{suffix}'\n",
    "intermediate_bucket_name = f'{knowledge_base_name_standard}-intermediate-{suffix}'\n",
    "lambda_function_name = f'{knowledge_base_name_custom}-lambda-{suffix}'\n",
    "foundation_model = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "# Define data sources\n",
    "data_source=[{\"type\": \"S3\", \"bucket_name\": bucket_name}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Create knowledge bases with fixed chunking strategy\n",
    "Let's start by creating a [Amazon Bedrock Knowledge Bases](https://aws.amazon.com/bedrock/knowledge-bases/) to store the restaurant menus. Knowledge Bases allow you to integrate with different vector databases including [Amazon OpenSearch Serverless](https://aws.amazon.com/opensearch-service/features/serverless/), [Amazon Aurora](https://aws.amazon.com/rds/aurora/), [Pinecone](http://app.pinecone.io/bedrock-integration), [Redis Enterprise]() and [MongoDB Atlas](). For this example, we will integrate the knowledge base with Amazon OpenSearch Serverless. To do so, we will use the helper class `BedrockKnowledgeBase` which will create the knowledge base and all of its pre-requisites:\n",
    "1. IAM roles and policies\n",
    "2. S3 bucket\n",
    "3. Amazon OpenSearch Serverless encryption, network and data access policies\n",
    "4. Amazon OpenSearch Serverless collection\n",
    "5. Amazon OpenSearch Serverless vector index\n",
    "6. Knowledge base\n",
    "7. Knowledge base data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a knowledge base using fixed chunking strategy followed by hierarchical chunking strategy. \n",
    "\n",
    "Parameter values: \n",
    "```\n",
    "\"chunkingStrategy\": \"FIXED_SIZE | NONE \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knowledge_base_standard = BedrockKnowledgeBase(\n",
    "    kb_name=f'{knowledge_base_name_standard}-{suffix}',\n",
    "    kb_description=knowledge_base_description,\n",
    "    data_sources=data_source,\n",
    "    chunking_strategy = \"FIXED_SIZE\", \n",
    "    suffix = f'{suffix}-f'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_standard = BedrockKnowledgeBase(\n",
    "    kb_name=f'{knowledge_base_name_standard}-{suffix}',\n",
    "    kb_description=knowledge_base_description,\n",
    "    data_sources=data_source,\n",
    "    chunking_strategy = \"FIXED_SIZE\", \n",
    "    suffix = f'{suffix}-f'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Upload the dataset to Amazon S3\n",
    "Now that we have created the knowledge base, let's populate it with the `Octank financial 10K` report dataset. The Knowledge Base data source expects the data to be available on the S3 bucket connected to it and changes on the data can be syncronized to the knowledge base using the `StartIngestionJob` API call. In this example we will use the [boto3 abstraction](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent/client/start_ingestion_job.html) of the API, via our helper classe. \n",
    "\n",
    "Let's first upload the menu's data available on the `dataset` folder to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def upload_directory(path, bucket_name):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            file_to_upload = os.path.join(root, file)\n",
    "            if file not in [\"LICENSE\", \"NOTICE\", \"README.md\"]:\n",
    "                print(f\"uploading file {file_to_upload} to {bucket_name}\")\n",
    "                s3_client.upload_file(file_to_upload, bucket_name, file)\n",
    "            else:\n",
    "                print(f\"Skipping file {file_to_upload}\")\n",
    "\n",
    "upload_directory(\"../synthetic_dataset\", bucket_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start the ingestion job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the kb is available\n",
    "time.sleep(30)\n",
    "# sync knowledge base\n",
    "knowledge_base_standard.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we save the Knowledge Base Id to test the solution at a later stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id_standard = knowledge_base_standard.get_knowledge_base_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test the Knowledge Base\n",
    "Now the Knowlegde Base is available we can test it out using the [**retrieve**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve.html) and [**retrieve_and_generate**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) functions. \n",
    "\n",
    "#### Testing Knowledge Base with Retrieve and Generate API\n",
    "\n",
    "Let's first test the knowledge base using the retrieve and generate API. With this API, Bedrock takes care of retrieving the necessary references from the knowledge base and generating the final answer using a foundation model from Bedrock.\n",
    "\n",
    "query = `Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.`\n",
    "\n",
    "The right response for this query as per ground truth QA pair is: \n",
    "\n",
    "```\n",
    "The cash flow statement for Octank Financial in the year ended December 31, 2019 reveals the following:\n",
    "- Cash generated from operating activities amounted to $710 million, which can be attributed to a $700 million profit and non-cash charges such as depreciation and amortization.\n",
    "- Cash outflow from investing activities totaled $240 million, with major expenditures being the acquisition of property, plant, and equipment ($200 million) and marketable securities ($60 million), partially offset by the sale of property, plant, and equipment ($40 million) and maturing marketable securities ($20 million).\n",
    "- Financing activities resulted in a cash inflow of $350 million, stemming from the issuance of common stock ($200 million) and long-term debt ($300 million), while common stock repurchases ($50 million) and long-term debt payments ($100 million) reduced the cash inflow. \n",
    "Overall, Octank Financial experienced a net cash enhancement of $120 million in 2019, bringing their total cash and cash equivalents to $210 million.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(20)\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id_standard,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with the retrieve and generate API we get the final response directly, now let's observe the citations for `RetreiveAndGenerate` API. Since, our primary focus on this notebook is to observe the retrieved chunks and citations returned by the model while generating the response. When we provide the relevant context to the foundation model alongwith the query, it will most likely generate the high quality response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def citations_rag_print(response_ret):\n",
    "#structure 'retrievalResults': list of contents. Each list has content, location, score, metadata\n",
    "    for num,chunk in enumerate(response_ret,1):\n",
    "        print(f'Chunk {num}: ',chunk['content']['text'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Location: ',chunk['location'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Metadata: ',chunk['metadata'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_standard = response['citations'][0]['retrievedReferences']\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_standard))\n",
    "citations_rag_print(response_standard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now inspect the source information from the knowledge base with the retrieve API.\n",
    "\n",
    "#### Testing Knowledge Base with Retrieve API\n",
    "If you need an extra layer of control, you can retrieve the chunks that best match your query using the retrieve API. In this setup, we can configure the desired number of results and control the final answer with your own application logic. The API then provides you with the matching content, its S3 location, the similarity score and the chunk metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_print(response_ret):\n",
    "#structure 'retrievalResults': list of contents. Each list has content, location, score, metadata\n",
    "    for num,chunk in enumerate(response_ret['retrievalResults'],1):\n",
    "        print(f'Chunk {num}: ',chunk['content']['text'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Location: ',chunk['location'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Score: ',chunk['score'],end='\\n'*2)\n",
    "        print(f'Chunk {num} Metadata: ',chunk['metadata'],end='\\n'*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_standard_ret = bedrock_agent_runtime_client.retrieve(\n",
    "    knowledgeBaseId=kb_id_standard, \n",
    "    nextToken='string',\n",
    "    retrievalConfiguration={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\":5,\n",
    "        } \n",
    "    },\n",
    "    retrievalQuery={\n",
    "        'text': query\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"# of retrieved results: \", len(response_standard_ret['retrievalResults']))\n",
    "response_print(response_standard_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can notice, that with `fixed chunking` we get 5 retrieved results as requested in the API using `semantic similarity` which is the default for `Retrieve API`. Let's now use `Custom chunking` strategy for Contextual retrieval and inspect the retrieved results using `RetrieveAndGenerate` API as well as `Retrieve` API. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Contextual retrieval using custom chunking option\n",
    "When creating an Knowledge Bases (KB) for Amazon Bedrock, you can connect a Lambda function to specify your custom chunking logic. During ingestion, if lambda function is provided, Knowledge Bases, will run the lambda function, and store the input and output values in the intermediate s3 bucket provided.\n",
    "\n",
    "> <br>\n",
    "> Note: Lambda function with KB can be used for adding custom chunking logic as well processing your chunks for example, adding chunk level metadata. In this example we are focusing on using Lambda function for custom chunking logic.\n",
    "> <br></br>\n",
    "\n",
    "### 2.1 Create the Lambda Function\n",
    "\n",
    "We will now create a lambda function which will have code for custom chunking. To do so we will:\n",
    "\n",
    "1. Create the `lambda_function.py` file which contains the logic for custom chunking.\n",
    "2. Create the IAM role for our Lambda function.\n",
    "3. Create the lambda function with the required permissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the function code\n",
    " Let's create the lambda function tha implements the functions for `reading your file from intermediate bucket`, `process the contents with custom chunking logic` and `write the output back to s3 bucket`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile lambda_function.py\n",
    "import json\n",
    "import boto3\n",
    "import os\n",
    "import logging\n",
    "import traceback\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "# Constants for chunking\n",
    "MAX_TOKENS = 1000\n",
    "OVERLAP_PERCENTAGE = 0.20\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"\n",
    "    Rough estimation of tokens (approximation: 1 token ≈ 0.75 words)\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return int(len(words) * 0.75)\n",
    "\n",
    "def chunk_text(text, max_tokens=MAX_TOKENS, overlap_percentage=OVERLAP_PERCENTAGE):\n",
    "    \"\"\"\n",
    "    Chunk text based on words with specified max tokens and overlap\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return []\n",
    "\n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    # Estimate words per chunk based on token limit\n",
    "    # Assuming average of 0.75 tokens per word\n",
    "    words_per_chunk = int(max_tokens * 1.33)  # Convert tokens to approximate words\n",
    "    overlap_words = int(words_per_chunk * overlap_percentage)\n",
    "\n",
    "    chunks = []\n",
    "    current_position = 0\n",
    "    total_words = len(words)\n",
    "\n",
    "    while current_position < total_words:\n",
    "        # Calculate end position for current chunk\n",
    "        chunk_end = min(current_position + words_per_chunk, total_words)\n",
    "        \n",
    "        # Get current chunk words\n",
    "        chunk_words = words[current_position:chunk_end]\n",
    "        \n",
    "        # If this isn't the last chunk, try to find a good break point\n",
    "        if chunk_end < total_words:\n",
    "            # Look for sentence-ending punctuation in the last few words\n",
    "            for i in range(len(chunk_words) - 1, max(len(chunk_words) - 10, 0), -1):\n",
    "                if chunk_words[i].endswith(('.', '!', '?')):\n",
    "                    chunk_end = current_position + i + 1\n",
    "                    chunk_words = chunk_words[:i + 1]\n",
    "                    break\n",
    "\n",
    "        # Join words back into text\n",
    "        chunk_text = ' '.join(chunk_words)\n",
    "        chunks.append(chunk_text.strip())\n",
    "        \n",
    "        # Move position considering overlap\n",
    "        current_position = chunk_end - overlap_words if chunk_end < total_words else chunk_end\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def write_output_to_s3(s3_client, bucket_name, file_name, json_data):\n",
    "    \"\"\"\n",
    "    Write JSON data to S3 bucket\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_string = json.dumps(json_data)\n",
    "        response = s3_client.put_object(\n",
    "            Bucket=bucket_name,\n",
    "            Key=file_name,\n",
    "            Body=json_string,\n",
    "            ContentType='application/json'\n",
    "        )\n",
    "\n",
    "        if response['ResponseMetadata']['HTTPStatusCode'] == 200:\n",
    "            print(f\"Successfully uploaded {file_name} to {bucket_name}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Failed to upload {file_name} to {bucket_name}\")\n",
    "            return False\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return False\n",
    "\n",
    "def read_from_s3(s3_client, bucket_name, file_name):\n",
    "    \"\"\"\n",
    "    Read JSON data from S3 bucket\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.get_object(Bucket=bucket_name, Key=file_name)\n",
    "        return json.loads(response['Body'].read().decode('utf-8'))\n",
    "    except ClientError as e:\n",
    "        print(f\"Error reading file from S3: {str(e)}\")\n",
    "\n",
    "def parse_s3_path(s3_path):\n",
    "    \"\"\"\n",
    "    Parse S3 path into bucket and key\n",
    "    \"\"\"\n",
    "    s3_path = s3_path.replace('s3://', '')\n",
    "    parts = s3_path.split('/', 1)\n",
    "    if len(parts) != 2:\n",
    "        raise ValueError(\"Invalid S3 path format\")\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "def invoke_model_with_response_stream(bedrock_runtime, prompt, max_tokens=1000):\n",
    "    \"\"\"\n",
    "    Invoke Bedrock model with streaming response\n",
    "    \"\"\"\n",
    "    model_id = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "    request_body = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.0,\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "            modelId=model_id,\n",
    "            contentType='application/json',\n",
    "            accept='application/json',\n",
    "            body=request_body\n",
    "        )\n",
    "\n",
    "        for event in response.get('body'):\n",
    "            chunk = json.loads(event['chunk']['bytes'].decode())\n",
    "            if chunk['type'] == 'content_block_delta':\n",
    "                yield chunk['delta']['text']\n",
    "            elif chunk['type'] == 'message_delta':\n",
    "                if 'stop_reason' in chunk['delta']:\n",
    "                    break\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        yield None\n",
    "\n",
    "# Define the contextual retrieval prompt\n",
    "contextual_retrieval_prompt = \"\"\"\n",
    "    <document>\n",
    "    {doc_content}\n",
    "    </document>\n",
    "\n",
    "    Here is the chunk we want to situate within the whole document\n",
    "    <chunk>\n",
    "    {chunk_content}\n",
    "    </chunk>\n",
    "\n",
    "    Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk.\n",
    "    Answer only with the succinct context and nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Lambda handler function\n",
    "    \"\"\"\n",
    "    logger.debug('input={}'.format(json.dumps(event)))\n",
    "\n",
    "    s3_client = boto3.client('s3')\n",
    "    bedrock_runtime = boto3.client(\n",
    "        service_name='bedrock-runtime',\n",
    "        region_name='us-east-1'\n",
    "    )\n",
    "\n",
    "    input_files = event.get('inputFiles')\n",
    "    input_bucket = event.get('bucketName')\n",
    "\n",
    "    if not all([input_files, input_bucket]):\n",
    "        raise ValueError(\"Missing required input parameters\")\n",
    "\n",
    "    output_files = []\n",
    "    for input_file in input_files:\n",
    "        processed_batches = []\n",
    "        for batch in input_file.get('contentBatches'):\n",
    "            input_key = batch.get('key')\n",
    "\n",
    "            if not input_key:\n",
    "                raise ValueError(\"Missing uri in content batch\")\n",
    "\n",
    "            file_content = read_from_s3(s3_client, bucket_name=input_bucket, file_name=input_key)\n",
    "            print(file_content.get('fileContents'))\n",
    "\n",
    "            original_document_content = ''.join(\n",
    "                content.get('contentBody') \n",
    "                for content in file_content.get('fileContents') \n",
    "                if content\n",
    "            )\n",
    "\n",
    "            chunked_content = {\n",
    "                'fileContents': []\n",
    "            }\n",
    "            \n",
    "            for content in file_content.get('fileContents'):\n",
    "                content_body = content.get('contentBody', '')\n",
    "                content_type = content.get('contentType', '')\n",
    "                content_metadata = content.get('contentMetadata', {})\n",
    "\n",
    "                # Apply chunking strategy\n",
    "                chunks = chunk_text(content_body)\n",
    "                \n",
    "                for chunk in chunks:\n",
    "                    prompt = contextual_retrieval_prompt.format(\n",
    "                        doc_content=original_document_content, \n",
    "                        chunk_content=chunk\n",
    "                    )\n",
    "                    response_stream = invoke_model_with_response_stream(bedrock_runtime, prompt)\n",
    "                    chunk_context = ''.join(chunk_text for chunk_text in response_stream if chunk_text)\n",
    "\n",
    "                    chunked_content['fileContents'].append({\n",
    "                        \"contentBody\": chunk_context + \"\\n\\n\" + chunk,\n",
    "                        \"contentType\": content_type,\n",
    "                        \"contentMetadata\": content_metadata,\n",
    "                    })\n",
    "\n",
    "            output_key = f\"Output/{input_key}\"\n",
    "            write_output_to_s3(s3_client, input_bucket, output_key, chunked_content)\n",
    "            processed_batches.append({\"key\": output_key})\n",
    "            \n",
    "        output_files.append({\n",
    "            \"originalFileLocation\": input_file.get('originalFileLocation'),\n",
    "            \"fileMetadata\": {},\n",
    "            \"contentBatches\": processed_batches\n",
    "        })\n",
    "\n",
    "    return {\n",
    "        \"outputFiles\": output_files\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard chunking strategies values provided by knowledge bases are following: \n",
    "\n",
    "**Parameter values:**\n",
    " \n",
    "```\n",
    "\"chunkingStrategy\": \"FIXED_SIZE | NONE | HIERARCHICAL | SEMANTIC\"\n",
    "```\n",
    "\n",
    "For implementing our custom logic, we have included an option in the `knowledge_base.py` class for passing a value of `CUSTOM\"`. \n",
    "If you pass the chunking strategy as `CUSTOM` in this class, it will do the following: \n",
    "\n",
    "1. It select the `chunkingStrategy` as `NONE`. \n",
    "2. It will add `customTransformationConfiguration` to the `vectorIngestionConfiguration` as follows: \n",
    "\n",
    "```\n",
    "{\n",
    "...\n",
    "   \"vectorIngestionConfiguration\": {\n",
    "    \"customTransformationConfiguration\": { \n",
    "         \"intermediateStorage\": { \n",
    "            \"s3Location\": { \n",
    "               \"uri\": \"string\"\n",
    "            }\n",
    "         },\n",
    "         \"transformations\": [\n",
    "            {\n",
    "               \"transformationFunction\": {\n",
    "                  \"lambdaConfiguration\": {\n",
    "                     \"lambdaArn\": \"string\"\n",
    "                  }\n",
    "               },\n",
    "               \"stepToApply\": \"string\" // enum of POST_CHUNKING\n",
    "            }\n",
    "         ]\n",
    "      },\n",
    "      \"chunkingConfiguration\": {\n",
    "         \"chunkingStrategy\": \"NONE\"\n",
    "         ...\n",
    "   }\n",
    "}\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_base_custom = BedrockKnowledgeBase(\n",
    "    kb_name=f'{knowledge_base_name_custom}-{suffix}',\n",
    "    kb_description=knowledge_base_description,\n",
    "    data_sources=data_source,\n",
    "    lambda_function_name=lambda_function_name,\n",
    "    intermediate_bucket_name=intermediate_bucket_name, \n",
    "    chunking_strategy = \"CUSTOM\", \n",
    "    suffix = f'{suffix}-c'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Update AWS Lambda IAM role with addiotional permissions to access the bedrock invole api and Lambda function timeout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize AWS clients\n",
    "lambda_client = boto3.client('lambda')\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "def update_lambda_timeout():\n",
    "    try:\n",
    "        # Update Lambda function configuration\n",
    "        response = lambda_client.update_function_configuration(\n",
    "            FunctionName=lambda_function_name,\n",
    "            Timeout=900  # 15 minutes in seconds\n",
    "        )\n",
    "        print(f\"Successfully updated Lambda timeout to 15 minutes\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error updating Lambda timeout: {str(e)}\")\n",
    "\n",
    "def get_lambda_role():\n",
    "    # Get Lambda function configuration\n",
    "    response = lambda_client.get_function_configuration(\n",
    "        FunctionName=lambda_function_name\n",
    "    )\n",
    "    \n",
    "    # Extract the role ARN\n",
    "    role_arn = response['Role']\n",
    "    role_name = role_arn.split('/')[-1]\n",
    "    \n",
    "    print(f\"Lambda Role ARN: {role_arn}\")\n",
    "    print(f\"Lambda Role Name: {role_name}\")\n",
    "    \n",
    "    return role_name\n",
    "\n",
    "def create_bedrock_policy(role_name):\n",
    "    # Define the policy document for Bedrock access\n",
    "    bedrock_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Action\": [\n",
    "                    \"bedrock:InvokeModelWithResponseStream\"\n",
    "                ],\n",
    "                \"Resource\": [\n",
    "                    \"*\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create the policy\n",
    "    try:\n",
    "        response = iam_client.create_policy(\n",
    "            PolicyName='BedrockClaudeAccess',\n",
    "            PolicyDocument=json.dumps(bedrock_policy)\n",
    "        )\n",
    "        policy_arn = response['Policy']['Arn']\n",
    "    except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "        # If policy already exists, get its ARN\n",
    "        account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "        policy_arn = f'arn:aws:iam::{account_id}:policy/BedrockClaudeAccess'\n",
    "    \n",
    "    # Attach the policy to the role\n",
    "    try:\n",
    "        iam_client.attach_role_policy(\n",
    "            RoleName=role_name,\n",
    "            PolicyArn=policy_arn\n",
    "        )\n",
    "        print(f\"Successfully attached Bedrock policy to role {role_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error attaching policy: {str(e)}\")\n",
    "\n",
    "# Execute the functions\n",
    "update_lambda_timeout()\n",
    "role_name = get_lambda_role()\n",
    "create_bedrock_policy(role_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start the ingestion job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure that the kb is available\n",
    "time.sleep(30)\n",
    "# sync knowledge base\n",
    "knowledge_base_custom.start_ingestion_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kb_id_custom = knowledge_base_custom.get_knowledge_base_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Test the Knowledge Base\n",
    "Now the Knowlegde Base is available we can test it out using the [**retrieve**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve.html) and [**retrieve_and_generate**](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) functions. \n",
    "\n",
    "#### Testing Knowledge Base with Retrieve and Generate API\n",
    "\n",
    "Let's first test the knowledge base using the retrieve and generate API. With this API, Bedrock takes care of retrieving the necessary references from the knowledge base and generating the final answer using a foundation model from Bedrock.\n",
    "\n",
    "query = `Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.`\n",
    "\n",
    "The right response for this query as per ground truth QA pair is: \n",
    "\n",
    "```\n",
    "The cash flow statement for Octank Financial in the year ended December 31, 2019 reveals the following:\n",
    "- Cash generated from operating activities amounted to $710 million, which can be attributed to a $700 million profit and non-cash charges such as depreciation and amortization.\n",
    "- Cash outflow from investing activities totaled $240 million, with major expenditures being the acquisition of property, plant, and equipment ($200 million) and marketable securities ($60 million), partially offset by the sale of property, plant, and equipment ($40 million) and maturing marketable securities ($20 million).\n",
    "- Financing activities resulted in a cash inflow of $350 million, stemming from the issuance of common stock ($200 million) and long-term debt ($300 million), while common stock repurchases ($50 million) and long-term debt payments ($100 million) reduced the cash inflow. \n",
    "Overall, Octank Financial experienced a net cash enhancement of $120 million in 2019, bringing their total cash and cash equivalents to $210 million.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Provide a summary of consolidated statements of cash flows of Octank Financial for the fiscal years ended December 31, 2019.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(10)\n",
    "\n",
    "response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "    input={\n",
    "        \"text\": query\n",
    "    },\n",
    "    retrieveAndGenerateConfiguration={\n",
    "        \"type\": \"KNOWLEDGE_BASE\",\n",
    "        \"knowledgeBaseConfiguration\": {\n",
    "            'knowledgeBaseId': kb_id_custom,\n",
    "            \"modelArn\": \"arn:aws:bedrock:{}::foundation-model/{}\".format(region, foundation_model),\n",
    "            \"retrievalConfiguration\": {\n",
    "                \"vectorSearchConfiguration\": {\n",
    "                    \"numberOfResults\":5\n",
    "                } \n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response['output']['text'],end='\\n'*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, with the `RetreiveAndGenerate` API we get the final response directly, now let's observe the citations for `RetreiveAndGenerate` API. Since, our primary focus on this notebook is to observe the retrieved chunks and citations returned by the model while generating the response. When we provide the relevant context to the foundation model alongwith the query, it will most likely generate the high quality response. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_custom = response['citations'][0]['retrievedReferences']\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_custom))\n",
    "citations_rag_print(response_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now retrieve the source information from the knowledge base with the retrieve API.\n",
    "\n",
    "#### Testing Knowledge Base with Retrieve API\n",
    "If you need an extra layer of control, you can retrieve the chuncks that best match your query using the retrieve API. In this setup, we can configure the desired number of results and control the final answer with your own application logic. The API then provides you with the matching content, its S3 location, the similarity score and the chunk metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_custom_ret = bedrock_agent_runtime_client.retrieve(\n",
    "    knowledgeBaseId=kb_id_custom, \n",
    "    nextToken='string',\n",
    "    retrievalConfiguration={\n",
    "        \"vectorSearchConfiguration\": {\n",
    "            \"numberOfResults\":5,\n",
    "        } \n",
    "    },\n",
    "    retrievalQuery={\n",
    "        'text': query\n",
    "    }\n",
    ")\n",
    "print(\"# of citations or chunks used to generate the response: \", len(response_custom_ret['retrievalResults']))\n",
    "response_print(response_custom_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you look at the chunks, each chunk includes context+chunk for a given chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all cases, while evaluating one query, we got the correct response. However, when you are building a RAG application, you need to evaluate with large number of Questions and Answers to figure out the accuracy improvements. In the next step, we will use RAG Assessment (RAGAS) open source framework to evaluate the responses on `your dataset` for the metrics related to evaluating the quality of the context or search results.\n",
    "We will focus only on 2 metrics: \n",
    "\n",
    "1. Context recall\n",
    "2. Context relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluating search results using RAG Assessment (RAGAS) framework on your dataset\n",
    "You can use RAGAS framework to evaluate your results for each chunking strategy. This approach can help you provide factual guidance on which chunking strategy to use for your dataset. \n",
    "\n",
    "Below approach will provide you heuristics as to which strategy could be used based on the default parameters recommended by Amazon Bedrock Knowledge Bases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Standard: \", kb_id_standard)\n",
    "print(\"Custom chunking: \", kb_id_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.client import Config\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain_aws import ChatBedrock\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "\n",
    "bedrock_client = boto3.client('bedrock-runtime')\n",
    "\n",
    "TEXT_GENERATION_MODEL_ID = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "EVALUATION_MODEL_ID = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "llm_for_evaluation = ChatBedrock(model_id=EVALUATION_MODEL_ID, client=bedrock_client)\n",
    "bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v2:0\", client=bedrock_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_and_generate(query, kb_id):\n",
    "    start = time.time()\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\n",
    "            'text': query\n",
    "        },\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            'type': 'KNOWLEDGE_BASE',\n",
    "            'knowledgeBaseConfiguration': {\n",
    "                'knowledgeBaseId': kb_id,\n",
    "                'modelArn': TEXT_GENERATION_MODEL_ID\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    time_spent = time.time() - start\n",
    "    print(f\"[Response]\\n{response['output']['text']}\\n\")\n",
    "    print(f\"[Invocation time]\\n{time_spent}\\n\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use RAGAS to evaluate RAG\n",
    "\n",
    "As RAGAS aims to be a reference-free evaluation framework, the required preparations of the evaluation dataset are minimal. You will need to prepare question and ground_truths pairs from which you can prepare the remaining information through inference as shown below. If you are not interested in the context_recall metric, you don’t need to provide the ground_truths information. In this case, all you need to prepare are the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ragas import SingleTurnSample, EvaluationDataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness\n",
    ")\n",
    "\n",
    "#specify the metrics here\n",
    "metrics = [\n",
    "    context_recall,\n",
    "    context_precision,\n",
    "    answer_correctness\n",
    "]\n",
    "\n",
    "\n",
    "questions = [\n",
    "    \"What was the primary reason for the increase in net cash provided by operating activities for Octank Financial in 2021?\",\n",
    "    \"In which year did Octank Financial have the highest net cash used in investing activities, and what was the primary reason for this?\",\n",
    "    \"What was the primary source of cash inflows from financing activities for Octank Financial in 2021?\",\n",
    "    \"Based on the information provided, what can you infer about Octank Financial's overall financial health and growth prospects?\"\n",
    "]\n",
    "ground_truths = [\n",
    "    \"The increase in net cash provided by operating activities was primarily due to an increase in net income and favorable changes in operating assets and liabilities.\",\n",
    "    \"Octank Financial had the highest net cash used in investing activities in 2021, at $360 million. The primary reason for this was an increase in purchases of property, plant, and equipment and marketable securities\",\n",
    "    \"The primary source of cash inflows from financing activities for Octank Financial in 2021 was an increase in proceeds from the issuance of common stock and long-term debt.\",\n",
    "    \"Based on the information provided, Octank Financial appears to be in a healthy financial position and has good growth prospects. The company has consistently increased its net cash provided by operating activities, indicating strong profitability and efficient management of working capital. Additionally, Octank Financial has been investing in long-term assets, such as property, plant, and equipment, and marketable securities, which suggests plans for future growth and expansion. The company has also been able to finance its growth through the issuance of common stock and long-term debt, indicating confidence from investors and lenders. Overall, Octank Financial's steady increase in cash and cash equivalents over the past three years provides a strong foundation for future growth and investment opportunities.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_eval_dataset(kb_id, questions, ground_truths):\n",
    "    # Lists to store SingleTurnSample objects\n",
    "    samples = []\n",
    "    \n",
    "    for question, ground_truth in zip(questions, ground_truths):\n",
    "        # Get response and context from your retrieval system\n",
    "        response = retrieve_and_generate(question, kb_id)\n",
    "        answer = response[\"output\"][\"text\"]\n",
    "        \n",
    "        # Process contexts\n",
    "        contexts = []\n",
    "        for citation in response[\"citations\"]:\n",
    "            context_texts = [\n",
    "                ref[\"content\"][\"text\"]\n",
    "                for ref in citation[\"retrievedReferences\"]\n",
    "                if \"content\" in ref and \"text\" in ref[\"content\"]\n",
    "            ]\n",
    "            contexts.extend(context_texts)\n",
    "        \n",
    "        # Create a SingleTurnSample\n",
    "        sample = SingleTurnSample(\n",
    "            user_input=question,\n",
    "            retrieved_contexts=contexts,\n",
    "            response=answer,\n",
    "            reference=ground_truth\n",
    "        )\n",
    "        \n",
    "        # Add the sample to our list\n",
    "        samples.append(sample)\n",
    "        \n",
    "        # Rate limiting\n",
    "        # time.sleep(10)\n",
    "\n",
    "    # Create EvaluationDataset from samples\n",
    "    eval_dataset = EvaluationDataset(samples=samples)\n",
    "    \n",
    "    return eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_chunking_dataset = prepare_eval_dataset(kb_id_custom, questions, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_chunking_result = evaluate(\n",
    "    dataset=contextual_chunking_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=llm_for_evaluation,\n",
    "    embeddings=bedrock_embeddings,\n",
    ")\n",
    "contextual_chunking_result_df = contextual_chunking_result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contextual_chunking_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_chunking_dataset = prepare_eval_dataset(kb_id_standard, questions, ground_truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_chunking_result = evaluate(\n",
    "    dataset=default_chunking_dataset,\n",
    "    metrics=metrics,\n",
    "    llm=llm_for_evaluation,\n",
    "    embeddings=bedrock_embeddings,\n",
    ")\n",
    "\n",
    "default_chunking_result_df = default_chunking_result.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_chunking_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_chunking_avg_metrics = default_chunking_result_df[['context_recall', 'context_precision', 'answer_correctness']].mean()\n",
    "contextual_chunking_avg_metrics = contextual_chunking_result_df[['context_recall', 'context_precision', 'answer_correctness']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Default Chunking': default_chunking_avg_metrics,\n",
    "    'Contextual Chunking': contextual_chunking_avg_metrics\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_max(s):\n",
    "    is_max = s == s.max()\n",
    "    return ['background-color: #90EE90' if v else '' for v in is_max]\n",
    "\n",
    "comparison_df.style.apply(highlight_max, axis=1, subset=['Default Chunking', 'Contextual Chunking'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Using Contextrial Retrieval in RAG chunking strategy can effectively improve retrieval accuracy. It preserves better context by solveing the problem of traditional RAG systems destroying context when splitting documents into chunks and adding relevant contextual information to each chunk before embedding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Remember to delete KB, OSS index and related IAM roles and policies to avoid incurring any charges.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
