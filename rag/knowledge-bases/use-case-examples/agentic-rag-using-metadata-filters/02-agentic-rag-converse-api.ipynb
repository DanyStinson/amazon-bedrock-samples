{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building an Agentic RAG with Amazon Bedrock Converse API\n",
    "\n",
    "> ⚠️ **Important**: Complete [01-metadata-extraction-and-kb-creation.ipynb](./01-metadata-extraction-and-kb-creation.ipynb) before starting this notebook.\n",
    "\n",
    "This notebook guides you through building an agentic RAG system that makes intelligent decisions about information retrieval. The system:\n",
    "- Uses a two-step retrieval process where an agent:\n",
    "  1. First analyzes document summaries to determine which documents are most relevant\n",
    "  2. Then specifically queries chunks from selected documents using metadata filters\n",
    "- Creates a sophisticated question-answering system that understands document context and relevance\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Notebook 1\n",
    "- Two Knowledge Bases populated with summaries and document chunks\n",
    "- Amazon Bedrock access configured. [Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that makes base models from Amazon and third-party model providers accessible through an API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> Amazon Bedrock users need to request access to models before they are available for use. If you want to add additional models for text, chat, and image generation, you need to request access to models in Amazon Bedrock. To request access to additional models, select the Model access link in the left side navigation panel in the Amazon Bedrock console. For more information see: <a href=\"https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html\">https://docs.aws.amazon.com/bedrock/latest/userguide/model-access.html</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we will be using different models of **Anthropic Claude on Amazon Bedrock**. The biggest model, Sonnet 3.5, will be in charge of planning the execution, while the smaller and faster, Haiku 3, will execute the plan. For this, you will need to request access to:\n",
    "\n",
    "- Planning model: **Sonnet 3.5**\n",
    "- Execution model: **Haiku 3**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "First, we'll install and import the necessary libraries. We need boto3 version > 1.34.123 for Amazon Bedrock Converse API support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the boto3 version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Amazon Bedrock clients\n",
    "\n",
    "Set up the necessary AWS clients for interacting with Bedrock services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region1=\"us-west-2\"\n",
    "region2=\"us-east-1\"\n",
    "\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=region1)\n",
    "bedrock_agent_runtime = boto3.client(\"bedrock-agent-runtime\", region_name=region2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Amazon Bedrock Converse API\n",
    "\n",
    "Let's test the API with a simple query to ensure everything is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"user\", \"content\": [{\"text\": \"What is your name?\"}]}]\n",
    "\n",
    "MODEL_NAME_1 = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "MODEL_NAME_2 = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "model_arn = \"arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "response = client.converse(\n",
    "    modelId=MODEL_NAME_1,\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the client-side tools\n",
    "\n",
    "Next, we'll define the client-side tools that our agent will use to assist customers. We'll create seven tools: **xxx, yyy,**\n",
    "\n",
    "These tools represent the capabilities that the agent will have access to when interacting with customers. Each tool is defined with a specific purpose and input schema, allowing the agent to use them appropriately based on the customer's needs.\n",
    "\n",
    "The tools defined include two key functions: \"get_filename\" and \"process_query\". \n",
    "\n",
    "- **get_filename** takes a user query and returns the most relevant document's filename, title, and summary. \n",
    "- **process_query** then uses this filename along with the original query to extract specific, relevant information from the identified document, returning a set of pertinent text chunks. \n",
    "\n",
    "Together, these tools enable a two-step information retrieval process, first identifying the most appropriate document and then extracting the most relevant information from it, thereby providing targeted responses to user queries.\n",
    "\n",
    "It's important to note that this code snippet only defines the specifications for these tools. The actual implementation of these functions will be created later in this notebook. These specifications serve as a blueprint for what each tool can do and what input it requires.\n",
    "\n",
    "For more information on building AI agents with Amazon Bedrock using tools, you might want to refer to the Amazon Bedrock Converse API tool use documentation: https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html \n",
    "\n",
    "This two-step process enhances the RAG's ability to handle complex queries, navigate large document repositories, and deliver tailored information, ultimately improving the overall quality and relevance of its responses.\n",
    "\n",
    "**Note:** Tool use with models is also known as **Function calling**.\n",
    "\n",
    "We'll create two tools for our agent: one to retrieve filenames and another to process queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"get_filename\",\n",
    "            \"description\": \"Useful to retrieve the filename of the document associated to the user's query. Returns the filename, title and summary\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The decomposed query\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"query\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"toolSpec\": {\n",
    "            \"name\": \"process_query\",\n",
    "            \"description\": \"Retrieves specific information related to the user's query using a filter containing the name of the filename in order to get specific details from the relevant document. Returns a set of relevant chunks\",\n",
    "            \"inputSchema\": {\n",
    "                \"json\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"filename\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The name of the filename corresponding to the relevant document\"\n",
    "                        },\n",
    "                        \"query\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The decomposed query\"\n",
    "                        }                    },\n",
    "                    \"required\": [\"filename\", \"query\"]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement tool functions\n",
    "\n",
    "Now we'll implement the functions that our tools will use to retrieve information.\n",
    "\n",
    "Here's how the process works:\n",
    "\n",
    "1. The LLM analyzes the user's request and formulates a plan using the available tools.\n",
    "2. When the LLM determines that a tool should be used, it doesn't execute the function itself. Instead, it indicates which tool should be used and with what parameters.\n",
    "3. The agent framework (which is separate from the LLM) then executes the corresponding function and returns the result.\n",
    "4. The LLM receives the result of the tool execution and uses this information to continue its plan or formulate a response to the user.\n",
    "5. This process of planning, tool use, and result interpretation continues until the LLM can generate a final answer without needing additional tool use.\n",
    "\n",
    "This combination allows for a realistic simulation of a production environment while still providing controlled data for certain operations.\n",
    "Remember, the LLM's role is to understand the user's request, plan the necessary steps using these tools, and interpret the results to provide a coherent response to the user. The actual execution of these functions is handled by the agent framework, creating a powerful and flexible agentic RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Load variables saved in prior Notebook\n",
    " \n",
    " At the end of Notebook 1 we saved several variables that are needed to continue. The following cell will load those variables into this lab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_filename(text):\n",
    "    response = bedrock_agent_runtime.retrieve(\n",
    "        knowledgeBaseId=summaries_kb_id,\n",
    "        retrievalConfiguration={\n",
    "            \"vectorSearchConfiguration\": {\n",
    "                \"numberOfResults\": 5\n",
    "            }\n",
    "        },\n",
    "        retrievalQuery={\n",
    "            'text': text\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def process_query(text, filename):\n",
    "    metadata_filter = construct_metadata_filter(filename)\n",
    "    print('Here is the prepared metadata filters:')\n",
    "    print(metadata_filter)\n",
    "\n",
    "    response = bedrock_agent_runtime.retrieve(\n",
    "        knowledgeBaseId=kb_id,\n",
    "        retrievalConfiguration={\n",
    "            \"vectorSearchConfiguration\": {\n",
    "                \"filter\": metadata_filter,\n",
    "                \"numberOfResults\": 5\n",
    "            }\n",
    "        },\n",
    "        retrievalQuery={\n",
    "            'text': text\n",
    "        }\n",
    "    )\n",
    "    return response\n",
    "\n",
    "def construct_metadata_filter(filename):\n",
    "    if not filename:\n",
    "        return None\n",
    "    metadata_filter = {\"equals\": []}\n",
    "\n",
    "    if filename and filename != 'unknown':\n",
    "        metadata_filter = {\n",
    "            \"equals\": {\n",
    "                \"key\": \"filename\",\n",
    "                \"value\": filename\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return metadata_filter if metadata_filter[\"equals\"] else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the tools\n",
    "\n",
    "Let's test our implemented tools to ensure they're working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"Which are the Retrieval-based Evaluation results for LongLora\"\n",
    "get_filename(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_query(\"Which are the Retrieval-based Evaluation results for LongLora\", \"longlora.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process tool calls and return results\n",
    "\n",
    "We'll create a function to process the tool calls made by Claude and return the appropriate results. The `process_tool_call` function is crucial for bridging the gap between the language model's decisions and the actual execution of tools in our system. Here's why it's important and how it works:\n",
    "\n",
    "### What is a tool call?\n",
    "A tool call is a request made by the language model (Claude 3) to use a specific tool with certain input parameters. It's the model's way of indicating that it needs to use a particular function to gather information or perform an action in response to a user's query.\n",
    "\n",
    "### Why do we need to process tool calls?\n",
    "We need to process tool calls for several reasons:\n",
    "1. The language model doesn't directly execute code or access data sources.\n",
    "2. We need to translate the model's high-level requests into actual function calls in our system.\n",
    "3. It allows us to control and monitor what actions are being taken on behalf of the model.\n",
    "4. We can add error handling, logging, or additional logic as needed.\n",
    "\n",
    "### How the model selects tools\n",
    "The model selects tools based on its understanding of:\n",
    "- The user's query\n",
    "- The available tools and their descriptions\n",
    "- The current context of the conversation\n",
    "- What information it needs to answer the query or perform the requested task\n",
    "\n",
    "The model uses its reasoning to determine which tool is most appropriate for gathering the necessary information or performing the required action.\n",
    "\n",
    "### The process_tool_call function\n",
    "This function acts as a bridge between the model's high-level tool requests and the actual function calls in our system. It:\n",
    "- Takes the tool name and input parameters as arguments\n",
    "- Maps these to the corresponding Python functions we defined earlier\n",
    "- Calls the appropriate function with the given inputs\n",
    "- Returns the result back to the model\n",
    "\n",
    "This setup allows the language model to make decisions about what information or actions are needed, while keeping the actual execution of these actions under the control of our system. It's a way of giving the model access to external data and capabilities without giving it direct control over the system's resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_tool_call(tool_name, tool_input):\n",
    "    if tool_name == \"get_filename\":\n",
    "        return get_filename(tool_input[\"query\"])\n",
    "    elif tool_name == \"process_query\":\n",
    "        return process_query(tool_input[\"query\"], tool_input[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interact with the chatbot\n",
    "\n",
    "Now, we'll create a function to manage the interaction between the user and our agentic RAG. This function will encapsulate the core logic of our agent, handling the flow of information between the user, the language model (Claude 3), and our defined tools.\n",
    "\n",
    "### Agent's Logic Flow\n",
    "\n",
    "1. **Initialization**: \n",
    "   - Set up the initial context and system prompt for the agent.\n",
    "   - This prompt will guide the agent's behavior.\n",
    "\n",
    "2. **User Input**: \n",
    "   - Receive the user's message or query.\n",
    "\n",
    "3. **LLM Processing**:\n",
    "   - Send the user's input, along with the current context and system prompt, to Claude.\n",
    "   - Claude analyzes the input and determines the next action (either responding directly or using a tool or a set of tools).\n",
    "\n",
    "4. **Tool Execution (if needed)**:\n",
    "   - If Claude decides to use a tool, our function will:\n",
    "     a. Extract the tool name and parameters from Claude's response.\n",
    "     b. Call the `process_tool_call` function to execute the appropriate tool.\n",
    "     c. Capture the tool's output.\n",
    "\n",
    "5. **Result Interpretation**:\n",
    "   - Send the tool's output back to Claude for interpretation.\n",
    "   - Claude may decide to use another tool or formulate a final response.\n",
    "\n",
    "6. **Response Generation**:\n",
    "   - Once Claude has gathered all necessary information, it generates a final response to the user.\n",
    "\n",
    "7. **Conversation Update**:\n",
    "   - Update the conversation history with the user's input and the agent's response.\n",
    "\n",
    "8. **Repeat**:\n",
    "   - The process repeats for each user input, maintaining context throughout the conversation.\n",
    "\n",
    "### System Prompt\n",
    "\n",
    "The system prompt is crucial as it sets the tone, capabilities, and limitations of our agent. Our system prompt includes:\n",
    "\n",
    "- The agent's role as an AI assistant to provide meaningful responses\n",
    "- A step-by-step guide for the agent to follow when assisting customers\n",
    "- Rules for interaction and information gathering\n",
    "- Guidelines for tool usage and response formulation\n",
    "\n",
    "By encapsulating this logic in a single function, we create a modular and maintainable structure for our chatbot. This allows for easy updates to the agent's behavior and capabilities as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first create the system message variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are an advanced AI assistant, designed to process user queries efficiently using a ReAct (Reasoning and Acting) approach. Your task is to break down complex queries, reason about each step, and utilize appropriate tools to provide accurate and comprehensive responses.\n",
    "\n",
    "Core Process\n",
    "\n",
    "    For each user query, follow these steps:\n",
    "\n",
    "    - Query Analysis and Decomposition\n",
    "    - Metadata Search\n",
    "    - Filtered Information Retrieval\n",
    "    - Response Formulation\n",
    "\n",
    "    After each step, engage in explicit reasoning to justify your actions and plan your next move.\n",
    "\n",
    "Detailed Instructions\n",
    "\n",
    "1. Query Analysis and Decomposition\n",
    "\n",
    "    - Carefully analyze the user's query.\n",
    "    - Break it down into smaller, more specific sub-queries.\n",
    "    - Reformulate each sub-query to optimize for semantic search.\n",
    "\n",
    "    Reasoning: Explain why you decomposed the query as you did and how it will help in the information retrieval process.\n",
    "\n",
    "2. Metadata Search\n",
    "\n",
    "    - Use the metadata Knowledge Base to identify the most relevant filename(s) for the query.\n",
    "    - If multiple filenames are relevant, prioritize them based on their likely relevance.\n",
    "\n",
    "    Reasoning: Justify your choice of filename(s) and explain how they relate to the user's query.\n",
    "\n",
    "3. Filtered Information Retrieval\n",
    "\n",
    "    - Use the process_query tool to retrieve information.\n",
    "    - Apply the identified filename(s) as a filter parameter.\n",
    "    - If multiple sub-queries exist, perform separate retrievals for each.\n",
    "\n",
    "    Reasoning: Explain why the retrieved information is relevant and how it addresses the sub-queries.\n",
    "\n",
    "4. Response Formulation\n",
    "\n",
    "    - Synthesize the retrieved information into a coherent response.\n",
    "    - Ensure your answer directly addresses the user's original query.\n",
    "    - If any aspects of the query remain unanswered, acknowledge this and explain why.\n",
    "\n",
    "    Reasoning: Justify how your response addresses the user's query and identify any potential gaps or areas for further exploration.\n",
    "\n",
    "5. Comprehensive Answer Compilation\n",
    "\n",
    "    - Before finalizing your response, review all subqueries and their corresponding answers.\n",
    "    - Ensure that each subquery has been addressed in your final response.\n",
    "    - If any subquery remains unanswered, explicitly state this and explain why (e.g., lack of information, ambiguity in the query).\n",
    "    - Organize your response to clearly address each part of the original query, using subheadings if necessary for clarity.\n",
    "\n",
    "    Reasoning: Explain how your final response comprehensively addresses all aspects of the user's original query, referencing each subquery explicitly.\n",
    "\n",
    "Key Principles\n",
    "\n",
    "- Always start with query decomposition, even for seemingly simple queries.\n",
    "- Continuously refine and reformulate sub-queries to enhance semantic search effectiveness.\n",
    "- Use explicit reasoning after each step to justify your actions and plan subsequent steps.\n",
    "- Prioritize relevance and accuracy in your information retrieval and response formulation.\n",
    "- Be transparent about the process you're following and any limitations encountered.\n",
    "\n",
    "By adhering to this ReAct approach, you will provide users with well-reasoned, accurate, and comprehensive responses while demonstrating your thought process throughout the interaction.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's create the main function chatbot interaction. This function will accept as arguments the user message and the chat history if available. Including the chat history is very important in order for the model to have the entire context before deciding the next step.\n",
    "\n",
    "## Chatbot Interaction Function\n",
    "\n",
    "This function encapsulates the core logic of our chatbot interaction:\n",
    "\n",
    "1. **Initialization**: It sets up the conversation with the user's message and any existing chat history by extending the messages list.\n",
    "\n",
    "2. **Initial Planning**: It makes a call to a larger language model (`MODEL_NAME_1`) for comprehensive response planning.\n",
    "\n",
    "3. **Tool Use Loop**: If tools are required, it enters a loop where it:\n",
    "   a. Extracts tool use information\n",
    "   b. Processes the tool call\n",
    "   c. Prepares the result for the next model call\n",
    "   d. Calls a smaller, quicker model (`MODEL_NAME_2`) for subsequent interactions\n",
    "\n",
    "4. **Iteration**: This loop continues until no more tool use is required.\n",
    "\n",
    "5. **Response Generation**: Finally, it extracts the final response text and returns it along with the updated message history.\n",
    "\n",
    "### Optimization Strategy\n",
    "\n",
    "The use of two different models (a larger one for initial planning and a smaller one for subsequent interactions) is an optimization technique. This approach aims to balance between:\n",
    "\n",
    "- Comprehensive planning\n",
    "- Quick responses\n",
    "- Cost reduction\n",
    "- Improved response times\n",
    "\n",
    "By using a larger model for initial planning and a smaller, faster model for follow-up actions, we can potentially achieve better performance and efficiency in our chatbot interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def chatbot_interaction(user_message, chat_history=None):\n",
    "    print(f\"\\n{'='*50}\\nUser Message: {user_message}\\n{'='*50}\")\n",
    "\n",
    "    messages = []\n",
    "    if chat_history:\n",
    "        messages.extend(chat_history)\n",
    "    messages.append({\"role\": \"user\", \"content\": [{\"text\": user_message}]})\n",
    "\n",
    "    response = client.converse(\n",
    "        modelId=MODEL_NAME_1,\n",
    "        inferenceConfig={\n",
    "            'maxTokens': 4096,\n",
    "            'temperature': 0,\n",
    "        },\n",
    "        messages=messages,\n",
    "        system=[\n",
    "            {\n",
    "                 'text': system_message\n",
    "            },\n",
    "        ],\n",
    "        toolConfig={\"tools\": tools}\n",
    "    )\n",
    "\n",
    "    print(f\"\\nInitial Response:\")\n",
    "    print(f\"Stop Reason: {response['stopReason']}\")\n",
    "    print(f\"Content: {response['output']['message']['content']}\")\n",
    "\n",
    "    while response['stopReason'] == \"tool_use\":\n",
    "        tool_use = next(block for block in response['output']['message']['content'] if isinstance(block, dict) and 'toolUse' in block)\n",
    "        tool_name = tool_use['toolUse']['name']\n",
    "        tool_input = tool_use['toolUse']['input']\n",
    "        tool_use_id = tool_use['toolUse']['toolUseId']\n",
    "\n",
    "        print(f\"\\nTool Used: {tool_name}\")\n",
    "        print(f\"Tool Input:\")\n",
    "        print(json.dumps(tool_input, indent=2))\n",
    "\n",
    "        tool_result = process_tool_call(tool_name, tool_input)\n",
    "\n",
    "        print(f\"\\nTool Result:\")\n",
    "        print(json.dumps(tool_result, indent=2))\n",
    "\n",
    "        messages_temp = [\n",
    "            {\"role\": \"assistant\", \"content\": response['output']['message']['content']},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"toolResult\": {\n",
    "                            \"toolUseId\": tool_use_id, \n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"json\": {\n",
    "                                        \"result\": tool_result\n",
    "                                    }\n",
    "                                }\n",
    "                            ]\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        messages.extend(messages_temp)\n",
    "\n",
    "        response = client.converse(\n",
    "            modelId=MODEL_NAME_2,\n",
    "            inferenceConfig={\n",
    "                'maxTokens': 4096,\n",
    "                'temperature': 0,\n",
    "            },\n",
    "            messages=messages,\n",
    "            system=[\n",
    "                {\n",
    "                    'text': system_message\n",
    "                },\n",
    "            ],\n",
    "            toolConfig={\"tools\": tools}\n",
    "        )\n",
    "\n",
    "        print(f\"\\nResponse:\")\n",
    "        print(f\"Stop Reason: {response['stopReason']}\")\n",
    "        print(f\"Content: {response['output']['message']['content']}\")\n",
    "\n",
    "    final_response = next(\n",
    "        (block['text'] for block in response['output']['message']['content'] if 'text' in block),\n",
    "        None,\n",
    "    )\n",
    "\n",
    "    if not final_response:\n",
    "        final_response = None\n",
    "\n",
    "    print(f\"\\nFinal Response: {final_response}\")\n",
    "\n",
    "    messages.append({\"role\": \"assistant\", \"content\": [{\"text\": final_response}]})\n",
    "\n",
    "    return final_response, messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better understanding, see the flowchart for function chatbot_interaction:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"static/function-flowchart.png\" alt=\"Flowchart of the function\" width=\"70%\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the chatbot\n",
    "\n",
    "Let's test our agent with a few sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# For subsequent iterations\n",
    "user_message = \"Hello!\"\n",
    "final_response, chat_history = chatbot_interaction(user_message, chat_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_message = \"Compare and contrast the LoRA papers (LongLoRA, LoftQ) and the metagpt paper. Analyze the approach in each paper first.\"\n",
    "final_response, chat_history = chatbot_interaction(user_message, chat_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a chatbot widget\n",
    "\n",
    "Let's create an interactive widget for our chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "def handle_user_input(user_message):\n",
    "    global chat_history\n",
    "\n",
    "    response, messages = chatbot_interaction(user_message, chat_history)\n",
    "\n",
    "    chat_history = messages\n",
    "\n",
    "    with chat_output:\n",
    "        clear_output()\n",
    "        print(f\"Response: {response}\")\n",
    "\n",
    "    user_input.value = ''\n",
    "\n",
    "def handle_button_click(sender):\n",
    "    handle_user_input(user_input.value)\n",
    "\n",
    "user_input = widgets.Text(\n",
    "    placeholder='Type your message here...',\n",
    "    description='User:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    on_submit=handle_user_input\n",
    ")\n",
    "\n",
    "chat_output = widgets.Output()\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "send_button = widgets.Button(description='Send')\n",
    "send_button.on_click(handle_button_click)\n",
    "\n",
    "display(widgets.HBox([user_input, send_button]))\n",
    "print(\"\\n\")\n",
    "display(chat_output)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now interact with the chatbot using the widget above. Try asking questions about the LoRA papers or the MetaGPT paper, such as:\n",
    "\n",
    "- \"Compare and contrast the LoRA papers (LongLoRA, LoftQ) and the one from metagpt. Analyze the approach in each paper first.\"\n",
    "- \"What are the evaluation metrics used in each study?\"\n",
    "- \"Which are the Retrieval-based Evaluation results for LongLora?\"\n",
    "\n",
    "Remember to run the above cell if you want to start a new conversation from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell to delete the created resources and avoid unnecesary costs. This should take about 2-3 minutes to complete. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# First, set up the session with the correct profile\n",
    "session = boto3.Session()\n",
    "\n",
    "# Now, create all clients using this session\n",
    "s3_client = session.client('s3')\n",
    "cloudformation = session.client('cloudformation')\n",
    "\n",
    "# Delete all objects in the bucket\n",
    "try:\n",
    "    response = s3_client.list_objects_v2(Bucket=s3_bucket)\n",
    "    if 'Contents' in response:\n",
    "        for obj in response['Contents']:\n",
    "            s3_client.delete_object(Bucket=s3_bucket, Key=obj['Key'])\n",
    "        print(f\"All objects in {s3_bucket} have been deleted.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting objects from {s3_bucket}: {e}\")\n",
    "\n",
    "time.sleep(60) # Wait until the objects have been deleted\n",
    "\n",
    "# Define the stack names to delete\n",
    "stack_names = [\"KB-E2E-KB-{}\".format(solution_id),\"KB-E2E-Base-{}\".format(solution_id)]\n",
    "\n",
    "# Iterate over the stack names and delete each stack\n",
    "for stack_name in stack_names:\n",
    "    try:\n",
    "        # Retrieve the stack information\n",
    "        stack_info = cloudformation.describe_stacks(StackName=stack_name)\n",
    "        stack_status = stack_info['Stacks'][0]['StackStatus']\n",
    "\n",
    "        # Check if the stack exists and is in a deletable state\n",
    "        if stack_status != 'DELETE_COMPLETE':\n",
    "            # Delete the stack\n",
    "            cloudformation.delete_stack(StackName=stack_name)\n",
    "            print(f'Deleting stack: {stack_name}')\n",
    "\n",
    "            # Wait for the stack deletion to complete\n",
    "            waiter = cloudformation.get_waiter('stack_delete_complete')\n",
    "            waiter.wait(StackName=stack_name)\n",
    "            print(f'Stack {stack_name} deleted successfully.')\n",
    "        else:\n",
    "            print(f'Stack {stack_name} does not exist or has already been deleted.')\n",
    "\n",
    "    except cloudformation.exceptions.ClientError as e:\n",
    "        print(f'Error deleting stack {stack_name}: {e.response[\"Error\"][\"Message\"]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
