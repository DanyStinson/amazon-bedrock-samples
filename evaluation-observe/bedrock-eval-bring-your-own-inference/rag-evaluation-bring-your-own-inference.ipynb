{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7b6d3af-6b1d-48b2-8d0e-2723cc6fef4f",
   "metadata": {},
   "source": [
    "# RAG Evaluation with Bring Your Own Inference Responses (BYOI) on Amazon Bedrock\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon Bedrock RAG Evaluation capabilities now support \"Bring Your Own Inference Responses\" (BYOI), enabling you to assess any Retrieval-Augmented Generation system regardless of where it's deployed. This notebook demonstrates how to evaluate the quality of RAG systems using specialized metrics including the newly available citation metrics - Citation Precision and Citation Coverage - providing deep insights into how effectively your system uses retrieved information.\n",
    "\n",
    "Through this guide, we'll explore:\n",
    "- Setting up RAG evaluation configurations with BYOI\n",
    "- The creation of retrieve-and-generate evaluation jobs\n",
    "- Analyzing citation quality with the new precision and coverage metrics\n",
    "- Monitoring evaluation progress \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we begin, make sure you have:\n",
    "- An active AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region\n",
    "- An S3 bucket for storing evaluation data and results\n",
    "- An IAM role with necessary permissions for S3 and Bedrock\n",
    "- RAG system outputs in the required BYOI format\n",
    "\n",
    "> **Important**: The evaluation process requires access to Amazon Bedrock evaluator models. Make sure these are enabled in your account.\n",
    "\n",
    "## Dataset Format for RAG BYOI\n",
    "\n",
    "### Retrieve-and-Generate Evaluation Format\n",
    "```json\n",
    "{\n",
    "  \"conversationTurns\": [\n",
    "    {\n",
    "      \"prompt\": {\n",
    "        \"content\": [\n",
    "          {\n",
    "            \"text\": \"Your prompt here\"\n",
    "          }\n",
    "        ]\n",
    "      },\n",
    "      \"referenceResponses\": [\n",
    "        {\n",
    "          \"content\": [\n",
    "            {\n",
    "              \"text\": \"Expected ground truth answer\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"output\": {\n",
    "        \"text\": \"Generated response text\",\n",
    "        \"knowledgeBaseIdentifier\": \"third-party-RAG\",\n",
    "        \"retrievedPassages\": {\n",
    "          \"retrievalResults\": [\n",
    "            {\n",
    "              \"name\": \"Optional passage name\",\n",
    "              \"content\": {\n",
    "                \"text\": \"Retrieved passage content\"\n",
    "              },\n",
    "              \"metadata\": {\n",
    "                \"source\": \"Optional metadata\"\n",
    "              }\n",
    "            }\n",
    "          ]\n",
    "        },\n",
    "        \"citations\": [\n",
    "          {\n",
    "            \"generatedResponsePart\": {\n",
    "              \"textResponsePart\": {\n",
    "                \"span\": {\n",
    "                  \"start\": 0,\n",
    "                  \"end\": 50\n",
    "                },\n",
    "                \"text\": \"Part of the response that uses cited material\"\n",
    "              }\n",
    "            },\n",
    "            \"retrievedReferences\": [\n",
    "              {\n",
    "                \"name\": \"Optional passage name\",\n",
    "                \"content\": {\n",
    "                  \"text\": \"Source passage for the citation\"\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                  \"source\": \"Optional metadata\"\n",
    "                }\n",
    "              }\n",
    "            ]\n",
    "          }\n",
    "        ]\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "## Implementation\n",
    "\n",
    "First, let's set up our configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f87802-b6fb-4101-8e73-7344c94b7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upgrade Boto3\n",
    "!pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42192fc5-ae68-4ca8-91d4-2dc8fd59d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify boto3 installed successfully\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4df8d49-7e2a-4302-a91a-24d7dd2ff4e4",
   "metadata": {},
   "source": [
    "To use the Python SDK for creating an RAG evaluation job with your own inference responses, use the following steps. First, set up the required configurations, which should include your model identifier for the evaluator, IAM role with appropriate permissions, S3 paths for input data containing your inference responses, and output location for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a680fc-1899-4afa-ab42-1bb67e4fbad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure knowledge base and model settings\n",
    "evaluator_model = \"<YOUR_EVALUATOR_MODEL>\"\n",
    "role_arn = \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>\"\n",
    "BUCKET_NAME = \"<YOUR_S3_BUCKET_NAME>\"\n",
    "PREFIX = \"<YOUR_BUCKET_PREFIX>\"\n",
    "RAG_dataset_custom_name = \"<YOUR_RAG_BYOI_DATASET_NAME>\" # without the \".jsonl file extension\n",
    "\n",
    "# Specify S3 locations\n",
    "input_data = f\"s3://{BUCKET_NAME}/{PREFIX}/{RAG_dataset_custom_name}.jsonl\"\n",
    "output_path = f\"s3://{BUCKET_NAME}/{PREFIX}/\"\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client('bedrock')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712a72a0-ec73-47c0-8cef-e35917d215af",
   "metadata": {},
   "source": [
    "## Configuring a Retrieve and Generate RAG Evaluation Job with BYOI\n",
    "\n",
    "The code below creates an evaluation job that analyzes both retrieval and generation quality from your RAG system. The most significant aspect is the `precomputedRagSourceConfig` parameter, which enables the Bring Your Own Inference capability. This configuration tells Bedrock to evaluate pre-generated responses rather than generating new ones.\n",
    "\n",
    "Note how we're configuring a rich set of evaluation metrics, including the new citation metrics:\n",
    "\n",
    "- **CitationPrecision**: Measures how accurately your RAG system cites sources by evaluating whether cited passages actually contain the information used in the response\n",
    "- **CitationCoverage**: Evaluates how well the response's content is supported by its citations, focusing on whether all information derived from retrieved passages has been properly cited\n",
    "\n",
    "The `ragSourceIdentifier` parameter must match the identifier in your dataset (in this example, \"third-party-RAG\"), creating the link between your evaluation configuration and the responses you've provided. The job will analyze your RAG system's performance across multiple dimensions, providing comprehensive insights into both information retrieval accuracy and generation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b02ad8-e289-4b0d-90db-7a9741c96cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve_generate_job_name = f\"rag-evaluation-generate-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "retrieve_generate_job = bedrock_client.create_evaluation_job(\n",
    "    jobName=retrieve_generate_job_name,\n",
    "    jobDescription=\"Evaluate retrieval and generation\",\n",
    "    roleArn=role_arn,\n",
    "    applicationType=\"RagEvaluation\",\n",
    "    inferenceConfig={\n",
    "        \"ragConfigs\": [\n",
    "            {\n",
    "                \"precomputedRagSourceConfig\": {\n",
    "                    \"retrieveAndGenerateSourceConfig\": {\n",
    "                        \"ragSourceIdentifier\": \"third-party-RAG\"  # Replace with your identifier\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\": output_path\n",
    "    },\n",
    "    evaluationConfig={\n",
    "        \"automated\": {\n",
    "            \"datasetMetricConfigs\": [{\n",
    "                \"taskType\": \"QuestionAndAnswer\",  \n",
    "                \"dataset\": {\n",
    "                    \"name\": \"RagDataset\",\n",
    "                    \"datasetLocation\": {\n",
    "                        \"s3Uri\": input_data\n",
    "                    }\n",
    "                },\n",
    "                \"metricNames\": [\n",
    "                    \"Builtin.Correctness\",\n",
    "                    \"Builtin.Completeness\",\n",
    "                    \"Builtin.Helpfulness\",\n",
    "                    \"Builtin.LogicalCoherence\",\n",
    "                    \"Builtin.Faithfulness\",\n",
    "                    \"Builtin.CitationPrecision\",\n",
    "                    \"Builtin.CitationCoverage\"\n",
    "                ]\n",
    "            }],\n",
    "            \"evaluatorModelConfig\": {\n",
    "                \"bedrockEvaluatorModels\": [{\n",
    "                    \"modelIdentifier\": evaluator_model\n",
    "                }]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b1e899-5f0d-41d2-a47b-ea88b678d3db",
   "metadata": {},
   "source": [
    "## Monitoring Your RAG Evaluation Jobs\n",
    "\n",
    "After submitting your evaluation jobs, you'll want to monitor their progress. The code below demonstrates how to check the status of both job types:\n",
    "\n",
    "You can run this code periodically to track your job's progress through its lifecycle. Typical status values include \"IN_PROGRESS\", \"COMPLETED\", or \"FAILED\". Once a job reaches \"COMPLETED\" status, you can proceed to retrieve and analyze the evaluation results from the S3 output location you specified when creating the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8713b284-d85b-4344-a8fe-8df94d642109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check status of retrieve-and-generate job\n",
    "retrieve_generate_job_arn = retrieve_generate_job['jobArn']\n",
    "retrieve_generate_status = bedrock_client.get_evaluation_job(jobIdentifier=retrieve_generate_job_arn)\n",
    "print(f\"Retrieve-and-Generate Job Status: {retrieve_generate_status['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9719d49-2d36-4398-9e52-4c448cb65b1f",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this guide, we've explored how to leverage Amazon Bedrock RAG Evaluation capabilities with Bring Your Own Inference Responses to assess any RAG system's performance. Key advantages of this approach include:\n",
    "\n",
    "- **Platform independence**: Evaluate RAG systems deployed anywhere - on Amazon Bedrock, other cloud providers, or on-premises\n",
    "- **Comprehensive assessment**: Analyze both retrieve and generate quality with specialized metrics\n",
    "- **Citation quality insights**: Leverage the new citation metrics to ensure responses are properly grounded in source information\n",
    "- **Systematic benchmarking**: Compare different RAG implementations to make data-driven optimization decisions\n",
    "\n",
    "By implementing regular evaluation workflows using these capabilities, you can continuously improve your RAG systems to deliver more accurate, relevant, and well-attributed responses. Whether you're fine-tuning retrieval strategies, optimizing prompt engineering, or exploring different foundation models for generation, these evaluation tools provide the quantitative insights needed to guide your development process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
