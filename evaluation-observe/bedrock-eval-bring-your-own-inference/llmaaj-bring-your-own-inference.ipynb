{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22e78c0b-89f5-4835-b29a-791d2b87f2a3",
   "metadata": {},
   "source": [
    "# LLM-as-a-Judge with Bring Your Own Inference Responses on Amazon Bedrock\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon Bedrock Model Evaluation capabilities now support \"Bring Your Own Inference responses\" (BYOI), allowing you to evaluate any model's outputs regardless of where they're hosted. This notebook demonstrates how to use LLM-as-a-Judge (LLMaJ) to evaluate model responses from any source - whether from other foundation model providers or your own deployed solutions.\n",
    "\n",
    "Through this guide, we'll explore:\n",
    "- Setting up evaluation configurations with BYOI\n",
    "- Creating and configuring LLM-as-a-Judge evaluation jobs\n",
    "- Monitoring evaluation progress and interpreting results\n",
    "- Analyzing model performance across various dimensions\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we begin, make sure you have:\n",
    "- An active AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region\n",
    "- An S3 bucket for storing evaluation data and results\n",
    "- An IAM role with necessary permissions for S3 and Bedrock\n",
    "- Model responses in the required BYOI format\n",
    "\n",
    "> **Important**: The evaluation process requires access to Amazon Bedrock evaluator models. Make sure these are enabled in your account.\n",
    "\n",
    "## Dataset Format for BYOI\n",
    "\n",
    "The evaluation data must follow specific JSON format:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"prompt\": \"What is the discount amount for a product with original price $80 and a 25% discount?\",\n",
    "    \"referenceResponse\": \"The discount amount is $20.\",\n",
    "    \"category\": \"Discount Calculation\",\n",
    "    \"modelResponses\": [\n",
    "        {\n",
    "            \"response\": \"To calculate the discount amount: $80 × 25% = $80 × 0.25 = $20. The discount amount is $20.\",\n",
    "            \"modelIdentifier\": \"third-party-model\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "## Dataset Requirements\n",
    "\n",
    "### Job Requirements\n",
    "- Each evaluation job can evaluate one model at a time\n",
    "- Maximum 1000 prompts per evaluation job\n",
    "\n",
    "### Data Structure Requirements\n",
    "- Must include `prompt` and `modelResponses` fields\n",
    "- `modelIdentifier` in the response must match your source configuration\n",
    "- `referenceResponse` is optional but recommended for most metrics\n",
    "- `category` is optional for classification of results\n",
    "\n",
    "> **Note**: When preparing your dataset, ensure your model identifier is consistent across all entries and matches the identifier you'll use when configuring the evaluation job.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "Let's set up our configuration parameters to get started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99762c98-6d33-4342-b9a2-54dc8afd1617",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Upgrade Boto3\n",
    "!pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6e53e9-d235-4429-b2ed-6e1d0867e545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify boto3 installed successfully\n",
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fbdae0-5530-4bbb-8674-fdb3f993e8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import random\n",
    "from datetime import datetime\n",
    "import botocore\n",
    "import time\n",
    "\n",
    "\n",
    "# AWS Configuration\n",
    "REGION = \"us-east-1\"\n",
    "ROLE_ARN = \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>\"\n",
    "BUCKET_NAME = \"<YOUR_S3_BUCKET_NAME>\"\n",
    "PREFIX = \"<YOUR_BUCKET_PREFIX>\"\n",
    "dataset_custom_name = \"<YOUR_BYOI_DATASET_NAME\"\n",
    "\n",
    "# Initialize AWS clients\n",
    "bedrock_client = boto3.client('bedrock', region_name=REGION)\n",
    "s3_client = boto3.client('s3', region_name=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de4007-e659-41a0-bc8a-730b7721c286",
   "metadata": {},
   "source": [
    "## OPTIONAL - Generating Synthetic Data for BYOI Demonstration\n",
    "\n",
    "To demonstrate the BYOI evaluation capability without requiring actual third-party model integration, we'll generate a synthetic dataset of shopping math problems. The code below uses Amazon Bedrock's Nova Lite model to simulate responses from a third-party model. This approach creates a realistic evaluation scenario while keeping the demonstration self-contained. Each problem involves discount calculations with a reference answer and a model-generated response, structured according to the required BYOI format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b452a87-fdd0-418b-8519-71dea4ecde1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shopping_problems(num_problems=30):\n",
    "    client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "    items = [\"apples\", \"oranges\", \"bananas\", \"books\", \"pencils\", \"notebooks\"]\n",
    "    problems = []\n",
    "    \n",
    "    for i in range(num_problems):\n",
    "        # Generate problem data\n",
    "        item = random.choice(items)\n",
    "        quantity = random.randint(3, 20)\n",
    "        price_per_item = round(random.uniform(1.5, 15.0), 2)\n",
    "        discount_percent = random.choice([10, 15, 20, 25, 30])\n",
    "        \n",
    "        # Calculate answer\n",
    "        total = quantity * price_per_item\n",
    "        discount = total * (discount_percent / 100)\n",
    "        final = round(total - discount, 2)\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"If {item} cost \\${price_per_item} each and you buy {quantity} of them with a {discount_percent}% discount, how much will you pay in total?\"\n",
    "        \n",
    "        # Get Nova response\n",
    "        try:\n",
    "            body = json.dumps({\n",
    "                \"schemaVersion\": \"messages-v1\",\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],\n",
    "                \"inferenceConfig\": {\"maxTokens\": 300, \"temperature\": 0.3, \"topP\": 0.1, \"topK\": 20}\n",
    "            })\n",
    "            \n",
    "            response = client.invoke_model(\n",
    "                body=body,\n",
    "                modelId=\"us.amazon.nova-lite-v1:0\",\n",
    "                accept='application/json',\n",
    "                contentType='application/json'\n",
    "            )\n",
    "            \n",
    "            # Correct way to handle the response body\n",
    "            response_body = json.loads(response.get('body').read())\n",
    "            model_response = response_body[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "        except Exception as e:\n",
    "            model_response = f\"Error calculating discount price: {str(e)[:50]}...\"\n",
    "        \n",
    "        # Create problem object\n",
    "        problems.append({\n",
    "            \"prompt\": prompt,\n",
    "            \"referenceResponse\": f\"The total price will be \\${final}. Original price: \\${total} minus {discount_percent}% discount (\\${discount})\",\n",
    "            \"category\": \"Shopping Math\",\n",
    "            \"modelResponses\": [{\"response\": model_response, \"modelIdentifier\": \"third-party-model\"}]\n",
    "        })\n",
    "        \n",
    "        if i < num_problems - 1:\n",
    "            time.sleep(0.5)\n",
    "    \n",
    "    return problems\n",
    "\n",
    "# Generate problems and save to JSONL\n",
    "dataset_custom_name = \"dummy-data-BYOI\"\n",
    "problems = generate_shopping_problems()\n",
    "with open(f\"{dataset_custom_name}.jsonl\", 'w') as f:\n",
    "    for problem in problems:\n",
    "        f.write(json.dumps(problem) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b223183e-c64b-4baf-a588-15bef25b9035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_s3(local_file: str, bucket: str, s3_key: str) -> bool:\n",
    "    \"\"\"\n",
    "    Upload a file to S3 with error handling.\n",
    "    \n",
    "    Returns:\n",
    "        bool: Success status\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client.upload_file(local_file, bucket, s3_key)\n",
    "        print(f\"✓ Successfully uploaded to s3://{bucket}/{s3_key}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error uploading to S3: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Upload dataset\n",
    "s3_key = f\"{PREFIX}/{dataset_custom_name}.jsonl\"\n",
    "upload_success = upload_to_s3(f\"{dataset_custom_name}.jsonl\", BUCKET_NAME, s3_key)\n",
    "\n",
    "if not upload_success:\n",
    "    raise Exception(\"Failed to upload dataset to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215432b2-0328-4882-953c-5f42e43420db",
   "metadata": {},
   "source": [
    "## Creating an LLM-as-a-Judge Evaluation Job\n",
    "\n",
    "Now that we have our dataset prepared and uploaded to S3, we need to create the evaluation job that will assess our model responses. The function below handles the creation of an LLM-as-a-judge evaluation job through the Bedrock API. This function configures all aspects of the evaluation, including selecting which metrics to evaluate, specifying the evaluator model that will act as judge, and most importantly, setting up the `precomputedInferenceSource` parameter that enables the Bring Your Own Inference capability. You can customize this function to select specific metrics relevant to your use case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec1224-a994-4ab3-9e61-fd49245055f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_judge_evaluation(\n",
    "    client,\n",
    "    job_name: str,\n",
    "    role_arn: str,\n",
    "    input_s3_uri: str,\n",
    "    output_s3_uri: str,\n",
    "    evaluator_model_id: str,\n",
    "    dataset_name: str = None,\n",
    "    task_type: str = \"General\" # must be General for LLMaaJ\n",
    "):    \n",
    "    # All available LLM-as-judge metrics\n",
    "    llm_judge_metrics = [\n",
    "        \"Builtin.Correctness\",\n",
    "        \"Builtin.Completeness\", \n",
    "        \"Builtin.Faithfulness\",\n",
    "        \"Builtin.Helpfulness\",\n",
    "        \"Builtin.Coherence\",\n",
    "        \"Builtin.Relevance\",\n",
    "        \"Builtin.FollowingInstructions\",\n",
    "        \"Builtin.ProfessionalStyleAndTone\",\n",
    "        \"Builtin.Harmfulness\",\n",
    "        \"Builtin.Stereotyping\",\n",
    "        \"Builtin.Refusal\"\n",
    "    ]\n",
    "\n",
    "    # Configure dataset\n",
    "    dataset_config = {\n",
    "        \"name\": dataset_name or \"CustomDataset\",\n",
    "        \"datasetLocation\": {\n",
    "            \"s3Uri\": input_s3_uri\n",
    "        }\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = client.create_evaluation_job(\n",
    "            jobName=job_name,\n",
    "            roleArn=role_arn,\n",
    "            applicationType=\"ModelEvaluation\",\n",
    "            evaluationConfig={\n",
    "                \"automated\": {\n",
    "                    \"datasetMetricConfigs\": [\n",
    "                        {\n",
    "                            \"taskType\": task_type,\n",
    "                            \"dataset\": dataset_config,\n",
    "                            \"metricNames\": llm_judge_metrics\n",
    "                        }\n",
    "                    ],\n",
    "                    \"evaluatorModelConfig\": {\n",
    "                        \"bedrockEvaluatorModels\": [\n",
    "                            {\n",
    "                                \"modelIdentifier\": evaluator_model_id\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            inferenceConfig={\n",
    "                \"models\": [\n",
    "                    {\n",
    "                        \"precomputedInferenceSource\": {\n",
    "                            \"inferenceSourceIdentifier\": \"third-party-model\"\n",
    "                        }\n",
    "                    }\n",
    "                ]\n",
    "            },\n",
    "            outputDataConfig={\n",
    "                \"s3Uri\": output_s3_uri\n",
    "            }\n",
    "        )\n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating evaluation job: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9a115-089d-4424-be37-39013da98e1d",
   "metadata": {},
   "source": [
    "## Executing the Evaluation Job\n",
    "\n",
    "The code below launches your evaluation workflow, selecting an appropriate evaluator model from Amazon Bedrock and configuring the job with your dataset of third-party model responses. When successful, the job ARN is returned, allowing you to track progress and access results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5062eb-c827-43f3-99c6-d6a9bd6e590c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job Configuration\n",
    "evaluator_model = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "job_name = f\"llmaaj-third-party-model-{evaluator_model.split('.')[0]}-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# S3 Paths\n",
    "input_data = f\"s3://{BUCKET_NAME}/{PREFIX}/{dataset_custom_name}.jsonl\"\n",
    "output_path = f\"s3://{BUCKET_NAME}/{PREFIX}\"\n",
    "\n",
    "# Create evaluation job\n",
    "try:\n",
    "    llm_as_judge_response = create_llm_judge_evaluation(\n",
    "        client=bedrock_client,\n",
    "        job_name=job_name,\n",
    "        role_arn=ROLE_ARN,\n",
    "        input_s3_uri=input_data,\n",
    "        output_s3_uri=output_path,\n",
    "        evaluator_model_id=evaluator_model,\n",
    "        task_type=\"General\"\n",
    "    )\n",
    "    print(f\"✓ Created evaluation job: {llm_as_judge_response['jobArn']}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Failed to create evaluation job: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8fdfc3-de06-4e0b-867d-e7a89c001019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job ARN based on job type\n",
    "evaluation_job_arn = llm_as_judge_response['jobArn']\n",
    "\n",
    "# Check job status\n",
    "check_status = bedrock_client.get_evaluation_job(jobIdentifier=evaluation_job_arn) \n",
    "print(f\"Job Status: {check_status['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe2d41d-3a28-4d10-a321-1e2365ffb470",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this guide, we've demonstrated how to leverage Amazon Bedrock Evaluation LLM-as-a-Judge with Bring Your Own Inference capabilities to evaluate any model's outputs, regardless of source. Key benefits include:\n",
    "\n",
    "- Platform-agnostic evaluation that works with any model or AI system\n",
    "- Comprehensive assessment across multiple quality dimensions simultaneously\n",
    "- Consistent benchmarking framework for comparing different AI implementations\n",
    "- Scalable approach for evaluating hundreds or thousands of model responses\n",
    "\n",
    "By implementing regular evaluation workflows with BYOI, you can make data-driven decisions about model selection, fine-tuning, and deployment across your entire AI portfolio, whether running on Amazon Bedrock or elsewhere."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
