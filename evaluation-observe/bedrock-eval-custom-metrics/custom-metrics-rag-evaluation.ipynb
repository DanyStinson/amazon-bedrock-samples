{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56726f49-50c0-4e99-9481-4775547c5335",
   "metadata": {},
   "source": [
    "# RAG Evaluation with Custom Metrics on Amazon Bedrock\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon Bedrock Evaluations now supports Custom Metrics for RAG (Retrieval-Augmented Generation) systems, enabling you to define specialized evaluation criteria tailored to your specific needs. This notebook demonstrates how to create and implement custom metrics for your RAG evaluation jobs, allowing you to measure unique aspects of your RAG system's performance beyond the built-in metrics.\n",
    "\n",
    "Through this guide, we'll explore:\n",
    "- Creating custom metrics for RAG evaluation with full configuration control\n",
    "- Implementing retrieve-and-generate evaluation jobs with custom metrics\n",
    "- Defining numerical and categorical scoring systems for your custom metrics\n",
    "- Analyzing evaluation results with your specialized metrics alongside built-in metrics\n",
    "- Monitoring evaluation progress and interpreting custom metric results\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before we begin, make sure you have:\n",
    "- An active AWS account with appropriate permissions\n",
    "- Amazon Bedrock access enabled in your preferred region\n",
    "- An S3 bucket for storing evaluation data and results\n",
    "- An IAM role with necessary permissions for S3 and Bedrock\n",
    "- A dataset formatted according to the RAG evaluation requirements\n",
    "\n",
    "> **Important**: The evaluation process requires access to Amazon Bedrock evaluator models. Make sure these are enabled in your account.\n",
    "\n",
    "## Custom Metrics for RAG Evaluation\n",
    "\n",
    "Custom metrics allow you to evaluate specific dimensions of your RAG system's performance beyond the default metrics. For example, you might want to evaluate:\n",
    "- Information comprehensiveness\n",
    "- Knowledge integration fidelity\n",
    "- Information relevance\n",
    "- Brand voice consistency\n",
    "- Domain-specific accuracy criteria\n",
    "\n",
    "Let's implement these custom evaluations using the Amazon Bedrock SDK.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "First, let's set up our configuration parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5160db6b-f65a-4ce9-a09f-ccc4b2c1318f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.11/site-packages (1.36.23)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.37.36-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting botocore<1.38.0,>=1.37.36 (from boto3)\n",
      "  Downloading botocore-1.37.36-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /opt/conda/lib/python3.11/site-packages (from boto3) (0.11.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.11/site-packages (from botocore<1.38.0,>=1.37.36->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /opt/conda/lib/python3.11/site-packages (from botocore<1.38.0,>=1.37.36->boto3) (2.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.38.0,>=1.37.36->boto3) (1.17.0)\n",
      "Downloading boto3-1.37.36-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.37.36-py3-none-any.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "Installing collected packages: botocore, boto3\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.36.23\n",
      "    Uninstalling botocore-1.36.23:\n",
      "      Successfully uninstalled botocore-1.36.23\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.36.23\n",
      "    Uninstalling boto3-1.36.23:\n",
      "      Successfully uninstalled boto3-1.36.23\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.2 requires nvidia-ml-py3==7.352.0, which is not installed.\n",
      "aiobotocore 2.20.0 requires botocore<1.36.24,>=1.36.20, but you have botocore 1.37.36 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.3 requires sqlparse==0.5.0, but you have sqlparse 0.5.3 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires jsonschema<4.22,>=4.18, but you have jsonschema 4.23.0 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires nltk<3.9,>=3.4.5, but you have nltk 3.9.1 which is incompatible.\n",
      "autogluon-multimodal 1.2 requires omegaconf<2.3.0,>=2.1.1, but you have omegaconf 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.37.36 botocore-1.37.36\n"
     ]
    }
   ],
   "source": [
    "#Upgrade Boto3\n",
    "!pip install --upgrade boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16ed4a13-df43-47c0-8c31-9d82bc9c3aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.37.36\n"
     ]
    }
   ],
   "source": [
    "# Verify boto3 installed successfully\n",
    "import boto3\n",
    "print(boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73466974-ee03-432b-958e-6e7564b4e16f",
   "metadata": {},
   "source": [
    "To use the Python SDK for creating an RAG evaluation job with your own inference responses, use the following steps. First, set up the required configurations, which should include your model identifier for the evaluator, IAM role with appropriate permissions, S3 paths for input data containing your inference responses, and output location for results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d16117b-4cc1-49e5-a839-289886bf4845",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate unique name for the job\n",
    "job_name = f\"rag-evaluation-custom-metrics-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# Configure knowledge base and model settings\n",
    "knowledge_base_id = \"<YOUR_KB_ID>\"\n",
    "evaluator_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "generator_model = \"amazon.nova-lite-v1:0\"\n",
    "custom_metrics_evaluator_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "role_arn = \"arn:aws:iam::<YOUR_ACCOUNT_ID>:role/<YOUR_IAM_ROLE>\"\n",
    "BUCKET_NAME = \"<YOUR_BUCKET_NAME>\"\n",
    "\n",
    "# Specify S3 locations\n",
    "input_data = f\"s3://{BUCKET_NAME}/evaluation_data/input.jsonl\"\n",
    "output_path = f\"s3://{BUCKET_NAME}/evaluation_output/\"\n",
    "\n",
    "# Configure retrieval settings\n",
    "num_results = 10\n",
    "search_type = \"HYBRID\"\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client('bedrock', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd29561b-10cf-411f-8624-86c5f3a5b1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Generate unique name for the job\n",
    "job_name = f\"rag-evaluation-custom-metrics-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# Configure knowledge base and model settings\n",
    "knowledge_base_id = \"STCXFRIFPT\"\n",
    "evaluator_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "generator_model = \"amazon.nova-lite-v1:0\"\n",
    "custom_metrics_evaluator_model = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "role_arn = \"arn:aws:iam::968116482887:role/AmazonBedrock\"\n",
    "BUCKET_NAME = \"wale-eval-bucket-us-east-1\"\n",
    "\n",
    "# Specify S3 locations\n",
    "input_data = f\"s3://{BUCKET_NAME}/evaluation_data/rag_dataset_prompt_with_gt.jsonl\"\n",
    "output_path = f\"s3://{BUCKET_NAME}/evaluation_output/\"\n",
    "\n",
    "# Configure retrieval settings\n",
    "num_results = 10\n",
    "search_type = \"HYBRID\"\n",
    "\n",
    "# Create Bedrock client\n",
    "bedrock_client = boto3.client('bedrock', region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bd77cf-8efa-49bf-8fd2-9db716d9e699",
   "metadata": {},
   "source": [
    "## Creating a Retrieval and Generation Evaluation Job with Custom Metrics\n",
    "\n",
    "For this evaluation job, we'll use three key built-in metrics:\n",
    "- `Builtin.Correctness`: Evaluates factual accuracy of generated responses\n",
    "- `Builtin.Completeness`: Assesses if all relevant information is included  \n",
    "- `Builtin.Helpfulness`: Measures how useful the response is\n",
    "\n",
    "Additionally, we'll implement our custom metric:\n",
    "- `information_comprehensiveness`: Evaluates how thoroughly the response utilizes retrieved information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9913ae1-97ce-4abd-a3b3-29bca0d9721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our custom information_comprehensiveness metric\n",
    "information_comprehensiveness_metric = {\n",
    "    \"customMetricDefinition\": {\n",
    "        \"name\": \"information_comprehensiveness\",\n",
    "        \"instructions\": \"\"\"\n",
    "        Your role is to evaluate how comprehensively the response addresses the query using the retrieved information. \n",
    "        Assess whether the response provides a thorough treatment of the subject by effectively utilizing the available retrieved passages.\n",
    "\n",
    "Carefully evaluate the comprehensiveness of the RAG response for the given query against all specified criteria. \n",
    "Assign a single overall score that best represents the comprehensiveness, and provide a brief explanation justifying your rating, referencing specific strengths and weaknesses observed.\n",
    "\n",
    "When evaluating response comprehensiveness, consider the following rubrics:\n",
    "- Coverage: Does the response utilize the key relevant information from the retrieved passages?\n",
    "- Depth: Does the response provide sufficient detail on important aspects from the retrieved information?\n",
    "- Context utilization: How effectively does the response leverage the available retrieved passages?\n",
    "- Information synthesis: Does the response combine retrieved information to create a thorough treatment?\n",
    "\n",
    "Evaluate using the following:\n",
    "\n",
    "Query: {{prompt}}\n",
    "\n",
    "Retrieved passages: {{context}}\n",
    "\n",
    "Response to evaluate: {{prediction}}\n",
    "\"\"\",\n",
    "        \"ratingScale\": [\n",
    "            {\n",
    "                \"definition\": \"Very comprehensive\",\n",
    "                \"value\": {\n",
    "                    \"floatValue\": 3\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"definition\": \"Moderately comprehensive\",\n",
    "                \"value\": {\n",
    "                    \"floatValue\": 2\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"definition\": \"Minimally comprehensive\",\n",
    "                \"value\": {\n",
    "                    \"floatValue\": 1\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"definition\": \"Not at all comprehensive\",\n",
    "                \"value\": {\n",
    "                    \"floatValue\": 0\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f17e5dc-af58-4ebd-8c7d-c3766907e68e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created evaluation job: rag-evaluation-generate-2025-04-18-18-26-22\n",
      "Job ID: arn:aws:bedrock:us-east-1:968116482887:evaluation-job/sr0ocq5n2a6l\n"
     ]
    }
   ],
   "source": [
    "# Create the evaluation job\n",
    "retrieve_generate_job_name = f\"rag-evaluation-generate-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "retrieve_generate_job = bedrock_client.create_evaluation_job(\n",
    "    jobName=retrieve_generate_job_name,\n",
    "    jobDescription=\"Evaluate retrieval and generation with custom metric\",\n",
    "    roleArn=role_arn,\n",
    "    applicationType=\"RagEvaluation\",\n",
    "    inferenceConfig={\n",
    "        \"ragConfigs\": [{\n",
    "            \"knowledgeBaseConfig\": {\n",
    "                \"retrieveAndGenerateConfig\": {\n",
    "                    \"type\": \"KNOWLEDGE_BASE\",\n",
    "                    \"knowledgeBaseConfiguration\": {\n",
    "                        \"knowledgeBaseId\": knowledge_base_id,\n",
    "                        \"modelArn\": generator_model,\n",
    "                        \"retrievalConfiguration\": {\n",
    "                            \"vectorSearchConfiguration\": {\n",
    "                                \"numberOfResults\": num_results\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }]\n",
    "    },\n",
    "    outputDataConfig={\n",
    "        \"s3Uri\": output_path\n",
    "    },\n",
    "    evaluationConfig={\n",
    "        \"automated\": {\n",
    "            \"datasetMetricConfigs\": [{\n",
    "                \"taskType\": \"General\",\n",
    "                \"dataset\": {\n",
    "                    \"name\": \"RagDataset\",\n",
    "                    \"datasetLocation\": {\n",
    "                        \"s3Uri\": input_data\n",
    "                    }\n",
    "                },\n",
    "                \"metricNames\": [\n",
    "                    \"Builtin.Correctness\",\n",
    "                    \"Builtin.Completeness\",\n",
    "                    \"Builtin.Helpfulness\",\n",
    "                    \"information_comprehensiveness\"\n",
    "                ]\n",
    "            }],\n",
    "            \"evaluatorModelConfig\": {\n",
    "                \"bedrockEvaluatorModels\": [{\n",
    "                    \"modelIdentifier\": evaluator_model\n",
    "                }]\n",
    "            },\n",
    "            \"customMetricConfig\": {\n",
    "                \"customMetrics\": [\n",
    "                    information_comprehensiveness_metric\n",
    "                ],\n",
    "                \"evaluatorModelConfig\": {\n",
    "                    \"bedrockEvaluatorModels\": [{\n",
    "                        \"modelIdentifier\": custom_metrics_evaluator_model\n",
    "                    }]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Created evaluation job: {retrieve_generate_job_name}\")\n",
    "print(f\"Job ID: {retrieve_generate_job['jobArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98be3a9d-cb4b-4fe4-a64c-9e6fd6078c31",
   "metadata": {},
   "source": [
    "### Monitoring Job Progress\n",
    "Track the status of your evaluation job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a37c3d15-1c0a-41bb-aa03-b506d216e4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job Status: Completed\n"
     ]
    }
   ],
   "source": [
    "# Get job ARN based on job type\n",
    "evaluation_job_arn = retrieve_generate_job['jobArn']  # or retrieve_generate_job['jobArn']\n",
    "\n",
    "# Check job status\n",
    "response = bedrock_client.get_evaluation_job(\n",
    "    jobIdentifier=evaluation_job_arn \n",
    ")\n",
    "print(f\"Job Status: {response['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eaf6ba-3519-48fc-9554-7e848a4117ef",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This guide demonstrated how to implement Custom Metrics for RAG Evaluation on Amazon Bedrock. This feature allows organizations to:\n",
    "- Create tailored evaluation criteria beyond standard metrics\n",
    "- Define specialized scoring systems for unique business requirements\n",
    "- Combine custom and built-in metrics for comprehensive RAG assessment\n",
    "  \n",
    "With these capabilities, you can systematically evaluate and optimize your RAG applications according to the dimensions that matter most for your specific use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfe6700-d8a1-4388-a569-9ad847276136",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
